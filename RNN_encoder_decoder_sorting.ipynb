{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section of the code focuses on the modules import required to create the encoder-decoder seq2seq architecture.\n",
    "import numpy as np\n",
    "import itertools\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np  # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "import seaborn as sns  # Fancier plots\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Set seaborn plotting style\n",
    "sns.set_style('darkgrid')\n",
    "# Set the seed for reproducability\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The section creates a tanh classifier for the tanh activation function using the OOP approach.\n",
    "class TanH:\n",
    "    \"\"\" Applies tanh activation function in the forward and backward propagation.\n",
    "    \"\"\"\n",
    "    #The forward propagation\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "    #The backward function to calculated the gradient.\n",
    "    def backward(self, x, gradient):\n",
    "        g_tanh = 1.0 - (x ** 2)\n",
    "        return g_tanh * gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The section creates a linear classifier for the linear activation function using the OOP approach.\n",
    "class Linear:\n",
    "\n",
    "    #The initialization\n",
    "    def __init__(self, input_size: int, output_size: int, tensor_dim: int,\n",
    "                    weights=None, bias=None):\n",
    "        a = np.sqrt(6.0 / (input_size + output_size))\n",
    "        self.W = (np.random.uniform(-a, a, (input_size, output_size))\n",
    "                    if weights is None else weights)\n",
    "        self.b = (np.zeros(output_size) if bias is None else bias)\n",
    "        self.axes = tuple(range(tensor_dim - 1))\n",
    "\n",
    "    #The forward propagation.\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        makes forward pass\n",
    "        :param x: n-d tensor\n",
    "        :return: linear transformation\n",
    "        \"\"\"\n",
    "        return np.tensordot(x, self.W, axes=((-1), (0))) + self.b\n",
    "\n",
    "    #This function calculates the gradient\n",
    "    def backward(self, x, gradient):\n",
    "        \"\"\"\n",
    "        :param x: n-d tensor\n",
    "        :param gradient: gradient on previous step of backpropagation\n",
    "        :return: gradient, g_w, g_b\n",
    "        \"\"\"\n",
    "        # Same as: gW = np.einsum('ijk,ijl->kl', X, gY)\n",
    "        # Same as: gW += np.dot(X[:,j,:].T, gY[:,j,:])\n",
    "        #          (for i,j in X.shape[0:1])\n",
    "        g_w = np.tensordot(x, gradient, axes=(self.axes, self.axes))\n",
    "        g_b = np.sum(gradient, axis=self.axes)\n",
    "        # Same as: gX = np.einsum('ijk,kl->ijl', gY, self.W.T)\n",
    "        # Same as: gX[i,j,:] = np.dot(gY[i,j,:], self.W.T)\n",
    "        #          (for i,j in gY.shape[0:1])\n",
    "        gradient = np.tensordot(gradient, self.W.T, axes=((-1), (0)))\n",
    "        return gradient, g_w, g_b\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The section creates a softmax classifier for the softmax activation function using the OOP approach.\n",
    "class SoftmaxClassifier:\n",
    "    # The forward propagation.\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"param x: 3d tensor (batch_size, seq_length, input_size)\n",
    "            return: softmax probabilities\n",
    "        \"\"\"\n",
    "        x = x - np.expand_dims(np.max(x, axis=2), 2)\n",
    "        exp = np.exp(x)\n",
    "        exp_sum = exp.sum(-1)\n",
    "        return exp / exp_sum[:,:,np.newaxis]\n",
    "    \n",
    "    #The loss function calculates the cross entropy loss which is the main method of performance evaluation for this model.\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes Cross entropy loss\n",
    "        param y_pred: softmax activations (batch_size, seq_length, number_of_classes)\n",
    "        param y_true: ground truth labels (batch_size, seq_length, 1)\n",
    "        return: mean cross entropy loss over batch\n",
    "        \"\"\"\n",
    "\n",
    "        seq_length = y_true.shape[1]\n",
    "\n",
    "        losses = []\n",
    "        for idx, p in enumerate(y_pred):\n",
    "            # compute log likelihood\n",
    "            log_likelihood = -np.log(p[range(seq_length), y_true[idx].flatten()])\n",
    "            loss = np.sum(log_likelihood) / seq_length\n",
    "            losses.append(loss)\n",
    "\n",
    "        return np.mean(losses)\n",
    "\n",
    "    #The backward function used to calculate the gradients of loss function.\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes gradients of loss function\n",
    "        :param y_pred: softmax activations (batch_size, seq_length, number_of_classes)\n",
    "        :param y_true: ground truth labels (batch_size, seq_length, 1)\n",
    "        :return: gradient\n",
    "        \"\"\"\n",
    "\n",
    "        delta = np.zeros(y_pred.shape)\n",
    "        m = y_true.shape[1]\n",
    "\n",
    "        for idx in range(len(delta)):\n",
    "            grad = y_pred[idx]\n",
    "            grad[range(m), y_true[idx].flatten()] -= 1\n",
    "            # grad = grad / m\n",
    "            delta[idx] = grad\n",
    "\n",
    "        # return delta\n",
    "        return delta / (y_pred.shape[0] * y_pred.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a single cell vanilla RNN class created with simply itertools and no automatic differentiation.\n",
    "\n",
    "class RNNCell:\n",
    "    #The RNN cell is initialized.\n",
    "    def __init__(self, hidden_size, W, b):\n",
    "        tensor_dim = 2\n",
    "        self.linear = Linear(hidden_size, hidden_size, tensor_dim,\n",
    "                                W, b)\n",
    "        self.tanh = TanH()\n",
    "\n",
    "    #The forward pass of the RNN cell.\n",
    "    def forward(self, x, previous_state):\n",
    "        \"\"\"\n",
    "        This function makes one forward pass\n",
    "        :param x: 2d tensor (batch_size, hidden_size)\n",
    "        :param previous_state: 2d tensor (batch_size x hidden_size) of rnn state on previous forward pass\n",
    "        :return: rnn cell output\n",
    "        \"\"\"\n",
    "        return self.tanh.forward(x + self.linear.forward(previous_state))\n",
    "\n",
    "    #The backward propagation.\n",
    "    def backward(self, previous_state, current_state, gradient_state):\n",
    "        \"\"\"\n",
    "        :param previous_state: 2d tensor (batch_size x hidden_size) of rnn state\n",
    "        :param current_state: 2d tensor (batch_size x hidden_size) of rnn state\n",
    "        :param gradient_state: accumulated gradient during BPTT\n",
    "        :return:gradient, gradient_state, g_w, g_b\n",
    "        \"\"\"\n",
    "\n",
    "        gradient = self.tanh.backward(current_state, gradient_state)\n",
    "        gradient_state, g_w, g_b = self.linear.backward(previous_state, gradient)\n",
    "        return gradient, gradient_state, g_w, g_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define layer that unfolds the states over time\n",
    "class RNNLayer:\n",
    "    \"\"\"Unfold the recurrent states.\"\"\"\n",
    "    def __init__(self, hidden_size, sequence_length):\n",
    "        a = np.sqrt(6. / (hidden_size * 2))\n",
    "        self.W = np.random.uniform(-a, a, (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((self.W.shape[0]))\n",
    "        self.rnn_cell = RNNCell(\n",
    "            hidden_size, self.W, self.b)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.initial_state = np.zeros(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: 3d tensor (batch_size, seq_length, input_length)\n",
    "        :return: all hidden states\n",
    "        \"\"\"\n",
    "        states = np.zeros((x.shape[0], x.shape[1] + 1, self.hidden_size))\n",
    "        states[:, 0] = self.initial_state  # Set initial state\n",
    "\n",
    "        for idx in range(self.sequence_length):\n",
    "            # Update the states iteratively\n",
    "            states[:, idx + 1] = self.rnn_cell.forward(x[:, idx], states[:, idx])\n",
    "        return states\n",
    "\n",
    "    def backward(self, x, states, input_gradient):\n",
    "        \"\"\"\n",
    "        This method computes BPTT and returns all necessary gradients\n",
    "        :param x: 3d tensor (batch_size, seq_length, input_length)\n",
    "        :param states: 3d tensor of all rnn states (batch_size, seq_length, hidden_size)\n",
    "        :param input_gradient: gradient value on previous layers\n",
    "        :return: gradient, g_w_sum, g_b_sum, g_initial_state\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialise gradient of state outputs\n",
    "        gradient_state = np.zeros_like(input_gradient[:, self.sequence_length - 1])\n",
    "\n",
    "        # Initialse gradient tensor for state inputs\n",
    "        gradient = np.zeros_like(x)\n",
    "        g_w_sum = np.zeros_like(self.W)  # Initialise weight gradients\n",
    "        g_b_sum = np.zeros_like(self.b)  # Initialise bias gradients\n",
    "\n",
    "        # Propagate the gradients iteratively\n",
    "        for k in range(self.sequence_length - 1, -1, -1):\n",
    "            # Gradient at state output is gradient from previous state plus gradient from output\n",
    "            gradient_state += input_gradient[:, k]\n",
    "\n",
    "            # Propagate the gradient back through one state\n",
    "            gradient[:, k], gradient_state, g_w, g_b = self.rnn_cell.backward(\n",
    "                states[:, k], states[:, k + 1], gradient_state)\n",
    "\n",
    "            g_w_sum += g_w  # Update total weight gradient\n",
    "            g_b_sum += g_b  # Update total bias gradient\n",
    "\n",
    "        # Get gradient of initial state over all samples\n",
    "        g_initial_state = np.sum(gradient_state, axis=0)\n",
    "        return gradient, g_w_sum, g_b_sum, g_initial_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the main seq2seq encoder-decoder architecture created by stacking RNN cells which is trained to sort numbers.\n",
    "class ModelSort:\n",
    "    #The initialisation\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int,\n",
    "                    sequence_len: int, tensor_dim: int = 3):\n",
    "        tensor_dim = 3\n",
    "        self.lr = 1e-3\n",
    "        self.input_linear = Linear(input_size, hidden_size, tensor_dim)\n",
    "        self.rnn = RNNLayer(hidden_size, sequence_len)\n",
    "        self.output_linear = Linear(hidden_size, output_size, tensor_dim)\n",
    "        self.classifier = SoftmaxClassifier()  # Classification output\n",
    "        self.sequence_len = sequence_len\n",
    "\n",
    "    #The forward propagation with updating the parameters.\n",
    "    def train_on_batch(self, x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        This method makes forward run and update its parameters\n",
    "        :param x_batch: 3d tensor (batch_size, seq_length, input_length)\n",
    "        :param y_batch: 3d tensor (batch_size, seq_length, 1)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        linear_out, rnn_states, out, probabilities = self.forward(x_batch)\n",
    "        gradients = self.backward(x_batch, probabilities, linear_out, rnn_states, y_batch)\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \"\"\"\n",
    "        :param x_batch: 3d tensor (batch_size, seq_length, input_length)\n",
    "        :return: linear_out, rnn_states, linear_out, probabilites\n",
    "        \"\"\"\n",
    "        linear_out = self.input_linear.forward(x_batch)\n",
    "        rnn_states = self.rnn.forward(linear_out)\n",
    "        out = self.output_linear.forward(rnn_states[:, 1:self.sequence_len + 1, :])\n",
    "        probabilities = self.classifier.forward(out)\n",
    "\n",
    "        return linear_out, rnn_states, out, probabilities\n",
    "\n",
    "\n",
    "    def backward(self, x_batch, y_pred, linear_out, rnn_states, y_batch):\n",
    "        \"\"\"\n",
    "        Make computing of gradients and update parameters\n",
    "        :param x_batch: 3d tensor (batch_size, seq_length, input_length)\n",
    "        :param y_pred (batch_size, seq_length, number_of_classes)\n",
    "        :param linear_out:\n",
    "        :param rnn_states: 3d tensor of all rnn states (batch_size, seq_length, hidden_size)\n",
    "        :param y_batch: 3d tensor (batch_size, seq_length, 1)\n",
    "        :return:gradients\n",
    "        \"\"\"\n",
    "\n",
    "        # get gradients for all layers\n",
    "        gradient = self.classifier.backward(y_pred, y_batch)\n",
    "        gradient, g_lout_w, g_lout_b = self.output_linear.backward(\n",
    "            rnn_states[:, 1:self.sequence_len + 1, :], gradient)\n",
    "\n",
    "        # Propagate gradient backwards through time\n",
    "        gradient, g_rnn_w, g_rnn_b, g_init_state = self.rnn.backward(\n",
    "            linear_out, rnn_states, gradient)\n",
    "\n",
    "        g_x, g_lin_w, g_lin_b = self.input_linear.backward(x_batch, gradient)\n",
    "        # Return the parameter gradients of: linear output weights,\n",
    "        #  linear output bias, recursive weights, recursive bias, #\n",
    "        #  linear input weights, linear input bias, initial state.\n",
    "\n",
    "        gradients = [g for g in itertools.chain(\n",
    "            np.nditer(g_init_state),\n",
    "            np.nditer(g_lin_w),\n",
    "            np.nditer(g_lin_b),\n",
    "            np.nditer(g_rnn_w),\n",
    "            np.nditer(g_rnn_b),\n",
    "            np.nditer(g_lout_w),\n",
    "            np.nditer(g_lout_b))]\n",
    "\n",
    "        # update weights\n",
    "        for idx, parameter in enumerate(self.get_params_iter()):\n",
    "            parameter -= self.lr * gradients[idx]\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    #These functions are used for the prediction section of the model to predict based on training.\n",
    "    def predict_proba(self, x_batch):\n",
    "        \"\"\"\n",
    "        :param x_batch: 3d tensor of (batch_size, seq_length, input_length)\n",
    "        :return: y_proba (batch_size, seq_length, number_of_classes)\n",
    "        \"\"\"\n",
    "        _, _, _, y_proba = self.forward(x_batch)\n",
    "        return y_proba\n",
    "\n",
    "    def predict(self, x_batch):\n",
    "        \"\"\"\n",
    "        This method predicts class\n",
    "        :param x_batch: 3d tensor of (batch_size, seq_length, input_length)\n",
    "        :return: 2d tensor of predictions (batch_size, seq_length)\n",
    "        \"\"\"\n",
    "        y_proba = self.predict_proba(x_batch)\n",
    "        return np.argmax(y_proba, axis=2)\n",
    "\n",
    "    def get_gradients(self, x_batch, y_batch):\n",
    "        \"\"\"Return the gradients with respect to input X and\n",
    "        target T as a list. The list has the same order as the\n",
    "        get_params_iter iterator.\"\"\"\n",
    "        linear_out, rnn_states, out, probabilities = self.forward(x_batch)\n",
    "        return self.backward(x_batch, probabilities, linear_out, rnn_states, y_batch)\n",
    "\n",
    "    #The cross entropy loss of the batch.\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        :param y_pred: predicted probabilities (batch_size, seq_length, number_of_classes)\n",
    "        :param y_true: true labels (batch_size, seq_length, 1)\n",
    "        :return: Cross entropy loss over batch\n",
    "        \"\"\"\n",
    "        return self.classifier.loss(y_pred, y_true)\n",
    "\n",
    "    #The iterator function.\n",
    "    def get_params_iter(self):\n",
    "        \"\"\"\n",
    "        Returns iterator over all parameters in model;\n",
    "        np.nditer is efficient iterator from numpy; parameters are iterable inplace\n",
    "        \"\"\"\n",
    "        return itertools.chain(\n",
    "            np.nditer(self.rnn.initial_state, op_flags=['readwrite']),\n",
    "            np.nditer(self.input_linear.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.input_linear.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnn.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnn.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.output_linear.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.output_linear.b, op_flags=['readwrite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a randomized data set for sorting.\n",
    "def create_sort_dataset(dataset_length, seq_length, max_number=999, fraction=0.8):\n",
    "    x_train = np.random.randint(low=0, high=max_number+1, size=(int(dataset_length*fraction),\n",
    "                                                                seq_length,\n",
    "                                                                1))\n",
    "    y_train = np.sort(x_train, axis=1)\n",
    "    \n",
    "    x_test = np.random.randint(low=0, high=max_number+1, size=(int(dataset_length*(1-fraction)),\n",
    "                                                                seq_length,\n",
    "                                                                1))\n",
    "    y_test = np.sort(x_test, axis=1)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "#A dummy data set\n",
    "def create_dummy_dataset(dataset_length, seq_length, max_number):\n",
    "    lower_bound = -1 * max_number\n",
    "    x_train = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_train = np.where(x_train.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    x_test = np.random.randint(low=lower_bound, high=max_number+1, size=(dataset_length, seq_length, 1))\n",
    "    y_test = np.where(x_test.sum(axis=2) > 0, 1, 0).reshape(x_train.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "#The evaluation using cross entropy loss\n",
    "def evaluate(y_test, x_test, model):\n",
    "    y_pred = model.predict(x_test)\n",
    "    loss = model.loss(model.predict_proba(x_test), y_test)\n",
    "    \n",
    "    print(f'Cross entropy loss {loss}')\n",
    "    print('-'*100)\n",
    "    print(classification_report(y_test.flatten(), y_pred.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameters\n",
    "batch_size = 20  # Size of the minibatches (number of samples)\n",
    "max_num = 10 \n",
    "seq_length = 5\n",
    "hidden_size = 30 \n",
    "dataset_size = 2000\n",
    "epoch = 40\n",
    "\n",
    "x_train, y_train, x_test, y_test = create_sort_dataset(dataset_size, seq_length, max_num)\n",
    "\n",
    "input_size = 1\n",
    "output_size = max_num+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "TRAIN: Cross entropy loss:  2.734433111535017\n",
      "VALIDATION: Cross entropy loss:  2.6867594294900736\n",
      "Epoch 2/40\n",
      "TRAIN: Cross entropy loss:  2.6315652899774715\n",
      "VALIDATION: Cross entropy loss:  2.6069654633902304\n",
      "Epoch 3/40\n",
      "TRAIN: Cross entropy loss:  2.567190697873727\n",
      "VALIDATION: Cross entropy loss:  2.55390525846204\n",
      "Epoch 4/40\n",
      "TRAIN: Cross entropy loss:  2.5223385977549437\n",
      "VALIDATION: Cross entropy loss:  2.5155098077714872\n",
      "Epoch 5/40\n",
      "TRAIN: Cross entropy loss:  2.488935015541911\n",
      "VALIDATION: Cross entropy loss:  2.4862935961779207\n",
      "Epoch 6/40\n",
      "TRAIN: Cross entropy loss:  2.4629719983660197\n",
      "VALIDATION: Cross entropy loss:  2.4632593244100027\n",
      "Epoch 7/40\n",
      "TRAIN: Cross entropy loss:  2.4421202312947163\n",
      "VALIDATION: Cross entropy loss:  2.4445757729903024\n",
      "Epoch 8/40\n",
      "TRAIN: Cross entropy loss:  2.4249099490949884\n",
      "VALIDATION: Cross entropy loss:  2.4290541434820874\n",
      "Epoch 9/40\n",
      "TRAIN: Cross entropy loss:  2.4103721141299332\n",
      "VALIDATION: Cross entropy loss:  2.4158913421148718\n",
      "Epoch 10/40\n",
      "TRAIN: Cross entropy loss:  2.397846395992703\n",
      "VALIDATION: Cross entropy loss:  2.404525096885601\n",
      "Epoch 11/40\n",
      "TRAIN: Cross entropy loss:  2.386866584484527\n",
      "VALIDATION: Cross entropy loss:  2.3945465640761254\n",
      "Epoch 12/40\n",
      "TRAIN: Cross entropy loss:  2.3770898628394956\n",
      "VALIDATION: Cross entropy loss:  2.3856468465245957\n",
      "Epoch 13/40\n",
      "TRAIN: Cross entropy loss:  2.3682535375156375\n",
      "VALIDATION: Cross entropy loss:  2.3775846295116834\n",
      "Epoch 14/40\n",
      "TRAIN: Cross entropy loss:  2.3601492007947265\n",
      "VALIDATION: Cross entropy loss:  2.370166868250434\n",
      "Epoch 15/40\n",
      "TRAIN: Cross entropy loss:  2.3526075991611837\n",
      "VALIDATION: Cross entropy loss:  2.3632372230171077\n",
      "Epoch 16/40\n",
      "TRAIN: Cross entropy loss:  2.345489738338553\n",
      "VALIDATION: Cross entropy loss:  2.3566688632826565\n",
      "Epoch 17/40\n",
      "TRAIN: Cross entropy loss:  2.33868142827652\n",
      "VALIDATION: Cross entropy loss:  2.3503596313867456\n",
      "Epoch 18/40\n",
      "TRAIN: Cross entropy loss:  2.3320896490048795\n",
      "VALIDATION: Cross entropy loss:  2.3442284617827704\n",
      "Epoch 19/40\n",
      "TRAIN: Cross entropy loss:  2.3256398620260144\n",
      "VALIDATION: Cross entropy loss:  2.3382124747327406\n",
      "Epoch 20/40\n",
      "TRAIN: Cross entropy loss:  2.31927379076024\n",
      "VALIDATION: Cross entropy loss:  2.332264401747897\n",
      "Epoch 21/40\n",
      "TRAIN: Cross entropy loss:  2.31294735737455\n",
      "VALIDATION: Cross entropy loss:  2.326350068670364\n",
      "Epoch 22/40\n",
      "TRAIN: Cross entropy loss:  2.3066285135727598\n",
      "VALIDATION: Cross entropy loss:  2.3204456899665176\n",
      "Epoch 23/40\n",
      "TRAIN: Cross entropy loss:  2.300294762000494\n",
      "VALIDATION: Cross entropy loss:  2.3145348355939523\n",
      "Epoch 24/40\n",
      "TRAIN: Cross entropy loss:  2.293930321155577\n",
      "VALIDATION: Cross entropy loss:  2.308605169629579\n",
      "Epoch 25/40\n",
      "TRAIN: Cross entropy loss:  2.2875231341930866\n",
      "VALIDATION: Cross entropy loss:  2.302645341198786\n",
      "Epoch 26/40\n",
      "TRAIN: Cross entropy loss:  2.281062137132782\n",
      "VALIDATION: Cross entropy loss:  2.296642545614804\n",
      "Epoch 27/40\n",
      "TRAIN: Cross entropy loss:  2.274535230616167\n",
      "VALIDATION: Cross entropy loss:  2.2905811446144986\n",
      "Epoch 28/40\n",
      "TRAIN: Cross entropy loss:  2.26792820808702\n",
      "VALIDATION: Cross entropy loss:  2.284442420111709\n",
      "Epoch 29/40\n",
      "TRAIN: Cross entropy loss:  2.2612246166431635\n",
      "VALIDATION: Cross entropy loss:  2.278205254111588\n",
      "Epoch 30/40\n",
      "TRAIN: Cross entropy loss:  2.254406343593143\n",
      "VALIDATION: Cross entropy loss:  2.271847434038716\n",
      "Epoch 31/40\n",
      "TRAIN: Cross entropy loss:  2.247454704042077\n",
      "VALIDATION: Cross entropy loss:  2.265347363918118\n",
      "Epoch 32/40\n",
      "TRAIN: Cross entropy loss:  2.240351892760496\n",
      "VALIDATION: Cross entropy loss:  2.2586860918681313\n",
      "Epoch 33/40\n",
      "TRAIN: Cross entropy loss:  2.23308274422981\n",
      "VALIDATION: Cross entropy loss:  2.251849613415885\n",
      "Epoch 34/40\n",
      "TRAIN: Cross entropy loss:  2.2256367306417095\n",
      "VALIDATION: Cross entropy loss:  2.2448313078481514\n",
      "Epoch 35/40\n",
      "TRAIN: Cross entropy loss:  2.2180099973149536\n",
      "VALIDATION: Cross entropy loss:  2.237634133056967\n",
      "Epoch 36/40\n",
      "TRAIN: Cross entropy loss:  2.210207059350221\n",
      "VALIDATION: Cross entropy loss:  2.2302719888815035\n",
      "Epoch 37/40\n",
      "TRAIN: Cross entropy loss:  2.2022417230753644\n",
      "VALIDATION: Cross entropy loss:  2.222769696118517\n",
      "Epoch 38/40\n",
      "TRAIN: Cross entropy loss:  2.19413698890246\n",
      "VALIDATION: Cross entropy loss:  2.2151614509989854\n",
      "Epoch 39/40\n",
      "TRAIN: Cross entropy loss:  2.185924045161855\n",
      "VALIDATION: Cross entropy loss:  2.2074881639645874\n",
      "Epoch 40/40\n",
      "TRAIN: Cross entropy loss:  2.1776406381868902\n",
      "VALIDATION: Cross entropy loss:  2.199794316489316\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the network\n",
    "#The training of the encoder-decoder network\n",
    "model = ModelSort(input_size, output_size, hidden_size, seq_length)\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    print(f'Epoch {i+1}/{epoch}')\n",
    "    epoch_train_loss = []\n",
    "    \n",
    "    for mb in range(dataset_size // batch_size):\n",
    "        x_batch = x_train[mb:mb + batch_size]  # Input minibatch\n",
    "        y_batch = y_train[mb:mb + batch_size]  # Target minibatch\n",
    "        model.train_on_batch(x_batch, y_batch)\n",
    "        \n",
    "        loss = model.loss(model.predict_proba(x_batch), y_batch)\n",
    "        epoch_train_loss.append(loss)\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    validation_loss.append(model.loss(model.predict_proba(x_test), y_test))\n",
    "    print(\"TRAIN: Cross entropy loss: \", train_loss[-1])\n",
    "    print(\"VALIDATION: Cross entropy loss: \", validation_loss[-1])\n",
    "# Plot the loss over the iterations\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss, 'b-')\n",
    "plt.plot(validation_loss, 'r')\n",
    "plt.xlabel('minibatch iteration')\n",
    "plt.ylabel('Cross entropy loss', fontsize=15)\n",
    "plt.xlim(0, 100)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss 2.199794316489316\n",
      "----------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.87      0.52       209\n",
      "           1       0.55      0.07      0.13       152\n",
      "           2       0.22      0.05      0.08       168\n",
      "           3       0.23      0.31      0.26       176\n",
      "           4       0.18      0.11      0.13       187\n",
      "           5       0.07      0.01      0.01       180\n",
      "           6       0.12      0.11      0.12       172\n",
      "           7       0.16      0.50      0.25       209\n",
      "           8       0.00      0.00      0.00       191\n",
      "           9       0.13      0.11      0.12       159\n",
      "          10       0.03      0.02      0.02       192\n",
      "\n",
      "    accuracy                           0.21      1995\n",
      "   macro avg       0.19      0.19      0.15      1995\n",
      "weighted avg       0.18      0.21      0.16      1995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The evaluation report\n",
    "evaluate(y_test, x_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter id: 0 Numerical gradient of 0.007555289727179114 is not very close to the backpropagation gradient of 0.007568030528712834!\n",
      "Parameter id: 1 Numerical gradient of -0.0052442494791193894 is not very close to the backpropagation gradient of -0.005277965431717676!\n",
      "Parameter id: 2 Numerical gradient of 0.008135936369058072 is not very close to the backpropagation gradient of 0.008131413731813135!\n",
      "Parameter id: 3 Numerical gradient of 0.002971622947711694 is not very close to the backpropagation gradient of 0.0029321404233255905!\n",
      "Parameter id: 4 Numerical gradient of -0.015066392577978148 is not very close to the backpropagation gradient of -0.015009526948930593!\n",
      "Parameter id: 6 Numerical gradient of 0.02219646688672583 is not very close to the backpropagation gradient of 0.022169127602243995!\n",
      "Parameter id: 7 Numerical gradient of -0.007405853708064568 is not very close to the backpropagation gradient of -0.007390035541005654!\n",
      "Parameter id: 8 Numerical gradient of 0.012651657499418434 is not very close to the backpropagation gradient of 0.012667599599436873!\n",
      "Parameter id: 9 Numerical gradient of -0.01901501178735998 is not very close to the backpropagation gradient of -0.01905378681422833!\n",
      "Parameter id: 10 Numerical gradient of -0.045451420405129284 is not very close to the backpropagation gradient of -0.04545014320775351!\n",
      "Parameter id: 11 Numerical gradient of -0.030741409418055806 is not very close to the backpropagation gradient of -0.030770607974677268!\n",
      "Parameter id: 12 Numerical gradient of -0.029530822232004535 is not very close to the backpropagation gradient of -0.029528464981926537!\n",
      "Parameter id: 13 Numerical gradient of 0.011653567000280418 is not very close to the backpropagation gradient of 0.01164516474339672!\n",
      "Parameter id: 14 Numerical gradient of -0.014553025451391475 is not very close to the backpropagation gradient of -0.014583153807252182!\n",
      "Parameter id: 15 Numerical gradient of -0.007798428569572024 is not very close to the backpropagation gradient of -0.007788612405860809!\n",
      "Parameter id: 16 Numerical gradient of 0.019398704864670435 is not very close to the backpropagation gradient of 0.0194549437874826!\n",
      "Parameter id: 17 Numerical gradient of -0.0015718537582642966 is not very close to the backpropagation gradient of -0.0016166359059185372!\n",
      "Parameter id: 18 Numerical gradient of -0.004441558232315401 is not very close to the backpropagation gradient of -0.004503306400205988!\n",
      "Parameter id: 19 Numerical gradient of 0.039076519797731635 is not very close to the backpropagation gradient of 0.03910601591375729!\n",
      "Parameter id: 20 Numerical gradient of 0.012344125721597266 is not very close to the backpropagation gradient of 0.012351674507067338!\n",
      "Parameter id: 21 Numerical gradient of 0.000496713781217295 is not very close to the backpropagation gradient of 0.0005223596611622537!\n",
      "Parameter id: 22 Numerical gradient of -0.019247714533321414 is not very close to the backpropagation gradient of -0.019279332726140738!\n",
      "Parameter id: 23 Numerical gradient of -0.015975443190541228 is not very close to the backpropagation gradient of -0.015996825339897266!\n",
      "Parameter id: 24 Numerical gradient of -0.036089575772280114 is not very close to the backpropagation gradient of -0.03612949773670729!\n",
      "Parameter id: 25 Numerical gradient of 0.028402835638985376 is not very close to the backpropagation gradient of 0.02840749134456769!\n",
      "Parameter id: 26 Numerical gradient of 0.003881117649484622 is not very close to the backpropagation gradient of 0.0038583436077133794!\n",
      "Parameter id: 27 Numerical gradient of -0.016592283103022964 is not very close to the backpropagation gradient of -0.016575689691961524!\n",
      "Parameter id: 28 Numerical gradient of 0.015907941630644018 is not very close to the backpropagation gradient of 0.01591097071767153!\n",
      "Parameter id: 29 Numerical gradient of 0.007355893671956436 is not very close to the backpropagation gradient of 0.007401845425963849!\n",
      "Parameter id: 30 Numerical gradient of 0.5163625083071111 is not very close to the backpropagation gradient of 0.5138167215017472!\n",
      "Parameter id: 31 Numerical gradient of -0.03451683383559612 is not very close to the backpropagation gradient of -0.03462314545740571!\n",
      "Parameter id: 32 Numerical gradient of 0.26803959052301707 is not very close to the backpropagation gradient of 0.26683423893558794!\n",
      "Parameter id: 33 Numerical gradient of 0.1396935900288554 is not very close to the backpropagation gradient of 0.13965942886858682!\n",
      "Parameter id: 34 Numerical gradient of -0.08411182861323141 is not very close to the backpropagation gradient of -0.08421693759859088!\n",
      "Parameter id: 35 Numerical gradient of -0.06767564286747074 is not very close to the backpropagation gradient of -0.06763761636124971!\n",
      "Parameter id: 36 Numerical gradient of 0.11097056606956812 is not very close to the backpropagation gradient of 0.11075907953551561!\n",
      "Parameter id: 37 Numerical gradient of 0.009121814414925211 is not very close to the backpropagation gradient of 0.00852070550748085!\n",
      "Parameter id: 38 Numerical gradient of 0.46288861632604034 is not very close to the backpropagation gradient of 0.46217589979059487!\n",
      "Parameter id: 39 Numerical gradient of -0.02003219812252155 is not very close to the backpropagation gradient of -0.020037714066640237!\n",
      "Parameter id: 40 Numerical gradient of 0.11045786507679622 is not very close to the backpropagation gradient of 0.11066354949525724!\n",
      "Parameter id: 41 Numerical gradient of -0.07932010603894923 is not very close to the backpropagation gradient of -0.07934055939224181!\n",
      "Parameter id: 42 Numerical gradient of -0.3207094589896542 is not very close to the backpropagation gradient of -0.319725571920468!\n",
      "Parameter id: 43 Numerical gradient of -0.048226977966692175 is not very close to the backpropagation gradient of -0.04837954722043285!\n",
      "Parameter id: 44 Numerical gradient of 0.01060240784056532 is not very close to the backpropagation gradient of 0.010658836371979185!\n",
      "Parameter id: 45 Numerical gradient of 0.31757840801560633 is not very close to the backpropagation gradient of 0.3174043284231914!\n",
      "Parameter id: 46 Numerical gradient of -0.008866463119261425 is not very close to the backpropagation gradient of -0.008836001370789787!\n",
      "Parameter id: 47 Numerical gradient of -0.06937339591672753 is not very close to the backpropagation gradient of -0.0695823084508279!\n",
      "Parameter id: 48 Numerical gradient of -0.08549339014507495 is not very close to the backpropagation gradient of -0.085559355401532!\n",
      "Parameter id: 49 Numerical gradient of 0.0342266215369591 is not very close to the backpropagation gradient of 0.03449608117079748!\n",
      "Parameter id: 50 Numerical gradient of -0.027289281945286348 is not very close to the backpropagation gradient of -0.02730785961182642!\n",
      "Parameter id: 51 Numerical gradient of 0.19057688760426572 is not very close to the backpropagation gradient of 0.19051165412038362!\n",
      "Parameter id: 52 Numerical gradient of -0.08512301974406 is not very close to the backpropagation gradient of -0.08513990990042147!\n",
      "Parameter id: 53 Numerical gradient of 0.02698397061351443 is not very close to the backpropagation gradient of 0.026805593525379798!\n",
      "Parameter id: 54 Numerical gradient of -0.11986123205076636 is not very close to the backpropagation gradient of -0.11996010803472543!\n",
      "Parameter id: 55 Numerical gradient of 0.20955215340734412 is not very close to the backpropagation gradient of 0.21004780697296505!\n",
      "Parameter id: 56 Numerical gradient of -0.09239942144745328 is not very close to the backpropagation gradient of -0.09267585717362907!\n",
      "Parameter id: 57 Numerical gradient of 0.008135048190638372 is not very close to the backpropagation gradient of 0.008123052736432617!\n",
      "Parameter id: 58 Numerical gradient of 0.09808243106590453 is not very close to the backpropagation gradient of 0.09813613413605435!\n",
      "Parameter id: 59 Numerical gradient of 0.013220979866446214 is not very close to the backpropagation gradient of 0.013097852005987896!\n",
      "Parameter id: 60 Numerical gradient of 0.12267409310595666 is not very close to the backpropagation gradient of 0.12239744253770243!\n",
      "Parameter id: 61 Numerical gradient of -0.013197887227534011 is not very close to the backpropagation gradient of -0.013279369191500896!\n",
      "Parameter id: 62 Numerical gradient of 0.05718292506173838 is not very close to the backpropagation gradient of 0.05698068232401206!\n",
      "Parameter id: 63 Numerical gradient of 0.020952128920725954 is not very close to the backpropagation gradient of 0.02101888079348742!\n",
      "Parameter id: 64 Numerical gradient of -0.020103030351492635 is not very close to the backpropagation gradient of -0.02015127312990671!\n",
      "Parameter id: 65 Numerical gradient of 0.002177591440499782 is not very close to the backpropagation gradient of 0.0021788965174013805!\n",
      "Parameter id: 66 Numerical gradient of 0.028489210990301213 is not very close to the backpropagation gradient of 0.028465696287666983!\n",
      "Parameter id: 67 Numerical gradient of 0.008270051310432791 is not very close to the backpropagation gradient of 0.008155769252840584!\n",
      "Parameter id: 68 Numerical gradient of 0.06760614290612921 is not very close to the backpropagation gradient of 0.0674648028095073!\n",
      "Parameter id: 69 Numerical gradient of -0.013396839193546839 is not very close to the backpropagation gradient of -0.013412471620459008!\n",
      "Parameter id: 70 Numerical gradient of 0.014355627797613122 is not very close to the backpropagation gradient of 0.014448528730647359!\n",
      "Parameter id: 71 Numerical gradient of -0.01765254609153999 is not very close to the backpropagation gradient of -0.017668077958465127!\n",
      "Parameter id: 72 Numerical gradient of -0.06631073468099657 is not very close to the backpropagation gradient of -0.06618441461169876!\n",
      "Parameter id: 73 Numerical gradient of -0.01800604110258064 is not very close to the backpropagation gradient of -0.018094542112890872!\n",
      "Parameter id: 74 Numerical gradient of 0.0007498446308318307 is not very close to the backpropagation gradient of 0.0007619039075168078!\n",
      "Parameter id: 75 Numerical gradient of 0.05129652258517581 is not very close to the backpropagation gradient of 0.051294631806757154!\n",
      "Parameter id: 76 Numerical gradient of -0.005068390152018765 is not very close to the backpropagation gradient of -0.00503089989914279!\n",
      "Parameter id: 77 Numerical gradient of -0.024627633266049997 is not very close to the backpropagation gradient of -0.024715339370260492!\n",
      "Parameter id: 78 Numerical gradient of -0.037762237781180374 is not very close to the backpropagation gradient of -0.037809226573035336!\n",
      "Parameter id: 79 Numerical gradient of 0.013502754470096079 is not very close to the backpropagation gradient of 0.013571411469147511!\n",
      "Parameter id: 80 Numerical gradient of -0.010438983011340497 is not very close to the backpropagation gradient of -0.010446840388634391!\n",
      "Parameter id: 81 Numerical gradient of 0.05300115901718527 is not very close to the backpropagation gradient of 0.05302258863190548!\n",
      "Parameter id: 82 Numerical gradient of -0.05106004508093065 is not very close to the backpropagation gradient of -0.05108487990609839!\n",
      "Parameter id: 83 Numerical gradient of 0.004635403172414954 is not very close to the backpropagation gradient of 0.004626285960930322!\n",
      "Parameter id: 84 Numerical gradient of -0.04693356814300387 is not very close to the backpropagation gradient of -0.046983014870181236!\n",
      "Parameter id: 85 Numerical gradient of 0.04010725085379363 is not very close to the backpropagation gradient of 0.040186612847057754!\n",
      "Parameter id: 86 Numerical gradient of -0.03325073549831359 is not very close to the backpropagation gradient of -0.03332773685659382!\n",
      "Parameter id: 87 Numerical gradient of -0.003698596984236246 is not very close to the backpropagation gradient of -0.003703326729148771!\n",
      "Parameter id: 88 Numerical gradient of 0.02715783153917073 is not very close to the backpropagation gradient of 0.027190662150599457!\n",
      "Parameter id: 89 Numerical gradient of 0.004866995695351761 is not very close to the backpropagation gradient of 0.004843921089785224!\n",
      "Parameter id: 90 Numerical gradient of 0.03607802945282401 is not very close to the backpropagation gradient of 0.03627792251294642!\n",
      "Parameter id: 91 Numerical gradient of -0.005398348434937361 is not very close to the backpropagation gradient of -0.005456344317483318!\n",
      "Parameter id: 92 Numerical gradient of 0.012351231148954867 is not very close to the backpropagation gradient of 0.012421455488666423!\n",
      "Parameter id: 93 Numerical gradient of 0.010968559394086697 is not very close to the backpropagation gradient of 0.011130486207851727!\n",
      "Parameter id: 94 Numerical gradient of -0.01216560185923754 is not very close to the backpropagation gradient of -0.012246344428411394!\n",
      "Parameter id: 95 Numerical gradient of 0.0026756374893466273 is not very close to the backpropagation gradient of 0.002653146963367235!\n",
      "Parameter id: 96 Numerical gradient of -0.00039479530755670567 is not very close to the backpropagation gradient of -0.00041259635739654486!\n",
      "Parameter id: 97 Numerical gradient of 0.004829026067909581 is not very close to the backpropagation gradient of 0.00483979034120271!\n",
      "Parameter id: 98 Numerical gradient of 0.017320145317967217 is not very close to the backpropagation gradient of 0.017439815957711016!\n",
      "Parameter id: 99 Numerical gradient of -0.004355849014814339 is not very close to the backpropagation gradient of -0.004368464814932058!\n",
      "Parameter id: 100 Numerical gradient of 0.012194911747087644 is not very close to the backpropagation gradient of 0.012311735558371225!\n",
      "Parameter id: 101 Numerical gradient of 0.0006417089082333405 is not very close to the backpropagation gradient of 0.0005968369237259869!\n",
      "Parameter id: 102 Numerical gradient of -0.0050173198928860074 is not very close to the backpropagation gradient of -0.005004735836029708!\n",
      "Parameter id: 103 Numerical gradient of -0.005303979477844223 is not very close to the backpropagation gradient of -0.005406466315332627!\n",
      "Parameter id: 104 Numerical gradient of -2.6423307986078726e-05 is not very close to the backpropagation gradient of -2.618988108413179e-05!\n",
      "Parameter id: 105 Numerical gradient of 0.006960432230584956 is not very close to the backpropagation gradient of 0.006981371762595152!\n",
      "Parameter id: 106 Numerical gradient of 0.0022666313270747196 is not very close to the backpropagation gradient of 0.0022904610725815173!\n",
      "Parameter id: 107 Numerical gradient of -0.003896216682619524 is not very close to the backpropagation gradient of -0.003930443077896219!\n",
      "Parameter id: 108 Numerical gradient of -0.004596989455762923 is not very close to the backpropagation gradient of -0.004669474572080629!\n",
      "Parameter id: 109 Numerical gradient of -0.00021405099914773018 is not very close to the backpropagation gradient of -0.0001280417864588507!\n",
      "Parameter id: 110 Numerical gradient of -0.0005428990590417015 is not very close to the backpropagation gradient of -0.0005493416134425088!\n",
      "Parameter id: 111 Numerical gradient of 0.0071374017807102055 is not very close to the backpropagation gradient of 0.0071758152879176865!\n",
      "Parameter id: 112 Numerical gradient of -0.005843103778602199 is not very close to the backpropagation gradient of -0.005880092377301282!\n",
      "Parameter id: 113 Numerical gradient of -0.005409450665183613 is not very close to the backpropagation gradient of -0.005400630488886113!\n",
      "Parameter id: 114 Numerical gradient of 0.00046185277824406507 is not very close to the backpropagation gradient of 0.0004521139738477702!\n",
      "Parameter id: 115 Numerical gradient of 0.010591305610319068 is not very close to the backpropagation gradient of 0.010669872717554868!\n",
      "Parameter id: 116 Numerical gradient of 0.0025652813206988867 is not very close to the backpropagation gradient of 0.0025153534733417748!\n",
      "Parameter id: 117 Numerical gradient of -0.0020088375407567582 is not very close to the backpropagation gradient of -0.0020373378691876096!\n",
      "Parameter id: 118 Numerical gradient of 0.002632116746781321 is not very close to the backpropagation gradient of 0.0026373665447512874!\n",
      "Parameter id: 119 Numerical gradient of -0.0007993605777301127 is not very close to the backpropagation gradient of -0.0008629394773603319!\n",
      "Parameter id: 120 Numerical gradient of -0.08201417323050464 is not very close to the backpropagation gradient of -0.08185755364115609!\n",
      "Parameter id: 121 Numerical gradient of 0.010022205287896213 is not very close to the backpropagation gradient of 0.0100822585426393!\n",
      "Parameter id: 122 Numerical gradient of -0.04645883677767415 is not very close to the backpropagation gradient of -0.046277647830653884!\n",
      "Parameter id: 123 Numerical gradient of -0.03514943891502753 is not very close to the backpropagation gradient of -0.03521013461319368!\n",
      "Parameter id: 124 Numerical gradient of 0.02241939967007056 is not very close to the backpropagation gradient of 0.022471859826755765!\n",
      "Parameter id: 125 Numerical gradient of 0.0004736211423050917 is not very close to the backpropagation gradient of 0.00049467895337677!\n",
      "Parameter id: 126 Numerical gradient of 0.004353406524160164 is not very close to the backpropagation gradient of 0.004347430357451971!\n",
      "Parameter id: 127 Numerical gradient of -0.012753798017683948 is not very close to the backpropagation gradient of -0.012681703637449826!\n",
      "Parameter id: 128 Numerical gradient of -0.0519626563999509 is not very close to the backpropagation gradient of -0.05177489154516101!\n",
      "Parameter id: 129 Numerical gradient of 0.007787104294720847 is not very close to the backpropagation gradient of 0.00777614949816001!\n",
      "Parameter id: 130 Numerical gradient of -0.02797584386371454 is not very close to the backpropagation gradient of -0.02804738610919616!\n",
      "Parameter id: 131 Numerical gradient of 0.013482992500257751 is not very close to the backpropagation gradient of 0.013476002911865579!\n",
      "Parameter id: 132 Numerical gradient of 0.012966294704597203 is not very close to the backpropagation gradient of 0.012930473095555886!\n",
      "Parameter id: 133 Numerical gradient of 0.01570810148621149 is not very close to the backpropagation gradient of 0.015778341960049305!\n",
      "Parameter id: 134 Numerical gradient of 0.00025135449277513544 is not very close to the backpropagation gradient of 0.00024661949446417144!\n",
      "Parameter id: 135 Numerical gradient of -0.015874634939905263 is not very close to the backpropagation gradient of -0.01582617563969876!\n",
      "Parameter id: 136 Numerical gradient of -0.005498268507153625 is not very close to the backpropagation gradient of -0.005508866686181874!\n",
      "Parameter id: 137 Numerical gradient of 0.006716627254377272 is not very close to the backpropagation gradient of 0.006769448453520219!\n",
      "Parameter id: 138 Numerical gradient of 0.018521850719821487 is not very close to the backpropagation gradient of 0.018547199668101123!\n",
      "Parameter id: 139 Numerical gradient of -0.015186296664637665 is not very close to the backpropagation gradient of -0.015212884873388031!\n",
      "Parameter id: 140 Numerical gradient of 0.0025850432905372145 is not very close to the backpropagation gradient of 0.0025764659525137294!\n",
      "Parameter id: 141 Numerical gradient of -0.016373791211776734 is not very close to the backpropagation gradient of -0.016371026250363928!\n",
      "Parameter id: 142 Numerical gradient of 0.014143797244514643 is not very close to the backpropagation gradient of 0.014151555176494131!\n",
      "Parameter id: 143 Numerical gradient of 0.000454303261676614 is not very close to the backpropagation gradient of 0.0004792987373159371!\n",
      "Parameter id: 144 Numerical gradient of 0.0001270095140171179 is not very close to the backpropagation gradient of 0.00013708364485373262!\n",
      "Parameter id: 145 Numerical gradient of -0.02350364347591949 is not very close to the backpropagation gradient of -0.023546401769863643!\n",
      "Parameter id: 146 Numerical gradient of 0.007035483307049616 is not very close to the backpropagation gradient of 0.007084576459248064!\n",
      "Parameter id: 147 Numerical gradient of 0.005028644167737184 is not very close to the backpropagation gradient of 0.005044969902180759!\n",
      "Parameter id: 148 Numerical gradient of -0.003906652779051001 is not very close to the backpropagation gradient of -0.003919650075108112!\n",
      "Parameter id: 149 Numerical gradient of 0.006080913550476907 is not very close to the backpropagation gradient of 0.006114665330847449!\n",
      "Parameter id: 150 Numerical gradient of 0.04739209025217406 is not very close to the backpropagation gradient of 0.04744461246801601!\n",
      "Parameter id: 151 Numerical gradient of -0.0037529979124428787 is not very close to the backpropagation gradient of -0.003798049533635496!\n",
      "Parameter id: 152 Numerical gradient of 0.02831868073371879 is not very close to the backpropagation gradient of 0.02833168086925699!\n",
      "Parameter id: 153 Numerical gradient of 0.01885558376102381 is not very close to the backpropagation gradient of 0.018961055507859693!\n",
      "Parameter id: 154 Numerical gradient of -0.013173462320992257 is not very close to the backpropagation gradient of -0.013235087464136396!\n",
      "Parameter id: 155 Numerical gradient of -0.001810551708558705 is not very close to the backpropagation gradient of -0.00183659577446311!\n",
      "Parameter id: 156 Numerical gradient of -0.005193179219986632 is not very close to the backpropagation gradient of -0.005198749826024712!\n",
      "Parameter id: 157 Numerical gradient of 0.009422240765388779 is not very close to the backpropagation gradient of 0.009404334442168809!\n",
      "Parameter id: 158 Numerical gradient of 0.023593571540914127 is not very close to the backpropagation gradient of 0.023561334841976325!\n",
      "Parameter id: 159 Numerical gradient of -0.0029041213878144845 is not very close to the backpropagation gradient of -0.0029181224910915515!\n",
      "Parameter id: 160 Numerical gradient of 0.015318857293777908 is not very close to the backpropagation gradient of 0.015410952232133547!\n",
      "Parameter id: 161 Numerical gradient of -0.00790190135546709 is not very close to the backpropagation gradient of -0.00791430591391848!\n",
      "Parameter id: 162 Numerical gradient of -0.002575717417130363 is not very close to the backpropagation gradient of -0.0025601903691433473!\n",
      "Parameter id: 163 Numerical gradient of -0.005808020731024044 is not very close to the backpropagation gradient of -0.005882068617319418!\n",
      "Parameter id: 164 Numerical gradient of -0.0014699352846037073 is not very close to the backpropagation gradient of -0.0014682561274412937!\n",
      "Parameter id: 165 Numerical gradient of 0.0062514438070593314 is not very close to the backpropagation gradient of 0.0062521884064097154!\n",
      "Parameter id: 166 Numerical gradient of 0.0021675994332781556 is not very close to the backpropagation gradient of 0.002188402474648993!\n",
      "Parameter id: 167 Numerical gradient of -0.0035944580645264064 is not very close to the backpropagation gradient of -0.0036457467321237376!\n",
      "Parameter id: 168 Numerical gradient of -0.009318101845678939 is not very close to the backpropagation gradient of -0.009360321382241032!\n",
      "Parameter id: 169 Numerical gradient of 0.011208589612010655 is not very close to the backpropagation gradient of 0.011286301958531553!\n",
      "Parameter id: 170 Numerical gradient of 0.0005657696533489798 is not very close to the backpropagation gradient of 0.0005560396381562553!\n",
      "Parameter id: 171 Numerical gradient of 0.008028466780274357 is not very close to the backpropagation gradient of 0.008056433685002223!\n",
      "Parameter id: 172 Numerical gradient of -0.006651124095924388 is not very close to the backpropagation gradient of -0.0066752590813415!\n",
      "Parameter id: 173 Numerical gradient of 0.000539124300757976 is not very close to the backpropagation gradient of 0.0005280468857697931!\n",
      "Parameter id: 174 Numerical gradient of 0.00010413891970983968 is not very close to the backpropagation gradient of 9.814267758132027e-05!\n",
      "Parameter id: 175 Numerical gradient of 0.011961542867311437 is not very close to the backpropagation gradient of 0.012030979731782427!\n",
      "Parameter id: 176 Numerical gradient of -0.007247757949357946 is not very close to the backpropagation gradient of -0.007303952616410736!\n",
      "Parameter id: 177 Numerical gradient of -0.003756328581516754 is not very close to the backpropagation gradient of -0.0037839718777589477!\n",
      "Parameter id: 178 Numerical gradient of 0.0017621459846850482 is not very close to the backpropagation gradient of 0.0017621042696826087!\n",
      "Parameter id: 179 Numerical gradient of -0.0030333513478808527 is not very close to the backpropagation gradient of -0.003079817682337351!\n",
      "Parameter id: 180 Numerical gradient of 0.056353810506948314 is not very close to the backpropagation gradient of 0.05631315574501891!\n",
      "Parameter id: 181 Numerical gradient of -0.0071371797361052805 is not very close to the backpropagation gradient of -0.007186460708909142!\n",
      "Parameter id: 182 Numerical gradient of 0.023562707340829547 is not very close to the backpropagation gradient of 0.02348244645906943!\n",
      "Parameter id: 183 Numerical gradient of 0.019234613901630837 is not very close to the backpropagation gradient of 0.01931615117365284!\n",
      "Parameter id: 184 Numerical gradient of -0.017433166021874058 is not very close to the backpropagation gradient of -0.017480134233109026!\n",
      "Parameter id: 185 Numerical gradient of 0.0017219559111936178 is not very close to the backpropagation gradient of 0.001692945380045665!\n",
      "Parameter id: 186 Numerical gradient of -0.002418731881448366 is not very close to the backpropagation gradient of -0.0024235911977233135!\n",
      "Parameter id: 187 Numerical gradient of 0.010067280342695994 is not very close to the backpropagation gradient of 0.01002967134605184!\n",
      "Parameter id: 188 Numerical gradient of 0.027434721161512243 is not very close to the backpropagation gradient of 0.027364961089092366!\n",
      "Parameter id: 189 Numerical gradient of -0.0051598725292478775 is not very close to the backpropagation gradient of -0.0051559425035486535!\n",
      "Parameter id: 190 Numerical gradient of 0.019265034012505566 is not very close to the backpropagation gradient of 0.01933375654666902!\n",
      "Parameter id: 191 Numerical gradient of -0.008742784274318183 is not very close to the backpropagation gradient of -0.00872928825426967!\n",
      "Parameter id: 192 Numerical gradient of -0.011221690243701232 is not very close to the backpropagation gradient of -0.011175808052069556!\n",
      "Parameter id: 193 Numerical gradient of -0.007857714479087008 is not very close to the backpropagation gradient of -0.007926455535166833!\n",
      "Parameter id: 194 Numerical gradient of -0.0006232792060245629 is not very close to the backpropagation gradient of -0.0006196749762091807!\n",
      "Parameter id: 195 Numerical gradient of 0.009780842802342704 is not very close to the backpropagation gradient of 0.009759486816914291!\n",
      "Parameter id: 196 Numerical gradient of 0.003368860745922575 is not very close to the backpropagation gradient of 0.0033793469880663667!\n",
      "Parameter id: 197 Numerical gradient of -0.004767963801555197 is not very close to the backpropagation gradient of -0.004805741038499138!\n",
      "Parameter id: 198 Numerical gradient of -0.012119194536808209 is not very close to the backpropagation gradient of -0.012142171791248144!\n",
      "Parameter id: 199 Numerical gradient of 0.002523536934972981 is not very close to the backpropagation gradient of 0.0025740850837049725!\n",
      "Parameter id: 200 Numerical gradient of -3.863576125695545e-05 is not very close to the backpropagation gradient of -3.932523983273678e-05!\n",
      "Parameter id: 201 Numerical gradient of 0.009956480084838404 is not very close to the backpropagation gradient of 0.009962495853738183!\n",
      "Parameter id: 202 Numerical gradient of -0.009908518450174597 is not very close to the backpropagation gradient of -0.009917643372305197!\n",
      "Parameter id: 203 Numerical gradient of -0.003699485162655946 is not very close to the backpropagation gradient of -0.0037067192411738064!\n",
      "Parameter id: 204 Numerical gradient of 0.0005853095785823825 is not very close to the backpropagation gradient of 0.0005747391403653133!\n",
      "Parameter id: 205 Numerical gradient of 0.012759571177411999 is not very close to the backpropagation gradient of 0.012810721956199013!\n",
      "Parameter id: 206 Numerical gradient of 0.0004498623695781134 is not very close to the backpropagation gradient of 0.00040296400958362384!\n",
      "Parameter id: 207 Numerical gradient of -0.003014033467252375 is not very close to the backpropagation gradient of -0.0030334104271623323!\n",
      "Parameter id: 208 Numerical gradient of 0.004191091917959966 is not very close to the backpropagation gradient of 0.00419141928435142!\n",
      "Parameter id: 209 Numerical gradient of -0.0010840217612440028 is not very close to the backpropagation gradient of -0.0011278183882436118!\n",
      "Parameter id: 210 Numerical gradient of -0.06238254357526784 is not very close to the backpropagation gradient of -0.06228081631101938!\n",
      "Parameter id: 211 Numerical gradient of 0.009140688206343839 is not very close to the backpropagation gradient of 0.009195610014910963!\n",
      "Parameter id: 212 Numerical gradient of -0.026569413336119396 is not very close to the backpropagation gradient of -0.02639256666074321!\n",
      "Parameter id: 213 Numerical gradient of -0.023635093882035108 is not very close to the backpropagation gradient of -0.02370090850967763!\n",
      "Parameter id: 214 Numerical gradient of 0.01968714080646805 is not very close to the backpropagation gradient of 0.019735101711646504!\n",
      "Parameter id: 215 Numerical gradient of -0.00415023571065376 is not very close to the backpropagation gradient of -0.004120189096855958!\n",
      "Parameter id: 216 Numerical gradient of 0.001503241975342462 is not very close to the backpropagation gradient of 0.0015007894682898364!\n",
      "Parameter id: 217 Numerical gradient of -0.008022693620546306 is not very close to the backpropagation gradient of -0.007971768370797473!\n",
      "Parameter id: 218 Numerical gradient of -0.03862754560657322 is not very close to the backpropagation gradient of -0.038515891033199204!\n",
      "Parameter id: 219 Numerical gradient of 0.006415756814703855 is not very close to the backpropagation gradient of 0.006405802037837267!\n",
      "Parameter id: 220 Numerical gradient of -0.020295098934752787 is not very close to the backpropagation gradient of -0.02036099533286384!\n",
      "Parameter id: 221 Numerical gradient of 0.007005063196174887 is not very close to the backpropagation gradient of 0.006984468107971215!\n",
      "Parameter id: 222 Numerical gradient of 0.012297718399167934 is not very close to the backpropagation gradient of 0.012259141203445567!\n",
      "Parameter id: 223 Numerical gradient of 0.011992407067396016 is not very close to the backpropagation gradient of 0.012054980830291875!\n",
      "Parameter id: 224 Numerical gradient of 0.00021382895454280515 is not very close to the backpropagation gradient of 0.0002083395915069941!\n",
      "Parameter id: 225 Numerical gradient of -0.013249179531271693 is not very close to the backpropagation gradient of -0.013213651890504025!\n",
      "Parameter id: 226 Numerical gradient of -0.004233502437500647 is not very close to the backpropagation gradient of -0.004238815449190779!\n",
      "Parameter id: 227 Numerical gradient of 0.004870992498240412 is not very close to the backpropagation gradient of 0.004907152928013327!\n",
      "Parameter id: 228 Numerical gradient of 0.01305178187749334 is not very close to the backpropagation gradient of 0.013067019233905177!\n",
      "Parameter id: 229 Numerical gradient of -0.0013935519405094965 is not very close to the backpropagation gradient of -0.001415618392112722!\n",
      "Parameter id: 230 Numerical gradient of 0.0011655121312514893 is not very close to the backpropagation gradient of 0.0011597059807547695!\n",
      "Parameter id: 231 Numerical gradient of -0.012955858608165727 is not very close to the backpropagation gradient of -0.012949152229486501!\n",
      "Parameter id: 232 Numerical gradient of 0.011252998532995662 is not very close to the backpropagation gradient of 0.011257451858425239!\n",
      "Parameter id: 233 Numerical gradient of 0.006380007633310925 is not very close to the backpropagation gradient of 0.006391693944730283!\n",
      "Parameter id: 234 Numerical gradient of -0.00019406698470447736 is not very close to the backpropagation gradient of -0.00018285960599547868!\n",
      "Parameter id: 235 Numerical gradient of -0.017567947097063552 is not very close to the backpropagation gradient of -0.017608433800984525!\n",
      "Parameter id: 236 Numerical gradient of -0.0027708946248594657 is not very close to the backpropagation gradient of -0.002734783986562628!\n",
      "Parameter id: 237 Numerical gradient of 0.002992495140574647 is not very close to the backpropagation gradient of 0.0030070382255947517!\n",
      "Parameter id: 238 Numerical gradient of -0.005170974759494129 is not very close to the backpropagation gradient of -0.005175253915472014!\n",
      "Parameter id: 239 Numerical gradient of 0.0031026292646174625 is not very close to the backpropagation gradient of 0.0031341180506800357!\n",
      "Parameter id: 240 Numerical gradient of 0.0466491290040949 is not very close to the backpropagation gradient of 0.04654128881250584!\n",
      "Parameter id: 241 Numerical gradient of -0.004949596288383873 is not very close to the backpropagation gradient of -0.004974922351297107!\n",
      "Parameter id: 242 Numerical gradient of 0.018034462812011043 is not very close to the backpropagation gradient of 0.017937961792900897!\n",
      "Parameter id: 243 Numerical gradient of 0.015642598327758606 is not very close to the backpropagation gradient of 0.015661340802766224!\n",
      "Parameter id: 244 Numerical gradient of -0.014401813075437529 is not very close to the backpropagation gradient of -0.014425683127390836!\n",
      "Parameter id: 245 Numerical gradient of -0.00039124259387790516 is not very close to the backpropagation gradient of -0.0004088541581830877!\n",
      "Parameter id: 246 Numerical gradient of -0.003185229857649574 is not very close to the backpropagation gradient of -0.0031824220030965103!\n",
      "Parameter id: 247 Numerical gradient of 0.010873302258573858 is not very close to the backpropagation gradient of 0.01082955079505689!\n",
      "Parameter id: 248 Numerical gradient of 0.018110402066895404 is not very close to the backpropagation gradient of 0.017989186150821727!\n",
      "Parameter id: 249 Numerical gradient of -0.0030033753262159735 is not very close to the backpropagation gradient of -0.002994595148441535!\n",
      "Parameter id: 250 Numerical gradient of 0.017638113192219862 is not very close to the backpropagation gradient of 0.017665681114678745!\n",
      "Parameter id: 251 Numerical gradient of -0.013826051414866924 is not very close to the backpropagation gradient of -0.013804833313123988!\n",
      "Parameter id: 252 Numerical gradient of -0.013917977881305887 is not very close to the backpropagation gradient of -0.013885364510471489!\n",
      "Parameter id: 253 Numerical gradient of -0.0036961544935820707 is not very close to the backpropagation gradient of -0.0037202330261384203!\n",
      "Parameter id: 254 Numerical gradient of -0.001056266185628374 is not very close to the backpropagation gradient of -0.0010544681749124775!\n",
      "Parameter id: 255 Numerical gradient of 0.006827205467629938 is not very close to the backpropagation gradient of 0.00680009551734908!\n",
      "Parameter id: 256 Numerical gradient of 0.002042366276100438 is not very close to the backpropagation gradient of 0.0020436874591067303!\n",
      "Parameter id: 257 Numerical gradient of -0.0035962344213658066 is not very close to the backpropagation gradient of -0.0036234450786929223!\n",
      "Parameter id: 258 Numerical gradient of -0.014819701021906438 is not very close to the backpropagation gradient of -0.014826540970894616!\n",
      "Parameter id: 259 Numerical gradient of 0.001212585587495596 is not very close to the backpropagation gradient of 0.00121419195332417!\n",
      "Parameter id: 260 Numerical gradient of 0.0009892087149410145 is not very close to the backpropagation gradient of 0.0009881118180541416!\n",
      "Parameter id: 261 Numerical gradient of 0.006324496482079667 is not very close to the backpropagation gradient of 0.006313513366388744!\n",
      "Parameter id: 262 Numerical gradient of -0.008604006396240038 is not very close to the backpropagation gradient of -0.008602392351929477!\n",
      "Parameter id: 263 Numerical gradient of -0.0006588063428125679 is not very close to the backpropagation gradient of -0.0006775140340322474!\n",
      "Parameter id: 264 Numerical gradient of 0.0006823430709346212 is not very close to the backpropagation gradient of 0.0006745674631710057!\n",
      "Parameter id: 265 Numerical gradient of 0.005597300400950189 is not very close to the backpropagation gradient of 0.0056083979539977005!\n",
      "Parameter id: 266 Numerical gradient of -0.0008568701304056958 is not very close to the backpropagation gradient of -0.0008756660066728023!\n",
      "Parameter id: 267 Numerical gradient of -0.001688427175849938 is not very close to the backpropagation gradient of -0.0016957090303353147!\n",
      "Parameter id: 268 Numerical gradient of 0.003992361996552063 is not very close to the backpropagation gradient of 0.003992820987248709!\n",
      "Parameter id: 269 Numerical gradient of 0.001839195462594034 is not very close to the backpropagation gradient of 0.001821566748765068!\n",
      "Parameter id: 270 Numerical gradient of 0.06715517031352647 is not very close to the backpropagation gradient of 0.06706490752092187!\n",
      "Parameter id: 271 Numerical gradient of -0.006683320563638517 is not very close to the backpropagation gradient of -0.00673379850038048!\n",
      "Parameter id: 272 Numerical gradient of 0.04228328798205894 is not very close to the backpropagation gradient of 0.042151023544288076!\n",
      "Parameter id: 273 Numerical gradient of 0.030119906568870643 is not very close to the backpropagation gradient of 0.030179652852128203!\n",
      "Parameter id: 274 Numerical gradient of -0.016544543512964083 is not very close to the backpropagation gradient of -0.016598204502925207!\n",
      "Parameter id: 275 Numerical gradient of -0.001781463865313526 is not very close to the backpropagation gradient of -0.0018013152779263813!\n",
      "Parameter id: 276 Numerical gradient of -0.004249711693660174 is not very close to the backpropagation gradient of -0.0042459123409957075!\n",
      "Parameter id: 277 Numerical gradient of 0.011143308498162696 is not very close to the backpropagation gradient of 0.011087960628111217!\n",
      "Parameter id: 278 Numerical gradient of 0.040664360767550534 is not very close to the backpropagation gradient of 0.04050355797907536!\n",
      "Parameter id: 279 Numerical gradient of -0.006200373547926574 is not very close to the backpropagation gradient of -0.006193392968932924!\n",
      "Parameter id: 280 Numerical gradient of 0.022852608694279297 is not very close to the backpropagation gradient of 0.022918362321054035!\n",
      "Parameter id: 281 Numerical gradient of -0.013483214544862676 is not very close to the backpropagation gradient of -0.013485104874414103!\n",
      "Parameter id: 282 Numerical gradient of -0.005712319506301355 is not very close to the backpropagation gradient of -0.005709710374945255!\n",
      "Parameter id: 283 Numerical gradient of -0.013192336112410885 is not very close to the backpropagation gradient of -0.013252785064992941!\n",
      "Parameter id: 284 Numerical gradient of -0.0008413270080609436 is not very close to the backpropagation gradient of -0.0008388838138170536!\n",
      "Parameter id: 285 Numerical gradient of 0.010454526133685249 is not very close to the backpropagation gradient of 0.010428661304543597!\n",
      "Parameter id: 286 Numerical gradient of 0.004482192395016682 is not very close to the backpropagation gradient of 0.0044932080174373236!\n",
      "Parameter id: 287 Numerical gradient of -0.0054989346409684 is not very close to the backpropagation gradient of -0.005546181957023648!\n",
      "Parameter id: 288 Numerical gradient of -0.015243362128103398 is not very close to the backpropagation gradient of -0.015276209452399051!\n",
      "Parameter id: 289 Numerical gradient of 0.01608979616207762 is not very close to the backpropagation gradient of 0.016117736063000626!\n",
      "Parameter id: 290 Numerical gradient of -0.0017936763185844027 is not very close to the backpropagation gradient of -0.0017883919148983778!\n",
      "Parameter id: 291 Numerical gradient of 0.011729950344374629 is not very close to the backpropagation gradient of 0.011736575832013272!\n",
      "Parameter id: 292 Numerical gradient of -0.010160317032159583 is not very close to the backpropagation gradient of -0.01017586059484217!\n",
      "Parameter id: 293 Numerical gradient of 0.0015587531265737198 is not very close to the backpropagation gradient of 0.0015330299305206807!\n",
      "Parameter id: 294 Numerical gradient of -0.00013655743202889425 is not very close to the backpropagation gradient of -0.000142640641299228!\n",
      "Parameter id: 295 Numerical gradient of 0.017048140676934054 is not very close to the backpropagation gradient of 0.017091297673585734!\n",
      "Parameter id: 296 Numerical gradient of -0.008377298854611581 is not very close to the backpropagation gradient of -0.008423107342369557!\n",
      "Parameter id: 297 Numerical gradient of -0.00489341900333784 is not very close to the backpropagation gradient of -0.004907472511497543!\n",
      "Parameter id: 298 Numerical gradient of 0.0018141044222375056 is not very close to the backpropagation gradient of 0.001829747770013087!\n",
      "Parameter id: 299 Numerical gradient of -0.006552092202127824 is not very close to the backpropagation gradient of -0.006578292908030524!\n",
      "Parameter id: 300 Numerical gradient of -0.05518074885912938 is not very close to the backpropagation gradient of -0.05505535102579075!\n",
      "Parameter id: 301 Numerical gradient of 0.006428191312579656 is not very close to the backpropagation gradient of 0.006468787855403717!\n",
      "Parameter id: 302 Numerical gradient of -0.023254953518403454 is not very close to the backpropagation gradient of -0.023097996303852706!\n",
      "Parameter id: 303 Numerical gradient of -0.018897328146749715 is not very close to the backpropagation gradient of -0.018929348093931233!\n",
      "Parameter id: 304 Numerical gradient of 0.017391643680753077 is not very close to the backpropagation gradient of 0.017424634105484994!\n",
      "Parameter id: 305 Numerical gradient of -0.0014435119766176285 is not very close to the backpropagation gradient of -0.0014205242406363225!\n",
      "Parameter id: 306 Numerical gradient of 0.002958522316021117 is not very close to the backpropagation gradient of 0.0029543578492555217!\n",
      "Parameter id: 307 Numerical gradient of -0.010168532682541809 is not very close to the backpropagation gradient of -0.010118137683859501!\n",
      "Parameter id: 308 Numerical gradient of -0.025951907289822884 is not very close to the backpropagation gradient of -0.025807998099061306!\n",
      "Parameter id: 309 Numerical gradient of 0.00457700544131967 is not very close to the backpropagation gradient of 0.004565735776641816!\n",
      "Parameter id: 310 Numerical gradient of -0.018215429165024943 is not very close to the backpropagation gradient of -0.018256655643187!\n",
      "Parameter id: 311 Numerical gradient of 0.008821832153671494 is not very close to the backpropagation gradient of 0.008788231434911496!\n",
      "Parameter id: 312 Numerical gradient of 0.010044853837598566 is not very close to the backpropagation gradient of 0.010005662640226657!\n",
      "Parameter id: 313 Numerical gradient of 0.006943556840610654 is not very close to the backpropagation gradient of 0.006983345435719551!\n",
      "Parameter id: 314 Numerical gradient of 0.0010387246618392965 is not very close to the backpropagation gradient of 0.0010350383921407442!\n",
      "Parameter id: 315 Numerical gradient of -0.00907762753854513 is not very close to the backpropagation gradient of -0.009042978382344656!\n",
      "Parameter id: 316 Numerical gradient of -0.003091304989766286 is not very close to the backpropagation gradient of -0.0030943914340241956!\n",
      "Parameter id: 317 Numerical gradient of 0.004388489571738319 is not very close to the backpropagation gradient of 0.004421677676337419!\n",
      "Parameter id: 318 Numerical gradient of 0.011415535183800785 is not very close to the backpropagation gradient of 0.011417479962332423!\n",
      "Parameter id: 319 Numerical gradient of -0.0027780000522170667 is not very close to the backpropagation gradient of -0.0027846106810768484!\n",
      "Parameter id: 320 Numerical gradient of -0.0005810907310888069 is not very close to the backpropagation gradient of -0.0005837694901905778!\n",
      "Parameter id: 321 Numerical gradient of -0.009631628827833083 is not very close to the backpropagation gradient of -0.00961895635425241!\n",
      "Parameter id: 322 Numerical gradient of 0.009238609877115778 is not very close to the backpropagation gradient of 0.009235690847672742!\n",
      "Parameter id: 323 Numerical gradient of 0.0030337954370907028 is not very close to the backpropagation gradient of 0.0030550491106626472!\n",
      "Parameter id: 324 Numerical gradient of -0.0005349054532644004 is not very close to the backpropagation gradient of -0.0005264348829750305!\n",
      "Parameter id: 325 Numerical gradient of -0.012893242029576868 is not very close to the backpropagation gradient of -0.01291315258154224!\n",
      "Parameter id: 326 Numerical gradient of 0.00042943426592501055 is not very close to the backpropagation gradient of 0.00045076475670677513!\n",
      "Parameter id: 327 Numerical gradient of 0.0031514790777009694 is not very close to the backpropagation gradient of 0.003162404552880859!\n",
      "Parameter id: 328 Numerical gradient of -0.004069189429856124 is not very close to the backpropagation gradient of -0.004069928479669619!\n",
      "Parameter id: 329 Numerical gradient of 0.0008912870441690756 is not very close to the backpropagation gradient of 0.0009153981282087938!\n",
      "Parameter id: 330 Numerical gradient of 0.003312683460876542 is not very close to the backpropagation gradient of 0.003469338371149433!\n",
      "Parameter id: 331 Numerical gradient of 0.002490896378049001 is not very close to the backpropagation gradient of 0.0024853951825783596!\n",
      "Parameter id: 332 Numerical gradient of 0.028177238320381544 is not very close to the backpropagation gradient of 0.028262326612393484!\n",
      "Parameter id: 333 Numerical gradient of 0.012363887691435593 is not very close to the backpropagation gradient of 0.012430299955425592!\n",
      "Parameter id: 334 Numerical gradient of 0.007892575482060238 is not very close to the backpropagation gradient of 0.00784233604113782!\n",
      "Parameter id: 335 Numerical gradient of -0.004121147867408581 is not very close to the backpropagation gradient of -0.004114950960140607!\n",
      "Parameter id: 336 Numerical gradient of -0.0025339730314044573 is not very close to the backpropagation gradient of -0.002541955152000068!\n",
      "Parameter id: 337 Numerical gradient of 0.001340039190722564 is not very close to the backpropagation gradient of 0.0013579715149287055!\n",
      "Parameter id: 338 Numerical gradient of 0.020229595776299902 is not very close to the backpropagation gradient of 0.020238965892528667!\n",
      "Parameter id: 339 Numerical gradient of -0.0011550760348200129 is not very close to the backpropagation gradient of -0.0011725929422245064!\n",
      "Parameter id: 340 Numerical gradient of 0.0013318235403403378 is not very close to the backpropagation gradient of 0.0013876592798561863!\n",
      "Parameter id: 341 Numerical gradient of -0.010222267476933666 is not very close to the backpropagation gradient of -0.010267095466186456!\n",
      "Parameter id: 342 Numerical gradient of 0.013839818180372276 is not very close to the backpropagation gradient of 0.013788491916540866!\n",
      "Parameter id: 343 Numerical gradient of -0.010807355010911124 is not very close to the backpropagation gradient of -0.010846427540073122!\n",
      "Parameter id: 344 Numerical gradient of 0.0003581579477440755 is not very close to the backpropagation gradient of 0.0003534542643094565!\n",
      "Parameter id: 345 Numerical gradient of -0.0021516122217235534 is not very close to the backpropagation gradient of -0.0021194014367695944!\n",
      "Parameter id: 346 Numerical gradient of 0.002987166070056446 is not very close to the backpropagation gradient of 0.0030071928218719443!\n",
      "Parameter id: 347 Numerical gradient of -0.00033373304120232206 is not very close to the backpropagation gradient of -0.00036084732980372044!\n",
      "Parameter id: 348 Numerical gradient of -0.007083222897108498 is not very close to the backpropagation gradient of -0.0071342530268323255!\n",
      "Parameter id: 349 Numerical gradient of 0.019100498960256118 is not very close to the backpropagation gradient of 0.01915169365706884!\n",
      "Parameter id: 350 Numerical gradient of -0.002930322651195638 is not very close to the backpropagation gradient of -0.002928665975743614!\n",
      "Parameter id: 351 Numerical gradient of 0.00048228088189716795 is not very close to the backpropagation gradient of 0.0005318708897058275!\n",
      "Parameter id: 352 Numerical gradient of 0.0024094060080415147 is not very close to the backpropagation gradient of 0.0023840044921333813!\n",
      "Parameter id: 353 Numerical gradient of 0.007644995747568827 is not very close to the backpropagation gradient of 0.007600926521987765!\n",
      "Parameter id: 354 Numerical gradient of -0.0012387868508767497 is not very close to the backpropagation gradient of -0.001231690536574846!\n",
      "Parameter id: 355 Numerical gradient of -0.0019806378759312793 is not very close to the backpropagation gradient of -0.0019397788460566428!\n",
      "Parameter id: 356 Numerical gradient of -0.00904143426794235 is not very close to the backpropagation gradient of -0.009065240219060374!\n",
      "Parameter id: 357 Numerical gradient of -0.003383293645242702 is not very close to the backpropagation gradient of -0.0034051441522124047!\n",
      "Parameter id: 358 Numerical gradient of -0.004727995772668692 is not very close to the backpropagation gradient of -0.004706225161357121!\n",
      "Parameter id: 359 Numerical gradient of -0.013897105688442934 is not very close to the backpropagation gradient of -0.013930881349454444!\n",
      "Parameter id: 360 Numerical gradient of -0.08609424284600209 is not very close to the backpropagation gradient of -0.0859342621849282!\n",
      "Parameter id: 361 Numerical gradient of 0.009994893801490434 is not very close to the backpropagation gradient of 0.010055950437247815!\n",
      "Parameter id: 362 Numerical gradient of -0.05109179745943493 is not very close to the backpropagation gradient of -0.0509221243029964!\n",
      "Parameter id: 363 Numerical gradient of -0.03832378858703578 is not very close to the backpropagation gradient of -0.038386584986583594!\n",
      "Parameter id: 364 Numerical gradient of 0.022352120154778277 is not very close to the backpropagation gradient of 0.02240625468392108!\n",
      "Parameter id: 365 Numerical gradient of 0.0012612133559741778 is not very close to the backpropagation gradient of 0.001282371387203642!\n",
      "Parameter id: 366 Numerical gradient of 0.0051316728644223986 is not very close to the backpropagation gradient of 0.0051273360019582735!\n",
      "Parameter id: 367 Numerical gradient of -0.01343747335624812 is not very close to the backpropagation gradient of -0.013362961194014427!\n",
      "Parameter id: 368 Numerical gradient of -0.05401901148616162 is not very close to the backpropagation gradient of -0.053822289492462524!\n",
      "Parameter id: 369 Numerical gradient of 0.00801181343490498 is not very close to the backpropagation gradient of 0.00800177413517322!\n",
      "Parameter id: 370 Numerical gradient of -0.029163116366248684 is not very close to the backpropagation gradient of -0.029237997533477587!\n",
      "Parameter id: 371 Numerical gradient of 0.015784262785700776 is not very close to the backpropagation gradient of 0.015783853645609254!\n",
      "Parameter id: 372 Numerical gradient of 0.012758238909782449 is not very close to the backpropagation gradient of 0.01272638068974834!\n",
      "Parameter id: 373 Numerical gradient of 0.016385781620442685 is not very close to the backpropagation gradient of 0.0164597378045201!\n",
      "Parameter id: 374 Numerical gradient of 0.0005728750807065808 is not very close to the backpropagation gradient of 0.0005687547730840519!\n",
      "Parameter id: 375 Numerical gradient of -0.01644373526232812 is not very close to the backpropagation gradient of -0.016396011569556196!\n",
      "Parameter id: 376 Numerical gradient of -0.005611067166455541 is not very close to the backpropagation gradient of -0.005624368628110892!\n",
      "Parameter id: 377 Numerical gradient of 0.006811218256075335 is not very close to the backpropagation gradient of 0.006867894753787731!\n",
      "Parameter id: 378 Numerical gradient of 0.01940736460426251 is not very close to the backpropagation gradient of 0.01943813709526315!\n",
      "Parameter id: 379 Numerical gradient of -0.018607337892717624 is not very close to the backpropagation gradient of -0.018643362120531467!\n",
      "Parameter id: 380 Numerical gradient of 0.002857936109990078 is not very close to the backpropagation gradient of 0.0028506617733604156!\n",
      "Parameter id: 381 Numerical gradient of -0.01662558979376172 is not very close to the backpropagation gradient of -0.016626653363566377!\n",
      "Parameter id: 382 Numerical gradient of 0.014900303213494224 is not very close to the backpropagation gradient of 0.014912515716660003!\n",
      "Parameter id: 383 Numerical gradient of -0.0016047163597932013 is not very close to the backpropagation gradient of -0.0015791962503159728!\n",
      "Parameter id: 384 Numerical gradient of 0.00019606538614880265 is not very close to the backpropagation gradient of 0.00020672899022955785!\n",
      "Parameter id: 385 Numerical gradient of -0.024804158726965397 is not very close to the backpropagation gradient of -0.02485131999602632!\n",
      "Parameter id: 386 Numerical gradient of 0.009769962616701378 is not very close to the backpropagation gradient of 0.009827929239753432!\n",
      "Parameter id: 387 Numerical gradient of 0.005515810030942703 is not very close to the backpropagation gradient of 0.005532540866528157!\n",
      "Parameter id: 388 Numerical gradient of -0.003713696017371148 is not very close to the backpropagation gradient of -0.0037286158363261804!\n",
      "Parameter id: 389 Numerical gradient of 0.006858291712319442 is not very close to the backpropagation gradient of 0.0068936880886483545!\n",
      "Parameter id: 390 Numerical gradient of 0.053893556284378974 is not very close to the backpropagation gradient of 0.05382176294357724!\n",
      "Parameter id: 391 Numerical gradient of -0.007365219545363288 is not very close to the backpropagation gradient of -0.0074144110270412495!\n",
      "Parameter id: 392 Numerical gradient of 0.01680611205756577 is not very close to the backpropagation gradient of 0.016671563357705026!\n",
      "Parameter id: 393 Numerical gradient of 0.01708277963530236 is not very close to the backpropagation gradient of 0.0171534485054263!\n",
      "Parameter id: 394 Numerical gradient of -0.01901057089526148 is not very close to the backpropagation gradient of -0.019056338641767808!\n",
      "Parameter id: 395 Numerical gradient of 0.0040805137047073 is not very close to the backpropagation gradient of 0.004048300913046506!\n",
      "Parameter id: 396 Numerical gradient of -0.0015687451337953462 is not very close to the backpropagation gradient of -0.0015706683376523877!\n",
      "Parameter id: 397 Numerical gradient of 0.009106715381790309 is not very close to the backpropagation gradient of 0.009065997149225067!\n",
      "Parameter id: 398 Numerical gradient of 0.025007107495866876 is not very close to the backpropagation gradient of 0.024938850923838143!\n",
      "Parameter id: 399 Numerical gradient of -0.004700462241657988 is not very close to the backpropagation gradient of -0.004692647323499289!\n",
      "Parameter id: 400 Numerical gradient of 0.01681454975255292 is not very close to the backpropagation gradient of 0.016876743601957882!\n",
      "Parameter id: 401 Numerical gradient of -0.0063582472620282715 is not very close to the backpropagation gradient of -0.006333386751201822!\n",
      "Parameter id: 402 Numerical gradient of -0.012500889212674338 is not very close to the backpropagation gradient of -0.012453357846736058!\n",
      "Parameter id: 403 Numerical gradient of -0.007104539179181301 is not very close to the backpropagation gradient of -0.007164259891074136!\n",
      "Parameter id: 404 Numerical gradient of -0.0008057998712729386 is not very close to the backpropagation gradient of -0.0008021799202231555!\n",
      "Parameter id: 405 Numerical gradient of 0.0102873265461767 is not very close to the backpropagation gradient of 0.01025923482728978!\n",
      "Parameter id: 406 Numerical gradient of 0.0030893065883219606 is not very close to the backpropagation gradient of 0.003096338078781303!\n",
      "Parameter id: 407 Numerical gradient of -0.004050759727647346 is not very close to the backpropagation gradient of -0.00408187063023357!\n",
      "Parameter id: 408 Numerical gradient of -0.009927836330803075 is not very close to the backpropagation gradient of -0.009939368751939428!\n",
      "Parameter id: 409 Numerical gradient of -0.003284261751446138 is not very close to the backpropagation gradient of -0.003257357654904359!\n",
      "Parameter id: 410 Numerical gradient of 0.0009423573033018328 is not very close to the backpropagation gradient of 0.0009423363338195334!\n",
      "Parameter id: 411 Numerical gradient of 0.010343059742012883 is not very close to the backpropagation gradient of 0.010333572524966195!\n",
      "Parameter id: 412 Numerical gradient of -0.009251266419596504 is not very close to the backpropagation gradient of -0.009253604022917247!\n",
      "Parameter id: 413 Numerical gradient of -0.006116440687264912 is not very close to the backpropagation gradient of -0.006122224289710454!\n",
      "Parameter id: 414 Numerical gradient of 0.0005795364188543317 is not very close to the backpropagation gradient of 0.0005708249426940573!\n",
      "Parameter id: 415 Numerical gradient of 0.013602230453102493 is not very close to the backpropagation gradient of 0.013641178015615225!\n",
      "Parameter id: 416 Numerical gradient of 0.004271472064942827 is not very close to the backpropagation gradient of 0.004240564159842921!\n",
      "Parameter id: 417 Numerical gradient of -0.0024769075679387242 is not very close to the backpropagation gradient of -0.0024932082184261992!\n",
      "Parameter id: 418 Numerical gradient of 0.005474287689821722 is not very close to the backpropagation gradient of 0.005470371243585253!\n",
      "Parameter id: 419 Numerical gradient of -1.354472090042691e-05 is not very close to the backpropagation gradient of -4.695285721539096e-05!\n",
      "Parameter id: 420 Numerical gradient of -0.063273608574832 is not very close to the backpropagation gradient of -0.06321111229958229!\n",
      "Parameter id: 421 Numerical gradient of 0.005680345083192151 is not very close to the backpropagation gradient of 0.005727478240539435!\n",
      "Parameter id: 422 Numerical gradient of -0.037656766593840985 is not very close to the backpropagation gradient of -0.03758134448806904!\n",
      "Parameter id: 423 Numerical gradient of -0.025660806812766168 is not very close to the backpropagation gradient of -0.025739844966761145!\n",
      "Parameter id: 424 Numerical gradient of 0.01688604811533878 is not very close to the backpropagation gradient of 0.01693508533860936!\n",
      "Parameter id: 425 Numerical gradient of 0.001775246616375625 is not very close to the backpropagation gradient of 0.0018005971262530112!\n",
      "Parameter id: 426 Numerical gradient of 0.005724531959572232 is not very close to the backpropagation gradient of 0.005721487051069689!\n",
      "Parameter id: 427 Numerical gradient of -0.012278622563144381 is not very close to the backpropagation gradient of -0.01222967671572554!\n",
      "Parameter id: 428 Numerical gradient of -0.03337996545837996 is not very close to the backpropagation gradient of -0.03325758872489186!\n",
      "Parameter id: 429 Numerical gradient of 0.004699129974028438 is not very close to the backpropagation gradient of 0.004700969032830741!\n",
      "Parameter id: 430 Numerical gradient of -0.020935475575356577 is not very close to the backpropagation gradient of -0.021007559260072653!\n",
      "Parameter id: 431 Numerical gradient of 0.011959322421262186 is not very close to the backpropagation gradient of 0.011964567727995331!\n",
      "Parameter id: 432 Numerical gradient of 0.005789368984210341 is not very close to the backpropagation gradient of 0.005768321920132062!\n",
      "Parameter id: 433 Numerical gradient of 0.009244605081448753 is not very close to the backpropagation gradient of 0.009315886462613703!\n",
      "Parameter id: 434 Numerical gradient of 0.0014992451724538114 is not very close to the backpropagation gradient of 0.0014956849005890365!\n",
      "Parameter id: 435 Numerical gradient of -0.009205969320191798 is not very close to the backpropagation gradient of -0.009184810345483727!\n",
      "Parameter id: 436 Numerical gradient of -0.0036388669855114126 is not very close to the backpropagation gradient of -0.0036555492691909826!\n",
      "Parameter id: 437 Numerical gradient of 0.0048905324234738146 is not very close to the backpropagation gradient of 0.0049415022589226355!\n",
      "Parameter id: 438 Numerical gradient of 0.013695933276380856 is not very close to the backpropagation gradient of 0.01372966442353541!\n",
      "Parameter id: 439 Numerical gradient of -0.01441113894884438 is not very close to the backpropagation gradient of -0.014465846144016492!\n",
      "Parameter id: 440 Numerical gradient of -1.887379141862766e-05 is not very close to the backpropagation gradient of -1.2728172548575164e-05!\n",
      "Parameter id: 441 Numerical gradient of -0.010864198429771932 is not very close to the backpropagation gradient of -0.010874616571081741!\n",
      "Parameter id: 442 Numerical gradient of 0.009557465929788123 is not very close to the backpropagation gradient of 0.009572713275665716!\n",
      "Parameter id: 443 Numerical gradient of -0.0015798473640415978 is not very close to the backpropagation gradient of -0.00156313101012021!\n",
      "Parameter id: 444 Numerical gradient of -0.00016076029396572267 is not very close to the backpropagation gradient of -0.00015206511825088324!\n",
      "Parameter id: 445 Numerical gradient of -0.015726309143815342 is not very close to the backpropagation gradient of -0.015776265069790615!\n",
      "Parameter id: 446 Numerical gradient of 0.008468781231840694 is not very close to the backpropagation gradient of 0.008521350281087251!\n",
      "Parameter id: 447 Numerical gradient of 0.004810152276490953 is not very close to the backpropagation gradient of 0.004830111130934415!\n",
      "Parameter id: 448 Numerical gradient of -0.002339684002095055 is not very close to the backpropagation gradient of -0.002343583598121708!\n",
      "Parameter id: 449 Numerical gradient of 0.004454436819401053 is not very close to the backpropagation gradient of 0.0044977703147050145!\n",
      "Parameter id: 450 Numerical gradient of -0.05970335337224241 is not very close to the backpropagation gradient of -0.059696544281403745!\n",
      "Parameter id: 451 Numerical gradient of 0.006105116412413736 is not very close to the backpropagation gradient of 0.006158340019028012!\n",
      "Parameter id: 452 Numerical gradient of -0.039959591191518484 is not very close to the backpropagation gradient of -0.03986351934152507!\n",
      "Parameter id: 453 Numerical gradient of -0.02761235684545227 is not very close to the backpropagation gradient of -0.027688436795379967!\n",
      "Parameter id: 454 Numerical gradient of 0.01376099234562389 is not very close to the backpropagation gradient of 0.01383268831607421!\n",
      "Parameter id: 455 Numerical gradient of 0.0016053824936079764 is not very close to the backpropagation gradient of 0.001620127329801816!\n",
      "Parameter id: 456 Numerical gradient of 0.0033997249460071544 is not very close to the backpropagation gradient of 0.003397053118398992!\n",
      "Parameter id: 457 Numerical gradient of -0.010039968856290216 is not very close to the backpropagation gradient of -0.009998618461396654!\n",
      "Parameter id: 458 Numerical gradient of -0.03814260018941695 is not very close to the backpropagation gradient of -0.03802237038044964!\n",
      "Parameter id: 459 Numerical gradient of 0.006034728272652501 is not very close to the backpropagation gradient of 0.006032900766270909!\n",
      "Parameter id: 460 Numerical gradient of -0.021426638241450746 is not very close to the backpropagation gradient of -0.021504527175212052!\n",
      "Parameter id: 461 Numerical gradient of 0.013350431871117507 is not very close to the backpropagation gradient of 0.013363463434198378!\n",
      "Parameter id: 462 Numerical gradient of 0.004303890577261882 is not very close to the backpropagation gradient of 0.004325738739791206!\n",
      "Parameter id: 463 Numerical gradient of 0.013247403174432293 is not very close to the backpropagation gradient of 0.013307776772902945!\n",
      "Parameter id: 464 Numerical gradient of 0.00029243274468626623 is not very close to the backpropagation gradient of 0.00029235864071727713!\n",
      "Parameter id: 465 Numerical gradient of -0.009177547610761394 is not very close to the backpropagation gradient of -0.009169600153694777!\n",
      "Parameter id: 466 Numerical gradient of -0.004580780199603396 is not very close to the backpropagation gradient of -0.004592759403240188!\n",
      "Parameter id: 467 Numerical gradient of 0.00549671419491915 is not very close to the backpropagation gradient of 0.005541879687515968!\n",
      "Parameter id: 468 Numerical gradient of 0.015201395697772567 is not very close to the backpropagation gradient of 0.015242573270333449!\n",
      "Parameter id: 469 Numerical gradient of -0.015635048811191155 is not very close to the backpropagation gradient of -0.01566113466470058!\n",
      "Parameter id: 470 Numerical gradient of 0.0023434587603787804 is not very close to the backpropagation gradient of 0.002337006478525601!\n",
      "Parameter id: 471 Numerical gradient of -0.010477174683387602 is not very close to the backpropagation gradient of -0.010496888683314493!\n",
      "Parameter id: 472 Numerical gradient of 0.009076961404730355 is not very close to the backpropagation gradient of 0.009105837388944837!\n",
      "Parameter id: 473 Numerical gradient of -0.0014059864383852982 is not very close to the backpropagation gradient of -0.0013789997038339001!\n",
      "Parameter id: 474 Numerical gradient of 0.00020028423364237824 is not very close to the backpropagation gradient of 0.0002051425490146539!\n",
      "Parameter id: 475 Numerical gradient of -0.013818279853694548 is not very close to the backpropagation gradient of -0.013879762115150625!\n",
      "Parameter id: 476 Numerical gradient of 0.0075344175343161615 is not very close to the backpropagation gradient of 0.007577540525518411!\n",
      "Parameter id: 477 Numerical gradient of 0.004477973547523106 is not very close to the backpropagation gradient of 0.004491890628477841!\n",
      "Parameter id: 478 Numerical gradient of -0.0011826095658307167 is not very close to the backpropagation gradient of -0.0012056848184552776!\n",
      "Parameter id: 479 Numerical gradient of 0.006777245431521806 is not very close to the backpropagation gradient of 0.006796775519713578!\n",
      "Parameter id: 480 Numerical gradient of -0.0775235431405008 is not very close to the backpropagation gradient of -0.07737275702211387!\n",
      "Parameter id: 481 Numerical gradient of 0.009954259638789154 is not very close to the backpropagation gradient of 0.010013489313628084!\n",
      "Parameter id: 482 Numerical gradient of -0.04278133403090578 is not very close to the backpropagation gradient of -0.04259486286983954!\n",
      "Parameter id: 483 Numerical gradient of -0.032944758032726895 is not very close to the backpropagation gradient of -0.03300379676759088!\n",
      "Parameter id: 484 Numerical gradient of 0.021717738718507462 is not very close to the backpropagation gradient of 0.021769744054519275!\n",
      "Parameter id: 485 Numerical gradient of -0.0005229150445984487 is not very close to the backpropagation gradient of -0.0004993885023304894!\n",
      "Parameter id: 486 Numerical gradient of 0.003453237695794087 is not very close to the backpropagation gradient of 0.0034478840528232445!\n",
      "Parameter id: 487 Numerical gradient of -0.011263212584822213 is not very close to the backpropagation gradient of -0.011193064899490814!\n",
      "Parameter id: 488 Numerical gradient of -0.049816595293350474 is not very close to the backpropagation gradient of -0.049641187478755504!\n",
      "Parameter id: 489 Numerical gradient of 0.0075066619587005326 is not very close to the backpropagation gradient of 0.00749632431540206!\n",
      "Parameter id: 490 Numerical gradient of -0.02610445193340638 is not very close to the backpropagation gradient of -0.026175910379731104!\n",
      "Parameter id: 491 Numerical gradient of 0.011972645097557688 is not very close to the backpropagation gradient of 0.011955919144994429!\n",
      "Parameter id: 492 Numerical gradient of 0.012653211811652909 is not very close to the backpropagation gradient of 0.012616043716918029!\n",
      "Parameter id: 493 Numerical gradient of 0.015004220088599139 is not very close to the backpropagation gradient of 0.015073509348996282!\n",
      "Parameter id: 494 Numerical gradient of 0.0001156852391659413 is not very close to the backpropagation gradient of 0.00010985949694296824!\n",
      "Parameter id: 495 Numerical gradient of -0.015246470752572348 is not very close to the backpropagation gradient of -0.015200669022626784!\n",
      "Parameter id: 496 Numerical gradient of -0.005089484389486643 is not very close to the backpropagation gradient of -0.0051003385289351616!\n",
      "Parameter id: 497 Numerical gradient of 0.006179279310458696 is not very close to the backpropagation gradient of 0.006228399862382679!\n",
      "Parameter id: 498 Numerical gradient of 0.017254864204119258 is not very close to the backpropagation gradient of 0.01727400562032111!\n",
      "Parameter id: 499 Numerical gradient of -0.01190958442975898 is not very close to the backpropagation gradient of -0.011935648589891393!\n",
      "Parameter id: 500 Numerical gradient of 0.0022533086507792177 is not very close to the backpropagation gradient of 0.0022459991247334122!\n",
      "Parameter id: 501 Numerical gradient of -0.01574962382733247 is not very close to the backpropagation gradient of -0.015746044465641866!\n",
      "Parameter id: 502 Numerical gradient of 0.013582912572474015 is not very close to the backpropagation gradient of 0.013588793016482258!\n",
      "Parameter id: 503 Numerical gradient of 0.0015216716775512396 is not very close to the backpropagation gradient of 0.0015462298249770703!\n",
      "Parameter id: 504 Numerical gradient of 2.6423307986078726e-05 is not very close to the backpropagation gradient of 3.586711363638673e-05!\n",
      "Parameter id: 505 Numerical gradient of -0.0219895213149357 is not very close to the backpropagation gradient of -0.022029473059957387!\n",
      "Parameter id: 506 Numerical gradient of 0.004597655589577698 is not very close to the backpropagation gradient of 0.004641399401619794!\n",
      "Parameter id: 507 Numerical gradient of 0.004531042208100189 is not very close to the backpropagation gradient of 0.004546285863776202!\n",
      "Parameter id: 508 Numerical gradient of -0.004358735594678365 is not very close to the backpropagation gradient of -0.0043678989348885835!\n",
      "Parameter id: 509 Numerical gradient of 0.005585976126099013 is not very close to the backpropagation gradient of 0.005620547225825854!\n",
      "Parameter id: 510 Numerical gradient of -0.07704437088307259 is not very close to the backpropagation gradient of -0.07689621787980847!\n",
      "Parameter id: 511 Numerical gradient of 0.007093436948935049 is not very close to the backpropagation gradient of 0.007143053608467164!\n",
      "Parameter id: 512 Numerical gradient of -0.05027067651042216 is not very close to the backpropagation gradient of -0.05011849329828324!\n",
      "Parameter id: 513 Numerical gradient of -0.035178748802877635 is not very close to the backpropagation gradient of -0.0352264774493406!\n",
      "Parameter id: 514 Numerical gradient of 0.01815081418499176 is not very close to the backpropagation gradient of 0.018198423996338864!\n",
      "Parameter id: 515 Numerical gradient of 0.002957190048391567 is not very close to the backpropagation gradient of 0.002976099445127586!\n",
      "Parameter id: 516 Numerical gradient of 0.005583977724654687 is not very close to the backpropagation gradient of 0.005578291407856858!\n",
      "Parameter id: 517 Numerical gradient of -0.012979617380892705 is not very close to the backpropagation gradient of -0.012910768253503702!\n",
      "Parameter id: 518 Numerical gradient of -0.047169157468829326 is not very close to the backpropagation gradient of -0.04695789124389183!\n",
      "Parameter id: 519 Numerical gradient of 0.006933564833389028 is not very close to the backpropagation gradient of 0.006923706292785298!\n",
      "Parameter id: 520 Numerical gradient of -0.02612732252771366 is not very close to the backpropagation gradient of -0.026188803706964876!\n",
      "Parameter id: 521 Numerical gradient of 0.016565859795036886 is not very close to the backpropagation gradient of 0.016564631104744373!\n",
      "Parameter id: 522 Numerical gradient of 0.006414202502469379 is not very close to the backpropagation gradient of 0.006399299405007735!\n",
      "Parameter id: 523 Numerical gradient of 0.015110801498963154 is not very close to the backpropagation gradient of 0.015172859812453818!\n",
      "Parameter id: 524 Numerical gradient of 0.001184607967275042 is not very close to the backpropagation gradient of 0.0011818560717664543!\n",
      "Parameter id: 525 Numerical gradient of -0.012127410187190435 is not very close to the backpropagation gradient of -0.012085726073262866!\n",
      "Parameter id: 526 Numerical gradient of -0.00524780219279819 is not very close to the backpropagation gradient of -0.005261178752574551!\n",
      "Parameter id: 527 Numerical gradient of 0.005799138946827043 is not very close to the backpropagation gradient of 0.005851548107984999!\n",
      "Parameter id: 528 Numerical gradient of 0.017292611786956513 is not very close to the backpropagation gradient of 0.01732406017565113!\n",
      "Parameter id: 529 Numerical gradient of -0.021361357127602787 is not very close to the backpropagation gradient of -0.02139139235053779!\n",
      "Parameter id: 530 Numerical gradient of 0.002368105711525459 is not very close to the backpropagation gradient of 0.0023620416905476543!\n",
      "Parameter id: 531 Numerical gradient of -0.013164580536795256 is not very close to the backpropagation gradient of -0.013167857868875471!\n",
      "Parameter id: 532 Numerical gradient of 0.012052359110725774 is not very close to the backpropagation gradient of 0.01206256234925161!\n",
      "Parameter id: 533 Numerical gradient of -0.0038433700666473665 is not very close to the backpropagation gradient of -0.0038115694786371133!\n",
      "Parameter id: 534 Numerical gradient of 0.0002149391775674303 is not very close to the backpropagation gradient of 0.00022085749836081835!\n",
      "Parameter id: 535 Numerical gradient of -0.020051516003150027 is not very close to the backpropagation gradient of -0.020085693136071125!\n",
      "Parameter id: 536 Numerical gradient of 0.011556755552533104 is not very close to the backpropagation gradient of 0.011606264571742048!\n",
      "Parameter id: 537 Numerical gradient of 0.005892397680895556 is not very close to the backpropagation gradient of 0.005906647068519039!\n",
      "Parameter id: 538 Numerical gradient of -0.0014228618283596006 is not very close to the backpropagation gradient of -0.0014389520136372795!\n",
      "Parameter id: 539 Numerical gradient of 0.008640865800657593 is not very close to the backpropagation gradient of 0.008671585325149726!\n",
      "Parameter id: 540 Numerical gradient of 0.05761102706003384 is not very close to the backpropagation gradient of 0.0576067862695147!\n",
      "Parameter id: 541 Numerical gradient of -0.004559019828320743 is not very close to the backpropagation gradient of -0.004609416735820795!\n",
      "Parameter id: 542 Numerical gradient of 0.0460760318787834 is not very close to the backpropagation gradient of 0.045981676202561314!\n",
      "Parameter id: 543 Numerical gradient of 0.030458968680591166 is not very close to the backpropagation gradient of 0.03053193456132129!\n",
      "Parameter id: 544 Numerical gradient of -0.010323741861384406 is not very close to the backpropagation gradient of -0.010400584384690497!\n",
      "Parameter id: 545 Numerical gradient of -0.003541167359344399 is not very close to the backpropagation gradient of -0.003553823649682862!\n",
      "Parameter id: 546 Numerical gradient of -0.004207301174119493 is not very close to the backpropagation gradient of -0.0042038601871340646!\n",
      "Parameter id: 547 Numerical gradient of 0.00939448518977315 is not very close to the backpropagation gradient of 0.009355747889149153!\n",
      "Parameter id: 548 Numerical gradient of 0.041067815814699316 is not very close to the backpropagation gradient of 0.04093376284504027!\n",
      "Parameter id: 549 Numerical gradient of -0.005702549543684654 is not very close to the backpropagation gradient of -0.005704583217559898!\n",
      "Parameter id: 550 Numerical gradient of 0.019841683851495873 is not very close to the backpropagation gradient of 0.019923690826229593!\n",
      "Parameter id: 551 Numerical gradient of -0.01511324398961733 is not very close to the backpropagation gradient of -0.015127999572176759!\n",
      "Parameter id: 552 Numerical gradient of 0.0008808509477375991 is not very close to the backpropagation gradient of 0.0008313237477047212!\n",
      "Parameter id: 553 Numerical gradient of -0.016383117085183585 is not very close to the backpropagation gradient of -0.016433888341759198!\n",
      "Parameter id: 554 Numerical gradient of -0.0003999023334699814 is not very close to the backpropagation gradient of -0.00040056212606886035!\n",
      "Parameter id: 555 Numerical gradient of 0.008028022691064507 is not very close to the backpropagation gradient of 0.008023595947433087!\n",
      "Parameter id: 556 Numerical gradient of 0.0049962256554181295 is not very close to the backpropagation gradient of 0.005008762586265076!\n",
      "Parameter id: 557 Numerical gradient of -0.0044573233992650785 is not very close to the backpropagation gradient of -0.004505998971986232!\n",
      "Parameter id: 558 Numerical gradient of -0.014542367310355074 is not very close to the backpropagation gradient of -0.014591891382490738!\n",
      "Parameter id: 559 Numerical gradient of 0.02195066350907382 is not very close to the backpropagation gradient of 0.02197840745054513!\n",
      "Parameter id: 560 Numerical gradient of -0.003090860900556436 is not very close to the backpropagation gradient of -0.0030841041758889946!\n",
      "Parameter id: 561 Numerical gradient of 0.009621858865216382 is not very close to the backpropagation gradient of 0.009648423552983938!\n",
      "Parameter id: 562 Numerical gradient of -0.007033040816395441 is not very close to the backpropagation gradient of -0.007068744469943778!\n",
      "Parameter id: 563 Numerical gradient of 0.003985700658404312 is not very close to the backpropagation gradient of 0.003952948603286561!\n",
      "Parameter id: 564 Numerical gradient of -0.0007263079027097774 is not very close to the backpropagation gradient of -0.0007256901878006418!\n",
      "Parameter id: 565 Numerical gradient of 0.012645662295085458 is not very close to the backpropagation gradient of 0.012711978304304383!\n",
      "Parameter id: 566 Numerical gradient of -0.010540235351186311 is not very close to the backpropagation gradient of -0.010587319000322809!\n",
      "Parameter id: 567 Numerical gradient of -0.0056208371290722425 is not very close to the backpropagation gradient of -0.005630690032001501!\n",
      "Parameter id: 568 Numerical gradient of -0.0008444356325298941 is not very close to the backpropagation gradient of -0.0008140591381923964!\n",
      "Parameter id: 569 Numerical gradient of -0.012094325541056605 is not very close to the backpropagation gradient of -0.012099990005856634!\n",
      "Parameter id: 570 Numerical gradient of 0.08763301195813256 is not very close to the backpropagation gradient of 0.0874537465727953!\n",
      "Parameter id: 571 Numerical gradient of -0.009662270983312737 is not very close to the backpropagation gradient of -0.009722849209923345!\n",
      "Parameter id: 572 Numerical gradient of 0.05161959748534173 is not very close to the backpropagation gradient of 0.0514437847528812!\n",
      "Parameter id: 573 Numerical gradient of 0.03829159211932165 is not very close to the backpropagation gradient of 0.03834602009231132!\n",
      "Parameter id: 574 Numerical gradient of -0.022712054459361752 is not very close to the backpropagation gradient of -0.022764688476815273!\n",
      "Parameter id: 575 Numerical gradient of -0.0008641976023682219 is not very close to the backpropagation gradient of -0.0008843936827398584!\n",
      "Parameter id: 576 Numerical gradient of -0.006162403920484394 is not very close to the backpropagation gradient of -0.006155712802156828!\n",
      "Parameter id: 577 Numerical gradient of 0.014569012662946077 is not very close to the backpropagation gradient of 0.014489866336841621!\n",
      "Parameter id: 578 Numerical gradient of 0.05400901947893999 is not very close to the backpropagation gradient of 0.05379882390808015!\n",
      "Parameter id: 579 Numerical gradient of -0.007685407865665183 is not very close to the backpropagation gradient of -0.007674869523339817!\n",
      "Parameter id: 580 Numerical gradient of 0.028864022283414666 is not very close to the backpropagation gradient of 0.028936388403484142!\n",
      "Parameter id: 581 Numerical gradient of -0.017432499888059283 is not very close to the backpropagation gradient of -0.017426993423305826!\n",
      "Parameter id: 582 Numerical gradient of -0.012845058350308136 is not very close to the backpropagation gradient of -0.012809397559587701!\n",
      "Parameter id: 583 Numerical gradient of -0.015784262785700776 is not very close to the backpropagation gradient of -0.015857254483626872!\n",
      "Parameter id: 584 Numerical gradient of -0.0014133139103478243 is not very close to the backpropagation gradient of -0.0014087594352921117!\n",
      "Parameter id: 585 Numerical gradient of 0.01637490143480136 is not very close to the backpropagation gradient of 0.016320944595526757!\n",
      "Parameter id: 586 Numerical gradient of 0.0056201709952574674 is not very close to the backpropagation gradient of 0.0056339849069790005!\n",
      "Parameter id: 587 Numerical gradient of -0.0063649086001760224 is not very close to the backpropagation gradient of -0.006423444166445137!\n",
      "Parameter id: 588 Numerical gradient of -0.019521939620403828 is not very close to the backpropagation gradient of -0.0195516973694122!\n",
      "Parameter id: 589 Numerical gradient of 0.019237500481494862 is not very close to the backpropagation gradient of 0.019267610023136327!\n",
      "Parameter id: 590 Numerical gradient of -0.0018569590309880366 is not very close to the backpropagation gradient of -0.0018512851189697705!\n",
      "Parameter id: 591 Numerical gradient of 0.01683342354397155 is not very close to the backpropagation gradient of 0.016832771107857556!\n",
      "Parameter id: 592 Numerical gradient of -0.015355938742800388 is not very close to the backpropagation gradient of -0.015364717970329188!\n",
      "Parameter id: 593 Numerical gradient of 0.002873257187729905 is not very close to the backpropagation gradient of 0.0028430667732299085!\n",
      "Parameter id: 594 Numerical gradient of -0.00016542323066914832 is not very close to the backpropagation gradient of -0.00017554559340686367!\n",
      "Parameter id: 595 Numerical gradient of 0.025520696667058473 is not very close to the backpropagation gradient of 0.02555957508778517!\n",
      "Parameter id: 596 Numerical gradient of -0.011108891584399316 is not very close to the backpropagation gradient of -0.011162382940160162!\n",
      "Parameter id: 597 Numerical gradient of -0.00578181946764289 is not very close to the backpropagation gradient of -0.0057977001652762435!\n",
      "Parameter id: 598 Numerical gradient of 0.003985034524589537 is not very close to the backpropagation gradient of 0.003998957065858495!\n",
      "Parameter id: 599 Numerical gradient of -0.006678879671540017 is not very close to the backpropagation gradient of -0.0067136137598082955!\n",
      "Parameter id: 600 Numerical gradient of -0.05812350600820081 is not very close to the backpropagation gradient of -0.058036268151549636!\n",
      "Parameter id: 601 Numerical gradient of 0.006155964626941568 is not very close to the backpropagation gradient of 0.006203373213418462!\n",
      "Parameter id: 602 Numerical gradient of -0.02691247225072857 is not very close to the backpropagation gradient of -0.026775860210739175!\n",
      "Parameter id: 603 Numerical gradient of -0.021071810962780546 is not very close to the backpropagation gradient of -0.021133303687782316!\n",
      "Parameter id: 604 Numerical gradient of 0.017557733045237 is not very close to the backpropagation gradient of 0.01760499056847222!\n",
      "Parameter id: 605 Numerical gradient of -0.001340261235327489 is not very close to the backpropagation gradient of -0.0013113474257225848!\n",
      "Parameter id: 606 Numerical gradient of 0.0032880365097298636 is not very close to the backpropagation gradient of 0.0032869582507293075!\n",
      "Parameter id: 607 Numerical gradient of -0.010029754804463664 is not very close to the backpropagation gradient of -0.009985292019947778!\n",
      "Parameter id: 608 Numerical gradient of -0.02866684667424124 is not very close to the backpropagation gradient of -0.028553380181992424!\n",
      "Parameter id: 609 Numerical gradient of 0.004814593168589454 is not very close to the backpropagation gradient of 0.0048070131073224675!\n",
      "Parameter id: 610 Numerical gradient of -0.01775890545729908 is not very close to the backpropagation gradient of -0.017819935192990497!\n",
      "Parameter id: 611 Numerical gradient of 0.008103073767529168 is not very close to the backpropagation gradient of 0.008087644133716288!\n",
      "Parameter id: 612 Numerical gradient of 0.00819722068001738 is not very close to the backpropagation gradient of 0.008169986013108624!\n",
      "Parameter id: 613 Numerical gradient of 0.007299938431515328 is not very close to the backpropagation gradient of 0.007355417949908532!\n",
      "Parameter id: 614 Numerical gradient of 0.0012612133559741778 is not very close to the backpropagation gradient of 0.0012571391548752966!\n",
      "Parameter id: 615 Numerical gradient of -0.009404921286204626 is not very close to the backpropagation gradient of -0.009379238705472465!\n",
      "Parameter id: 616 Numerical gradient of -0.0029980462556977727 is not very close to the backpropagation gradient of -0.003006346013523092!\n",
      "Parameter id: 617 Numerical gradient of 0.0045428105721612155 is not very close to the backpropagation gradient of 0.004581598546411604!\n",
      "Parameter id: 618 Numerical gradient of 0.010238476733093194 is not very close to the backpropagation gradient of 0.010256773546468997!\n",
      "Parameter id: 619 Numerical gradient of -0.004402922471058446 is not very close to the backpropagation gradient of -0.004425901054927186!\n",
      "Parameter id: 620 Numerical gradient of -0.0004050093593832571 is not very close to the backpropagation gradient of -0.00040663717109519157!\n",
      "Parameter id: 621 Numerical gradient of -0.01010858063921205 is not very close to the backpropagation gradient of -0.01010429911776213!\n",
      "Parameter id: 622 Numerical gradient of 0.008947953489268912 is not very close to the backpropagation gradient of 0.008954163976181269!\n",
      "Parameter id: 623 Numerical gradient of 0.002880362615087506 is not very close to the backpropagation gradient of 0.0028960143849560147!\n",
      "Parameter id: 624 Numerical gradient of -0.0004449773882697627 is not very close to the backpropagation gradient of -0.000435733131490146!\n",
      "Parameter id: 625 Numerical gradient of -0.015202950010007042 is not very close to the backpropagation gradient of -0.015237537006381764!\n",
      "Parameter id: 626 Numerical gradient of 0.0013957723865587468 is not very close to the backpropagation gradient of 0.001430578778131421!\n",
      "Parameter id: 627 Numerical gradient of 0.0037991831902672852 is not very close to the backpropagation gradient of 0.0038144826663435716!\n",
      "Parameter id: 628 Numerical gradient of -0.0038347103270552903 is not very close to the backpropagation gradient of -0.0038370848483052448!\n",
      "Parameter id: 629 Numerical gradient of 0.002665645482125001 is not very close to the backpropagation gradient of 0.002692656392715151!\n",
      "Parameter id: 630 Numerical gradient of -0.06864775414783253 is not very close to the backpropagation gradient of -0.06850601129489081!\n",
      "Parameter id: 631 Numerical gradient of 0.00720690174205174 is not very close to the backpropagation gradient of 0.007260871201020324!\n",
      "Parameter id: 632 Numerical gradient of -0.027168711724812056 is not very close to the backpropagation gradient of -0.027012573628466156!\n",
      "Parameter id: 633 Numerical gradient of -0.02166289370109098 is not very close to the backpropagation gradient of -0.021719593233548648!\n",
      "Parameter id: 634 Numerical gradient of 0.022305712832348945 is not very close to the backpropagation gradient of 0.022351233962581825!\n",
      "Parameter id: 635 Numerical gradient of -0.003261613201743785 is not very close to the backpropagation gradient of -0.003228813734073475!\n",
      "Parameter id: 636 Numerical gradient of 0.005553335569175033 is not very close to the backpropagation gradient of 0.0055513677131980715!\n",
      "Parameter id: 637 Numerical gradient of -0.014656720281891465 is not very close to the backpropagation gradient of -0.014598808671779867!\n",
      "Parameter id: 638 Numerical gradient of -0.030646818416357743 is not very close to the backpropagation gradient of -0.030507833066183862!\n",
      "Parameter id: 639 Numerical gradient of 0.004673372799857134 is not very close to the backpropagation gradient of 0.004665593022868273!\n",
      "Parameter id: 640 Numerical gradient of -0.019558576980216458 is not very close to the backpropagation gradient of -0.019625266188184616!\n",
      "Parameter id: 641 Numerical gradient of 0.01372657543186051 is not very close to the backpropagation gradient of 0.013691461142771207!\n",
      "Parameter id: 642 Numerical gradient of 0.014736656339664476 is not very close to the backpropagation gradient of 0.01468073172824412!\n",
      "Parameter id: 643 Numerical gradient of 0.006515676886920119 is not very close to the backpropagation gradient of 0.006573168143023367!\n",
      "Parameter id: 644 Numerical gradient of 0.0025510704659836847 is not very close to the backpropagation gradient of 0.0025468287130360747!\n",
      "Parameter id: 645 Numerical gradient of -0.012650991365603659 is not very close to the backpropagation gradient of -0.012608201978785264!\n",
      "Parameter id: 646 Numerical gradient of -0.003754108135467504 is not very close to the backpropagation gradient of -0.0037620331378204553!\n",
      "Parameter id: 647 Numerical gradient of 0.0041169290199150055 is not very close to the backpropagation gradient of 0.0041654731801295045!\n",
      "Parameter id: 648 Numerical gradient of 0.013210765814619663 is not very close to the backpropagation gradient of 0.013225514679283349!\n",
      "Parameter id: 649 Numerical gradient of -0.0015862866575844237 is not very close to the backpropagation gradient of -0.0016117364699295485!\n",
      "Parameter id: 650 Numerical gradient of -0.0023352431099965543 is not very close to the backpropagation gradient of -0.0023325612428239197!\n",
      "Parameter id: 651 Numerical gradient of -0.012325696019388488 is not very close to the backpropagation gradient of -0.012315601481675391!\n",
      "Parameter id: 652 Numerical gradient of 0.01252042913790774 is not very close to the backpropagation gradient of 0.012521737050444267!\n",
      "Parameter id: 653 Numerical gradient of 0.002454925152051146 is not very close to the backpropagation gradient of 0.0024763077134608404!\n",
      "Parameter id: 654 Numerical gradient of -0.0006827871601444713 is not very close to the backpropagation gradient of -0.0006705125726798682!\n",
      "Parameter id: 655 Numerical gradient of -0.018595347484051672 is not very close to the backpropagation gradient of -0.018626945769258!\n",
      "Parameter id: 656 Numerical gradient of 0.000985433956657289 is not very close to the backpropagation gradient of 0.001021890487631953!\n",
      "Parameter id: 657 Numerical gradient of 0.004354738791789714 is not very close to the backpropagation gradient of 0.0043714535980038174!\n",
      "Parameter id: 658 Numerical gradient of -0.006352252057695296 is not very close to the backpropagation gradient of -0.00634931046766293!\n",
      "Parameter id: 659 Numerical gradient of 0.0005715428130770306 is not very close to the backpropagation gradient of 0.0006044754942875104!\n",
      "Parameter id: 660 Numerical gradient of 0.07485323472167238 is not very close to the backpropagation gradient of 0.07470447960112268!\n",
      "Parameter id: 661 Numerical gradient of -0.0077540196485870174 is not very close to the backpropagation gradient of -0.007804730228049629!\n",
      "Parameter id: 662 Numerical gradient of 0.04429856481635852 is not very close to the backpropagation gradient of 0.044137699131497045!\n",
      "Parameter id: 663 Numerical gradient of 0.03171751750130625 is not very close to the backpropagation gradient of 0.03176036944478321!\n",
      "Parameter id: 664 Numerical gradient of -0.01926037107580214 is not very close to the backpropagation gradient of -0.019307341640839453!\n",
      "Parameter id: 665 Numerical gradient of -0.001624700374236454 is not very close to the backpropagation gradient of -0.0016424643207374947!\n",
      "Parameter id: 666 Numerical gradient of -0.0051740833839630795 is not very close to the backpropagation gradient of -0.005166719700549231!\n",
      "Parameter id: 667 Numerical gradient of 0.013137713139599327 is not very close to the backpropagation gradient of 0.013070795752188226!\n",
      "Parameter id: 668 Numerical gradient of 0.043299586138800805 is not very close to the backpropagation gradient of 0.04309922320154829!\n",
      "Parameter id: 669 Numerical gradient of -0.006552314246732749 is not very close to the backpropagation gradient of -0.0065426803846720845!\n",
      "Parameter id: 670 Numerical gradient of 0.025458524177679465 is not very close to the backpropagation gradient of 0.0255180499748898!\n",
      "Parameter id: 671 Numerical gradient of -0.015279777443311103 is not very close to the backpropagation gradient of -0.015271105527293345!\n",
      "Parameter id: 672 Numerical gradient of -0.009195533223760322 is not very close to the backpropagation gradient of -0.009173528465859745!\n",
      "Parameter id: 673 Numerical gradient of -0.013247847263642143 is not very close to the backpropagation gradient of -0.013307390045157439!\n",
      "Parameter id: 674 Numerical gradient of -0.0011659562204613394 is not very close to the backpropagation gradient of -0.0011619822086868835!\n",
      "Parameter id: 675 Numerical gradient of 0.012179368624742892 is not very close to the backpropagation gradient of 0.01213762653780053!\n",
      "Parameter id: 676 Numerical gradient of 0.0049258375156568945 is not very close to the backpropagation gradient of 0.004936167192265686!\n",
      "Parameter id: 677 Numerical gradient of -0.005771605415816339 is not very close to the backpropagation gradient of -0.0058224789258892705!\n",
      "Parameter id: 678 Numerical gradient of -0.017008172648047548 is not very close to the backpropagation gradient of -0.01703351670255075!\n",
      "Parameter id: 679 Numerical gradient of 0.01641109470540414 is not very close to the backpropagation gradient of 0.016430675169410523!\n",
      "Parameter id: 680 Numerical gradient of -0.001319833131674386 is not very close to the backpropagation gradient of -0.0013152785997125934!\n",
      "Parameter id: 681 Numerical gradient of 0.01318900544333701 is not very close to the backpropagation gradient of 0.013188564266038847!\n",
      "Parameter id: 682 Numerical gradient of -0.012397194382174348 is not very close to the backpropagation gradient of -0.012405555794415503!\n",
      "Parameter id: 683 Numerical gradient of 0.002070787985530842 is not very close to the backpropagation gradient of 0.00203922568087203!\n",
      "Parameter id: 684 Numerical gradient of 6.816769371198461e-05 is not very close to the backpropagation gradient of 5.92266995091926e-05!\n",
      "Parameter id: 685 Numerical gradient of 0.01890554379713194 is not very close to the backpropagation gradient of 0.018936501047674168!\n",
      "Parameter id: 686 Numerical gradient of -0.008873790591223951 is not very close to the backpropagation gradient of -0.008914601174625527!\n",
      "Parameter id: 687 Numerical gradient of -0.005184963569604406 is not very close to the backpropagation gradient of -0.005198139356289303!\n",
      "Parameter id: 688 Numerical gradient of 0.0027260416146646094 is not very close to the backpropagation gradient of 0.002740023058630279!\n",
      "Parameter id: 689 Numerical gradient of -0.0058519855627992 is not very close to the backpropagation gradient of -0.00588049617334722!\n",
      "Parameter id: 690 Numerical gradient of -0.08394507311493271 is not very close to the backpropagation gradient of -0.08378671074941649!\n",
      "Parameter id: 691 Numerical gradient of 0.008669953643902772 is not very close to the backpropagation gradient of 0.008726420200696488!\n",
      "Parameter id: 692 Numerical gradient of -0.05180100792756548 is not very close to the backpropagation gradient of -0.05164269263690149!\n",
      "Parameter id: 693 Numerical gradient of -0.03792766101184952 is not very close to the backpropagation gradient of -0.037984461997058294!\n",
      "Parameter id: 694 Numerical gradient of 0.020600410266524705 is not very close to the backpropagation gradient of 0.02065257562994569!\n",
      "Parameter id: 695 Numerical gradient of 0.002504219054344503 is not very close to the backpropagation gradient of 0.002523968225663652!\n",
      "Parameter id: 696 Numerical gradient of 0.005758282739520837 is not very close to the backpropagation gradient of 0.005752876353915777!\n",
      "Parameter id: 697 Numerical gradient of -0.013776979557178493 is not very close to the backpropagation gradient of -0.013701655147678904!\n",
      "Parameter id: 698 Numerical gradient of -0.05110112333284178 is not very close to the backpropagation gradient of -0.05089359031976665!\n",
      "Parameter id: 699 Numerical gradient of 0.007579492589115943 is not very close to the backpropagation gradient of 0.007569816372914656!\n",
      "Parameter id: 700 Numerical gradient of -0.028354651959716645 is not very close to the backpropagation gradient of -0.02842383097761408!\n",
      "Parameter id: 701 Numerical gradient of 0.017681189845575318 is not very close to the backpropagation gradient of 0.017678259597360028!\n",
      "Parameter id: 702 Numerical gradient of 0.010021761198686363 is not very close to the backpropagation gradient of 0.009995994954025797!\n",
      "Parameter id: 703 Numerical gradient of 0.016108669953496246 is not very close to the backpropagation gradient of 0.016179431659740444!\n",
      "Parameter id: 704 Numerical gradient of 0.001177280495312516 is not very close to the backpropagation gradient of 0.0011737901262582975!\n",
      "Parameter id: 705 Numerical gradient of -0.014463985564816538 is not very close to the backpropagation gradient of -0.014418154462439138!\n",
      "Parameter id: 706 Numerical gradient of -0.005418998583195389 is not very close to the backpropagation gradient of -0.005433158213798879!\n",
      "Parameter id: 707 Numerical gradient of 0.006228351168147128 is not very close to the backpropagation gradient of 0.006284545464861178!\n",
      "Parameter id: 708 Numerical gradient of 0.019009460672236855 is not very close to the backpropagation gradient of 0.019040956743927685!\n",
      "Parameter id: 709 Numerical gradient of -0.02113420549676448 is not very close to the backpropagation gradient of -0.021168657553466162!\n",
      "Parameter id: 710 Numerical gradient of 0.002439826118916244 is not very close to the backpropagation gradient of 0.002434467519097247!\n",
      "Parameter id: 711 Numerical gradient of -0.014810597193104512 is not very close to the backpropagation gradient of -0.014812748409645372!\n",
      "Parameter id: 712 Numerical gradient of 0.014064749365161331 is not very close to the backpropagation gradient of 0.0140747257152663!\n",
      "Parameter id: 713 Numerical gradient of -0.003931965864012454 is not very close to the backpropagation gradient of -0.003900712387224122!\n",
      "Parameter id: 714 Numerical gradient of 0.00022182256032010625 is not very close to the backpropagation gradient of 0.00022894137188286953!\n",
      "Parameter id: 715 Numerical gradient of -0.023064439247377777 is not very close to the backpropagation gradient of -0.023105963705236956!\n",
      "Parameter id: 716 Numerical gradient of 0.01195110677087996 is not very close to the backpropagation gradient of 0.012005253868061873!\n",
      "Parameter id: 717 Numerical gradient of 0.00581756864903582 is not very close to the backpropagation gradient of 0.0058341317678311634!\n",
      "Parameter id: 718 Numerical gradient of -0.002545297306255634 is not very close to the backpropagation gradient of -0.0025610564888307766!\n",
      "Parameter id: 719 Numerical gradient of 0.007465805751394327 is not very close to the backpropagation gradient of 0.007500916970715942!\n",
      "Parameter id: 720 Numerical gradient of 0.07160871895450782 is not very close to the backpropagation gradient of 0.07152195513095125!\n",
      "Parameter id: 721 Numerical gradient of -0.007531308909847211 is not very close to the backpropagation gradient of -0.007585732572500203!\n",
      "Parameter id: 722 Numerical gradient of 0.04675060338854564 is not very close to the backpropagation gradient of 0.04661333967806387!\n",
      "Parameter id: 723 Numerical gradient of 0.03310085538998919 is not very close to the backpropagation gradient of 0.03316448769990214!\n",
      "Parameter id: 724 Numerical gradient of -0.016957768522729566 is not very close to the backpropagation gradient of -0.017018730755724798!\n",
      "Parameter id: 725 Numerical gradient of -0.002255751141433393 is not very close to the backpropagation gradient of -0.002272443128242707!\n",
      "Parameter id: 726 Numerical gradient of -0.004121591956618431 is not very close to the backpropagation gradient of -0.004118865773817647!\n",
      "Parameter id: 727 Numerical gradient of 0.01124589310563806 is not very close to the backpropagation gradient of 0.011189083470854856!\n",
      "Parameter id: 728 Numerical gradient of 0.04533573516596334 is not very close to the backpropagation gradient of 0.04516524733932321!\n",
      "Parameter id: 729 Numerical gradient of -0.007043254868221992 is not very close to the backpropagation gradient of -0.0070356320150115875!\n",
      "Parameter id: 730 Numerical gradient of 0.024974911028152746 is not very close to the backpropagation gradient of 0.025046213176264216!\n",
      "Parameter id: 731 Numerical gradient of -0.014189538433129199 is not very close to the backpropagation gradient of -0.014197663263291197!\n",
      "Parameter id: 732 Numerical gradient of -0.006090905557698534 is not very close to the backpropagation gradient of -0.0060936603367366445!\n",
      "Parameter id: 733 Numerical gradient of -0.015267342945435301 is not very close to the backpropagation gradient of -0.015331319946612198!\n",
      "Parameter id: 734 Numerical gradient of -0.0004578559753554145 is not very close to the backpropagation gradient of -0.000457053435292946!\n",
      "Parameter id: 735 Numerical gradient of 0.011636247521096266 is not very close to the backpropagation gradient of 0.01161011072549522!\n",
      "Parameter id: 736 Numerical gradient of 0.005052180895859237 is not very close to the backpropagation gradient of 0.005065264982752063!\n",
      "Parameter id: 737 Numerical gradient of -0.006029177157529375 is not very close to the backpropagation gradient of -0.006077785565069547!\n",
      "Parameter id: 738 Numerical gradient of -0.016574963623838812 is not very close to the backpropagation gradient of -0.0166111467846817!\n",
      "Parameter id: 739 Numerical gradient of 0.018696155734687636 is not very close to the backpropagation gradient of 0.018724689680158203!\n",
      "Parameter id: 740 Numerical gradient of -0.002744471316873387 is not very close to the backpropagation gradient of -0.0027372618631308038!\n",
      "Parameter id: 741 Numerical gradient of 0.012779777236460177 is not very close to the backpropagation gradient of 0.012790000571363019!\n",
      "Parameter id: 742 Numerical gradient of -0.011158629575902523 is not very close to the backpropagation gradient of -0.011177916201812878!\n",
      "Parameter id: 743 Numerical gradient of 0.0021522783555383285 is not very close to the backpropagation gradient of 0.0021242179722842773!\n",
      "Parameter id: 744 Numerical gradient of -0.00023048229991218247 is not very close to the backpropagation gradient of -0.00023586127794331227!\n",
      "Parameter id: 745 Numerical gradient of 0.018316237415660908 is not very close to the backpropagation gradient of 0.01836580243579261!\n",
      "Parameter id: 746 Numerical gradient of -0.009425349389857729 is not very close to the backpropagation gradient of -0.009474337657677103!\n",
      "Parameter id: 747 Numerical gradient of -0.005160760707667578 is not very close to the backpropagation gradient of -0.005174671909939398!\n",
      "Parameter id: 748 Numerical gradient of 0.0015698553568199713 is not very close to the backpropagation gradient of 0.0015903571230430251!\n",
      "Parameter id: 749 Numerical gradient of -0.0077544637377968675 is not very close to the backpropagation gradient of -0.007779595469605019!\n",
      "Parameter id: 750 Numerical gradient of -0.08409672958009651 is not very close to the backpropagation gradient of -0.08393220371341897!\n",
      "Parameter id: 751 Numerical gradient of 0.009648060128597535 is not very close to the backpropagation gradient of 0.009708651970856769!\n",
      "Parameter id: 752 Numerical gradient of -0.048828940890643935 is not very close to the backpropagation gradient of -0.04865336285577426!\n",
      "Parameter id: 753 Numerical gradient of -0.03657518732325116 is not very close to the backpropagation gradient of -0.03663197437739157!\n",
      "Parameter id: 754 Numerical gradient of 0.02229794127117657 is not very close to the backpropagation gradient of 0.02235053520231095!\n",
      "Parameter id: 755 Numerical gradient of 0.0008177902799388903 is not very close to the backpropagation gradient of 0.0008385307048496128!\n",
      "Parameter id: 756 Numerical gradient of 0.005448086426440568 is not very close to the backpropagation gradient of 0.005440938701271916!\n",
      "Parameter id: 757 Numerical gradient of -0.013710588220305908 is not very close to the backpropagation gradient of -0.013636152446527513!\n",
      "Parameter id: 758 Numerical gradient of -0.05187650309323999 is not very close to the backpropagation gradient of -0.05167794635823054!\n",
      "Parameter id: 759 Numerical gradient of 0.007496891996083831 is not very close to the backpropagation gradient of 0.007487276190869837!\n",
      "Parameter id: 760 Numerical gradient of -0.027934543567198485 is not very close to the backpropagation gradient of -0.028006285936919985!\n",
      "Parameter id: 761 Numerical gradient of 0.01573408070498772 is not very close to the backpropagation gradient of 0.015726656968465844!\n",
      "Parameter id: 762 Numerical gradient of 0.01240696434479105 is not very close to the backpropagation gradient of 0.012375601106662384!\n",
      "Parameter id: 763 Numerical gradient of 0.015460743796325003 is not very close to the backpropagation gradient of 0.015532215087658058!\n",
      "Parameter id: 764 Numerical gradient of 0.0010671463712697005 is not very close to the backpropagation gradient of 0.0010628621075484987!\n",
      "Parameter id: 765 Numerical gradient of -0.015622170224105501 is not very close to the backpropagation gradient of -0.015574440274689686!\n",
      "Parameter id: 766 Numerical gradient of -0.005427658322787465 is not very close to the backpropagation gradient of -0.0054400225821729055!\n",
      "Parameter id: 767 Numerical gradient of 0.0063296035079929425 is not very close to the backpropagation gradient of 0.0063856969277697705!\n",
      "Parameter id: 768 Numerical gradient of 0.01875810617946172 is not very close to the backpropagation gradient of 0.018785929669797138!\n",
      "Parameter id: 769 Numerical gradient of -0.017323698031646018 is not very close to the backpropagation gradient of -0.01735260909686658!\n",
      "Parameter id: 770 Numerical gradient of 0.001767475055203249 is not very close to the backpropagation gradient of 0.0017623082326490395!\n",
      "Parameter id: 772 Numerical gradient of 0.01451527786855422 is not very close to the backpropagation gradient of 0.014525069914555547!\n",
      "Parameter id: 773 Numerical gradient of -0.0018407497748285093 is not very close to the backpropagation gradient of -0.0018120616892099131!\n",
      "Parameter id: 774 Numerical gradient of 0.00011834977442504167 is not very close to the backpropagation gradient of 0.00012888906871216061!\n",
      "Parameter id: 775 Numerical gradient of -0.02400102339095156 is not very close to the backpropagation gradient of -0.02404367416824577!\n",
      "Parameter id: 776 Numerical gradient of 0.00946465128492946 is not very close to the backpropagation gradient of 0.009515956824763992!\n",
      "Parameter id: 777 Numerical gradient of 0.005433209437910591 is not very close to the backpropagation gradient of 0.005449256043833178!\n",
      "Parameter id: 778 Numerical gradient of -0.003943068094258706 is not very close to the backpropagation gradient of -0.003957583898400227!\n",
      "Parameter id: 779 Numerical gradient of 0.006174394329150346 is not very close to the backpropagation gradient of 0.0062079138488306315!\n",
      "Parameter id: 780 Numerical gradient of -0.0047695181137896725 is not very close to the backpropagation gradient of -0.0047322744707456!\n",
      "Parameter id: 781 Numerical gradient of -0.0027129409829740325 is not very close to the backpropagation gradient of -0.002720044335751768!\n",
      "Parameter id: 782 Numerical gradient of -0.026866509017509088 is not very close to the backpropagation gradient of -0.02684991048616272!\n",
      "Parameter id: 783 Numerical gradient of -0.012571721441645423 is not very close to the backpropagation gradient of -0.012569134130904833!\n",
      "Parameter id: 784 Numerical gradient of -0.0058049121065550935 is not very close to the backpropagation gradient of -0.005797995261191201!\n",
      "Parameter id: 785 Numerical gradient of 0.004554800980827167 is not very close to the backpropagation gradient of 0.004552786289539742!\n",
      "Parameter id: 786 Numerical gradient of 0.003521405389506071 is not very close to the backpropagation gradient of 0.0035153807248692872!\n",
      "Parameter id: 787 Numerical gradient of -0.001291189377639057 is not very close to the backpropagation gradient of -0.0012762280565357842!\n",
      "Parameter id: 788 Numerical gradient of -0.01791833348363525 is not very close to the backpropagation gradient of -0.01782586103845229!\n",
      "Parameter id: 789 Numerical gradient of 0.00023891999489933366 is not very close to the backpropagation gradient of 0.0002416875049517063!\n",
      "Parameter id: 790 Numerical gradient of -0.001468380972369232 is not very close to the backpropagation gradient of -0.0014714364171903895!\n",
      "Parameter id: 791 Numerical gradient of 0.009239942144745328 is not very close to the backpropagation gradient of 0.009256968835032794!\n",
      "Parameter id: 792 Numerical gradient of -0.013452128300173172 is not very close to the backpropagation gradient of -0.013396238497165458!\n",
      "Parameter id: 793 Numerical gradient of 0.009049205829114726 is not very close to the backpropagation gradient of 0.00904462307503333!\n",
      "Parameter id: 794 Numerical gradient of -6.972200594645983e-05 is not very close to the backpropagation gradient of -6.830863321614184e-05!\n",
      "Parameter id: 795 Numerical gradient of 0.002069899807111142 is not very close to the backpropagation gradient of 0.002068525987498844!\n",
      "Parameter id: 796 Numerical gradient of -0.002227107387398064 is not very close to the backpropagation gradient of -0.0022312144661062964!\n",
      "Parameter id: 797 Numerical gradient of -6.084022174945857e-05 is not very close to the backpropagation gradient of -4.649429908444667e-05!\n",
      "Parameter id: 798 Numerical gradient of 0.0057911453410497415 is not very close to the backpropagation gradient of 0.0058177447236269445!\n",
      "Parameter id: 799 Numerical gradient of -0.019253931782259315 is not very close to the backpropagation gradient of -0.019263837726064506!\n",
      "Parameter id: 800 Numerical gradient of 0.00241229258790554 is not very close to the backpropagation gradient of 0.002407940780011496!\n",
      "Parameter id: 801 Numerical gradient of -0.0005209166431541234 is not very close to the backpropagation gradient of -0.0005375374820311531!\n",
      "Parameter id: 802 Numerical gradient of -0.0022448709557920665 is not very close to the backpropagation gradient of -0.0022354364811578145!\n",
      "Parameter id: 803 Numerical gradient of -0.006787459483348357 is not very close to the backpropagation gradient of -0.006765998827618856!\n",
      "Parameter id: 804 Numerical gradient of 0.0012261303083960229 is not very close to the backpropagation gradient of 0.0012184820682633944!\n",
      "Parameter id: 805 Numerical gradient of 0.0006141753772226366 is not very close to the backpropagation gradient of 0.0006101328034204736!\n",
      "Parameter id: 806 Numerical gradient of 0.010279110895794474 is not very close to the backpropagation gradient of 0.010294824377365057!\n",
      "Parameter id: 807 Numerical gradient of 0.0034077185517844555 is not very close to the backpropagation gradient of 0.003406427129905289!\n",
      "Parameter id: 808 Numerical gradient of 0.004414246745909622 is not very close to the backpropagation gradient of 0.004397712804019378!\n",
      "Parameter id: 809 Numerical gradient of 0.012521539360932366 is not very close to the backpropagation gradient of 0.012512438998360697!\n",
      "Parameter id: 810 Numerical gradient of -0.0794386778579792 is not very close to the backpropagation gradient of -0.07931935344021841!\n",
      "Parameter id: 811 Numerical gradient of 0.008198774992251856 is not very close to the backpropagation gradient of 0.008254942245641078!\n",
      "Parameter id: 812 Numerical gradient of -0.051421311653143675 is not very close to the backpropagation gradient of -0.051271652160816555!\n",
      "Parameter id: 813 Numerical gradient of -0.037125413854255385 is not very close to the backpropagation gradient of -0.037185905240470135!\n",
      "Parameter id: 814 Numerical gradient of 0.018735901718969217 is not very close to the backpropagation gradient of 0.018794179716839342!\n",
      "Parameter id: 815 Numerical gradient of 0.002843503210669951 is not very close to the backpropagation gradient of 0.002861163556484433!\n",
      "Parameter id: 816 Numerical gradient of 0.004944711307075522 is not very close to the backpropagation gradient of 0.004940454644528593!\n",
      "Parameter id: 817 Numerical gradient of -0.012526424342240716 is not very close to the backpropagation gradient of -0.012458685840977809!\n",
      "Parameter id: 818 Numerical gradient of -0.04997890989955067 is not very close to the backpropagation gradient of -0.049785989133367306!\n",
      "Parameter id: 819 Numerical gradient of 0.007554845637969264 is not very close to the backpropagation gradient of 0.007546498857130108!\n",
      "Parameter id: 820 Numerical gradient of -0.02741518123627884 is not very close to the backpropagation gradient of -0.027484772878818394!\n",
      "Parameter id: 821 Numerical gradient of 0.016858292539723152 is not very close to the backpropagation gradient of 0.016863181043778423!\n",
      "Parameter id: 822 Numerical gradient of 0.00776911868172192 is not very close to the backpropagation gradient of 0.00776485789103493!\n",
      "Parameter id: 823 Numerical gradient of 0.01663980064847692 is not very close to the backpropagation gradient of 0.016707538753506085!\n",
      "Parameter id: 824 Numerical gradient of 0.0007214229214014267 is not very close to the backpropagation gradient of 0.0007198669147953331!\n",
      "Parameter id: 825 Numerical gradient of -0.013217427152767414 is not very close to the backpropagation gradient of -0.013184683844885805!\n",
      "Parameter id: 826 Numerical gradient of -0.005396572078097961 is not very close to the backpropagation gradient of -0.005409995133773837!\n",
      "Parameter id: 827 Numerical gradient of 0.00627875529346511 is not very close to the backpropagation gradient of 0.0063324364850133805!\n",
      "Parameter id: 828 Numerical gradient of 0.018455015293739052 is not very close to the backpropagation gradient of 0.018490980167856268!\n",
      "Parameter id: 829 Numerical gradient of -0.021224577650968968 is not very close to the backpropagation gradient of -0.0212576700228976!\n",
      "Parameter id: 830 Numerical gradient of 0.0029767299736249697 is not very close to the backpropagation gradient of 0.0029694916752710794!\n",
      "Parameter id: 831 Numerical gradient of -0.014003687098806948 is not very close to the backpropagation gradient of -0.014013421458894771!\n",
      "Parameter id: 832 Numerical gradient of 0.012763345935695725 is not very close to the backpropagation gradient of 0.0127815947243035!\n",
      "Parameter id: 833 Numerical gradient of -0.0030884184099022605 is not very close to the backpropagation gradient of -0.0030595122944222903!\n",
      "Parameter id: 834 Numerical gradient of 0.00031374902675906924 is not very close to the backpropagation gradient of 0.00032203276794417346!\n",
      "Parameter id: 835 Numerical gradient of -0.02078781591308143 is not very close to the backpropagation gradient of -0.020838828791565393!\n",
      "Parameter id: 836 Numerical gradient of 0.011479261985414269 is not very close to the backpropagation gradient of 0.011534922948922862!\n",
      "Parameter id: 837 Numerical gradient of 0.005658584711909498 is not very close to the backpropagation gradient of 0.005673524810711436!\n",
      "Parameter id: 838 Numerical gradient of -0.0018534063173092361 is not very close to the backpropagation gradient of -0.0018743630881815731!\n",
      "Parameter id: 839 Numerical gradient of 0.00816058332020475 is not very close to the backpropagation gradient of 0.008190078779510886!\n",
      "Parameter id: 840 Numerical gradient of 0.003127942349578916 is not very close to the backpropagation gradient of 0.003227481572350602!\n",
      "Parameter id: 841 Numerical gradient of -0.0012230216839270724 is not very close to the backpropagation gradient of -0.00124099448112408!\n",
      "Parameter id: 842 Numerical gradient of 0.0027560176363294886 is not very close to the backpropagation gradient of 0.002748337887930496!\n",
      "Parameter id: 843 Numerical gradient of 0.002668087972779176 is not very close to the backpropagation gradient of 0.0026986752694509084!\n",
      "Parameter id: 844 Numerical gradient of 0.00020028423364237824 is not very close to the backpropagation gradient of 0.00016523186466156138!\n",
      "Parameter id: 845 Numerical gradient of 0.00034794389591752406 is not very close to the backpropagation gradient of 0.0003543259154780983!\n",
      "Parameter id: 846 Numerical gradient of 0.0022668533716796446 is not very close to the backpropagation gradient of 0.0022669614623640516!\n",
      "Parameter id: 847 Numerical gradient of -0.0015754064719430971 is not very close to the backpropagation gradient of -0.001562802389130754!\n",
      "Parameter id: 848 Numerical gradient of 0.006257661055997232 is not very close to the backpropagation gradient of 0.006291007420026829!\n",
      "Parameter id: 849 Numerical gradient of -0.0024071855619922644 is not very close to the backpropagation gradient of -0.0024083671051125994!\n",
      "Parameter id: 850 Numerical gradient of 0.0016084911180769268 is not very close to the backpropagation gradient of 0.0016400033100402772!\n",
      "Parameter id: 851 Numerical gradient of 0.0019217960556261458 is not very close to the backpropagation gradient of 0.001900705701862747!\n",
      "Parameter id: 852 Numerical gradient of 0.001431965657161527 is not very close to the backpropagation gradient of 0.0013970063263475006!\n",
      "Parameter id: 853 Numerical gradient of -0.004404476783292921 is not very close to the backpropagation gradient of -0.004420326874398331!\n",
      "Parameter id: 854 Numerical gradient of 0.0009674483436583613 is not very close to the backpropagation gradient of 0.0009657545892099881!\n",
      "Parameter id: 855 Numerical gradient of 0.0017794654638692007 is not very close to the backpropagation gradient of 0.001793081341400007!\n",
      "Parameter id: 856 Numerical gradient of 0.0013151701949709604 is not very close to the backpropagation gradient of 0.0013176975091858942!\n",
      "Parameter id: 857 Numerical gradient of -0.0010651479698253752 is not very close to the backpropagation gradient of -0.0010678150048670607!\n",
      "Parameter id: 858 Numerical gradient of 0.0015478729409323932 is not very close to the backpropagation gradient of 0.0015267971812079968!\n",
      "Parameter id: 859 Numerical gradient of 0.0016942003355779889 is not very close to the backpropagation gradient of 0.0016873243369860957!\n",
      "Parameter id: 860 Numerical gradient of -0.002255085007618618 is not very close to the backpropagation gradient of -0.002248259400307317!\n",
      "Parameter id: 861 Numerical gradient of 0.0009430234371166079 is not very close to the backpropagation gradient of 0.0009557373634493272!\n",
      "Parameter id: 862 Numerical gradient of -0.0004882760862301438 is not very close to the backpropagation gradient of -0.000508959248813243!\n",
      "Parameter id: 863 Numerical gradient of -0.0009185985305748544 is not very close to the backpropagation gradient of -0.000921700504063316!\n",
      "Parameter id: 864 Numerical gradient of -0.00020095036745715333 is not very close to the backpropagation gradient of -0.00020078681377391338!\n",
      "Parameter id: 865 Numerical gradient of 0.0030091484859440243 is not very close to the backpropagation gradient of 0.0030290266596453143!\n",
      "Parameter id: 866 Numerical gradient of 0.0013913314944602462 is not very close to the backpropagation gradient of 0.0013921156569401717!\n",
      "Parameter id: 867 Numerical gradient of -0.0008451017663446692 is not very close to the backpropagation gradient of -0.0008435359575141117!\n",
      "Parameter id: 868 Numerical gradient of -0.0012172485241990216 is not very close to the backpropagation gradient of -0.0012001781969859435!\n",
      "Parameter id: 869 Numerical gradient of -0.0027990942896849447 is not very close to the backpropagation gradient of -0.002794546434079106!\n",
      "Parameter id: 870 Numerical gradient of -0.07130718238101963 is not very close to the backpropagation gradient of -0.07116728210686363!\n",
      "Parameter id: 871 Numerical gradient of 0.007608802476966047 is not very close to the backpropagation gradient of 0.007662132992924492!\n",
      "Parameter id: 872 Numerical gradient of -0.03678501947490531 is not very close to the backpropagation gradient of -0.03661542868361192!\n",
      "Parameter id: 873 Numerical gradient of -0.027837732119451175 is not very close to the backpropagation gradient of -0.02788625929565231!\n",
      "Parameter id: 874 Numerical gradient of 0.020317525439850215 is not very close to the backpropagation gradient of 0.020366797318011047!\n",
      "Parameter id: 875 Numerical gradient of -0.000674127420552395 is not very close to the backpropagation gradient of -0.0006524219218776912!\n",
      "Parameter id: 876 Numerical gradient of 0.00467847982577041 is not very close to the backpropagation gradient of 0.0046749909712743435!\n",
      "Parameter id: 877 Numerical gradient of -0.012478684752181834 is not very close to the backpropagation gradient of -0.012418166774441967!\n",
      "Parameter id: 878 Numerical gradient of -0.039103387194927564 is not very close to the backpropagation gradient of -0.03893447846061171!\n",
      "Parameter id: 879 Numerical gradient of 0.005928812996103261 is not very close to the backpropagation gradient of 0.005918641212113171!\n",
      "Parameter id: 880 Numerical gradient of -0.022431390078736513 is not very close to the backpropagation gradient of -0.022494230968861762!\n",
      "Parameter id: 881 Numerical gradient of 0.013126166820143226 is not very close to the backpropagation gradient of 0.013109608679645886!\n",
      "Parameter id: 882 Numerical gradient of 0.01074362820929764 is not very close to the backpropagation gradient of 0.010713005170731737!\n",
      "Parameter id: 883 Numerical gradient of 0.010486722601399379 is not very close to the backpropagation gradient of 0.010543888837805606!\n",
      "Parameter id: 884 Numerical gradient of 0.0016544543512964083 is not very close to the backpropagation gradient of 0.001650843271988779!\n",
      "Parameter id: 885 Numerical gradient of -0.012026379891949546 is not very close to the backpropagation gradient of -0.011986701871800528!\n",
      "Parameter id: 886 Numerical gradient of -0.0043089976031751576 is not very close to the backpropagation gradient of -0.0043186144177161665!\n",
      "Parameter id: 887 Numerical gradient of 0.0051310067306076235 is not very close to the backpropagation gradient of 0.005177727246790369!\n",
      "Parameter id: 888 Numerical gradient of 0.014562129280193402 is not very close to the backpropagation gradient of 0.014582303198461672!\n",
      "Parameter id: 889 Numerical gradient of -0.00918709552877317 is not very close to the backpropagation gradient of -0.009203721357311168!\n",
      "Parameter id: 890 Numerical gradient of -8.881784197001252e-06 is not very close to the backpropagation gradient of -1.3245678799884031e-05!\n",
      "Parameter id: 891 Numerical gradient of -0.012744250099672172 is not very close to the backpropagation gradient of -0.012739079279621533!\n",
      "Parameter id: 892 Numerical gradient of 0.011973755320582313 is not very close to the backpropagation gradient of 0.011979440917108891!\n",
      "Parameter id: 893 Numerical gradient of 0.0005611067166455541 is not very close to the backpropagation gradient of 0.0005876813653450314!\n",
      "Parameter id: 894 Numerical gradient of -0.0003383959779057477 is not very close to the backpropagation gradient of -0.00032980842527750745!\n",
      "Parameter id: 895 Numerical gradient of -0.018804291457286126 is not very close to the backpropagation gradient of -0.01883582036313276!\n",
      "Parameter id: 896 Numerical gradient of 0.004531264252705114 is not very close to the backpropagation gradient of 0.004567167690114153!\n",
      "Parameter id: 897 Numerical gradient of 0.004763300864851772 is not very close to the backpropagation gradient of 0.004777876515836795!\n",
      "Parameter id: 898 Numerical gradient of -0.004154676602752261 is not very close to the backpropagation gradient of -0.00416303170558557!\n",
      "Parameter id: 899 Numerical gradient of 0.0041384673465927335 is not very close to the backpropagation gradient of 0.004167299329269164!\n",
      "Parameter id: 900 Numerical gradient of -0.07896883147395783 is not very close to the backpropagation gradient of -0.07881230716591868!\n",
      "Parameter id: 901 Numerical gradient of 0.00989897053216282 is not very close to the backpropagation gradient of 0.009957894501261697!\n",
      "Parameter id: 902 Numerical gradient of -0.04415423582315725 is not very close to the backpropagation gradient of -0.043963114273978865!\n",
      "Parameter id: 903 Numerical gradient of -0.033876013105782476 is not very close to the backpropagation gradient of -0.03393166714768396!\n",
      "Parameter id: 904 Numerical gradient of 0.021861401577893957 is not very close to the backpropagation gradient of 0.021911692721261138!\n",
      "Parameter id: 905 Numerical gradient of 0.00019406698470447736 is not very close to the backpropagation gradient of 0.0002162558739249392!\n",
      "Parameter id: 906 Numerical gradient of 0.0038238301414139637 is not very close to the backpropagation gradient of 0.0038174013524971827!\n",
      "Parameter id: 907 Numerical gradient of -0.011718626069523452 is not very close to the backpropagation gradient of -0.011646771593069188!\n",
      "Parameter id: 908 Numerical gradient of -0.05037104067184828 is not very close to the backpropagation gradient of -0.05018413387060757!\n",
      "Parameter id: 909 Numerical gradient of 0.007645883925988527 is not very close to the backpropagation gradient of 0.007634711614536114!\n",
      "Parameter id: 910 Numerical gradient of -0.026831425969930933 is not very close to the backpropagation gradient of -0.02689949163245993!\n",
      "Parameter id: 911 Numerical gradient of 0.012348122524485916 is not very close to the backpropagation gradient of 0.012335454144108334!\n",
      "Parameter id: 912 Numerical gradient of 0.012492229473082261 is not very close to the backpropagation gradient of 0.012455669482114703!\n",
      "Parameter id: 913 Numerical gradient of 0.015302425993013456 is not very close to the backpropagation gradient of 0.015371836155018607!\n",
      "Parameter id: 914 Numerical gradient of 0.0002084998840246044 is not very close to the backpropagation gradient of 0.00020183549279642415!\n",
      "Parameter id: 915 Numerical gradient of -0.01533662086217191 is not very close to the backpropagation gradient of -0.015288821710640554!\n",
      "Parameter id: 916 Numerical gradient of -0.005263345315142942 is not very close to the backpropagation gradient of -0.005273175897699556!\n",
      "Parameter id: 917 Numerical gradient of 0.006420197706802355 is not very close to the backpropagation gradient of 0.006469993317872958!\n",
      "Parameter id: 918 Numerical gradient of 0.017585710665457555 is not very close to the backpropagation gradient of 0.017606955777080546!\n",
      "Parameter id: 919 Numerical gradient of -0.013349987781907657 is not very close to the backpropagation gradient of -0.013371752611654323!\n",
      "Parameter id: 920 Numerical gradient of 0.002504441098949428 is not very close to the backpropagation gradient of 0.002497089759683588!\n",
      "Parameter id: 921 Numerical gradient of -0.015742740444579795 is not very close to the backpropagation gradient of -0.0157384689708796!\n",
      "Parameter id: 922 Numerical gradient of 0.013643530749618549 is not very close to the backpropagation gradient of 0.013650336301156987!\n",
      "Parameter id: 923 Numerical gradient of 0.001113553693699032 is not very close to the backpropagation gradient of 0.0011400318597146403!\n",
      "Parameter id: 924 Numerical gradient of 6.59472476627343e-05 is not very close to the backpropagation gradient of 7.604757555217589e-05!\n",
      "Parameter id: 925 Numerical gradient of -0.022329249560471 is not very close to the backpropagation gradient of -0.022367698644420127!\n",
      "Parameter id: 926 Numerical gradient of 0.005562883487186809 is not very close to the backpropagation gradient of 0.0056063061620905425!\n",
      "Parameter id: 927 Numerical gradient of 0.004767519712345347 is not very close to the backpropagation gradient of 0.004782313946993472!\n",
      "Parameter id: 928 Numerical gradient of -0.003958833261208383 is not very close to the backpropagation gradient of -0.003970200461601158!\n",
      "Parameter id: 929 Numerical gradient of 0.005902611732722107 is not very close to the backpropagation gradient of 0.005935656903021794!\n",
      "Parameter id: 930 Numerical gradient of 0.07676814739454585 is not very close to the backpropagation gradient of 0.0766448111348897!\n",
      "Parameter id: 931 Numerical gradient of -0.007950529123945671 is not very close to the backpropagation gradient of -0.008005359629342367!\n",
      "Parameter id: 932 Numerical gradient of 0.04774314277256053 is not very close to the backpropagation gradient of 0.04759992495266643!\n",
      "Parameter id: 933 Numerical gradient of 0.03452105268308969 is not very close to the backpropagation gradient of 0.034580686436460076!\n",
      "Parameter id: 934 Numerical gradient of -0.018844259486172632 is not very close to the backpropagation gradient of -0.018897438882087184!\n",
      "Parameter id: 935 Numerical gradient of -0.002354338946020107 is not very close to the backpropagation gradient of -0.0023736786347828822!\n",
      "Parameter id: 936 Numerical gradient of -0.004952704912852823 is not very close to the backpropagation gradient of -0.004947964466644772!\n",
      "Parameter id: 937 Numerical gradient of 0.012652989767047984 is not very close to the backpropagation gradient of 0.012586909590076575!\n",
      "Parameter id: 938 Numerical gradient of 0.046675330267476056 is not very close to the backpropagation gradient of 0.04649035283023588!\n",
      "Parameter id: 939 Numerical gradient of -0.007126521595068879 is not very close to the backpropagation gradient of -0.007118157687244028!\n",
      "Parameter id: 940 Numerical gradient of 0.026433077948695427 is not very close to the backpropagation gradient of 0.026500157024959906!\n",
      "Parameter id: 941 Numerical gradient of -0.01568611907032391 is not very close to the backpropagation gradient of -0.015685186752965055!\n",
      "Parameter id: 942 Numerical gradient of -0.00818967116344993 is not very close to the backpropagation gradient of -0.008174222422916237!\n",
      "Parameter id: 943 Numerical gradient of -0.015220713578401044 is not very close to the backpropagation gradient of -0.015287701266744691!\n",
      "Parameter id: 944 Numerical gradient of -0.0008033573806187633 is not very close to the backpropagation gradient of -0.0008008384625063322!\n",
      "Parameter id: 945 Numerical gradient of 0.012669199023207511 is not very close to the backpropagation gradient of 0.012633699866928621!\n",
      "Parameter id: 946 Numerical gradient of 0.005139444425594775 is not very close to the backpropagation gradient of 0.005152558008264286!\n",
      "Parameter id: 947 Numerical gradient of -0.0060671467849715555 is not very close to the backpropagation gradient of -0.006119065107451957!\n",
      "Parameter id: 948 Numerical gradient of -0.017815082742345112 is not very close to the backpropagation gradient of -0.01784663035721133!\n",
      "Parameter id: 949 Numerical gradient of 0.01910938074445312 is not very close to the backpropagation gradient of 0.019141982349721705!\n",
      "Parameter id: 950 Numerical gradient of -0.0023867574583391615 is not very close to the backpropagation gradient of -0.0023818062052131782!\n",
      "Parameter id: 951 Numerical gradient of 0.013455458969247047 is not very close to the backpropagation gradient of 0.013462235134934275!\n",
      "Parameter id: 952 Numerical gradient of -0.0124136256829388 is not very close to the backpropagation gradient of -0.012427657866160596!\n",
      "Parameter id: 953 Numerical gradient of 0.0028275159991153487 is not very close to the backpropagation gradient of 0.002800199512519581!\n",
      "Parameter id: 954 Numerical gradient of -0.00016275869541004795 is not very close to the backpropagation gradient of -0.00016978623649892033!\n",
      "Parameter id: 955 Numerical gradient of 0.02017164213441447 is not very close to the backpropagation gradient of 0.020217169435464963!\n",
      "Parameter id: 956 Numerical gradient of -0.010201395284070713 is not very close to the backpropagation gradient of -0.01025358536535289!\n",
      "Parameter id: 957 Numerical gradient of -0.005392797319814235 is not very close to the backpropagation gradient of -0.005408111700922585!\n",
      "Parameter id: 958 Numerical gradient of 0.002085664974060819 is not very close to the backpropagation gradient of 0.0021022419936251213!\n",
      "Parameter id: 959 Numerical gradient of -0.007126077505859029 is not very close to the backpropagation gradient of -0.007158210860805623!\n",
      "Parameter id: 960 Numerical gradient of 0.03137778925577095 is not very close to the backpropagation gradient of 0.03134976631122232!\n",
      "Parameter id: 961 Numerical gradient of -0.005267786207241443 is not very close to the backpropagation gradient of -0.005305051240515479!\n",
      "Parameter id: 962 Numerical gradient of -0.00040811798385220754 is not very close to the backpropagation gradient of -0.0004889882308468782!\n",
      "Parameter id: 963 Numerical gradient of 0.004481082171992057 is not very close to the backpropagation gradient of 0.0045344705044274725!\n",
      "Parameter id: 964 Numerical gradient of -0.013800072196090696 is not very close to the backpropagation gradient of -0.013824566538571607!\n",
      "Parameter id: 965 Numerical gradient of 0.005909051026264933 is not very close to the backpropagation gradient of 0.0058839107214296174!\n",
      "Parameter id: 966 Numerical gradient of -0.00018829382497642655 is not very close to the backpropagation gradient of -0.0001916588621140268!\n",
      "Parameter id: 967 Numerical gradient of 0.004912514839361393 is not very close to the backpropagation gradient of 0.00489403063639215!\n",
      "Parameter id: 968 Numerical gradient of 0.007570832849523867 is not very close to the backpropagation gradient of 0.007573347936236572!\n",
      "Parameter id: 969 Numerical gradient of -0.002871036741680655 is not very close to the backpropagation gradient of -0.002866253407407318!\n",
      "Parameter id: 970 Numerical gradient of 0.00900124419445092 is not very close to the backpropagation gradient of 0.009041794885040713!\n",
      "Parameter id: 971 Numerical gradient of 0.0027551294579097885 is not very close to the backpropagation gradient of 0.0027831722723538647!\n",
      "Parameter id: 972 Numerical gradient of -0.009150236124355615 is not very close to the backpropagation gradient of -0.009092696926382818!\n",
      "Parameter id: 973 Numerical gradient of -0.0006226130722097878 is not very close to the backpropagation gradient of -0.0006707221035879844!\n",
      "Parameter id: 974 Numerical gradient of -0.0006343814362708144 is not very close to the backpropagation gradient of -0.0006307546377350233!\n",
      "Parameter id: 975 Numerical gradient of 0.007112976874168452 is not very close to the backpropagation gradient of 0.007096792049604818!\n",
      "Parameter id: 976 Numerical gradient of 0.0011499690089067371 is not very close to the backpropagation gradient of 0.0011536932012619595!\n",
      "Parameter id: 977 Numerical gradient of -0.002644773289262048 is not very close to the backpropagation gradient of -0.002660745836671159!\n",
      "Parameter id: 978 Numerical gradient of -0.002078115457493368 is not very close to the backpropagation gradient of -0.0020759539181125857!\n",
      "Parameter id: 979 Numerical gradient of -0.009723777338876971 is not very close to the backpropagation gradient of -0.00970016038097966!\n",
      "Parameter id: 980 Numerical gradient of 0.0017337242752546445 is not very close to the backpropagation gradient of 0.0017302552708164276!\n",
      "Parameter id: 981 Numerical gradient of 0.006747047365252001 is not very close to the backpropagation gradient of 0.006737259852063807!\n",
      "Parameter id: 982 Numerical gradient of -0.005471179065352771 is not very close to the backpropagation gradient of -0.005468849876605191!\n",
      "Parameter id: 983 Numerical gradient of -0.008925304939566558 is not very close to the backpropagation gradient of -0.008920804450644094!\n",
      "Parameter id: 984 Numerical gradient of 0.0008615330671091215 is not very close to the backpropagation gradient of 0.0008503434378269639!\n",
      "Parameter id: 985 Numerical gradient of 0.00914912590133099 is not very close to the backpropagation gradient of 0.009179008263558533!\n",
      "Parameter id: 986 Numerical gradient of 0.00756128493151209 is not very close to the backpropagation gradient of 0.0075440877934301355!\n",
      "Parameter id: 987 Numerical gradient of -0.000929478716216181 is not very close to the backpropagation gradient of -0.0009432065950287935!\n",
      "Parameter id: 988 Numerical gradient of 0.005263567359747867 is not very close to the backpropagation gradient of 0.00525329692523514!\n",
      "Parameter id: 989 Numerical gradient of 0.0029620750296999176 is not very close to the backpropagation gradient of 0.002931884467086092!\n",
      "Parameter id: 990 Numerical gradient of 0.12267475923977143 is not very close to the backpropagation gradient of 0.12239744253770235!\n",
      "Parameter id: 991 Numerical gradient of -0.01319744313832416 is not very close to the backpropagation gradient of -0.013279369191500889!\n",
      "Parameter id: 992 Numerical gradient of 0.05718292506173838 is not very close to the backpropagation gradient of 0.05698068232401204!\n",
      "Parameter id: 993 Numerical gradient of 0.020952573009935804 is not very close to the backpropagation gradient of 0.02101888079348741!\n",
      "Parameter id: 994 Numerical gradient of -0.02010325239609756 is not very close to the backpropagation gradient of -0.020151273129906687!\n",
      "Parameter id: 995 Numerical gradient of 0.002177591440499782 is not very close to the backpropagation gradient of 0.0021788965174013813!\n",
      "Parameter id: 996 Numerical gradient of 0.028489210990301213 is not very close to the backpropagation gradient of 0.02846569628766696!\n",
      "Parameter id: 997 Numerical gradient of 0.008270051310432791 is not very close to the backpropagation gradient of 0.008155769252840584!\n",
      "Parameter id: 998 Numerical gradient of 0.06760658699533906 is not very close to the backpropagation gradient of 0.06746480280950727!\n",
      "Parameter id: 999 Numerical gradient of -0.013396839193546839 is not very close to the backpropagation gradient of -0.013412471620459008!\n",
      "Parameter id: 1000 Numerical gradient of 0.014355627797613122 is not very close to the backpropagation gradient of 0.014448528730647347!\n",
      "Parameter id: 1001 Numerical gradient of -0.01765254609153999 is not very close to the backpropagation gradient of -0.017668077958465124!\n",
      "Parameter id: 1002 Numerical gradient of -0.06631073468099657 is not very close to the backpropagation gradient of -0.06618441461169874!\n",
      "Parameter id: 1003 Numerical gradient of -0.018006263147185564 is not very close to the backpropagation gradient of -0.018094542112890872!\n",
      "Parameter id: 1004 Numerical gradient of 0.0007496225862269057 is not very close to the backpropagation gradient of 0.0007619039075168104!\n",
      "Parameter id: 1005 Numerical gradient of 0.05129696667438566 is not very close to the backpropagation gradient of 0.05129463180675713!\n",
      "Parameter id: 1006 Numerical gradient of -0.005068390152018765 is not very close to the backpropagation gradient of -0.0050308998991427925!\n",
      "Parameter id: 1007 Numerical gradient of -0.024627633266049997 is not very close to the backpropagation gradient of -0.024715339370260495!\n",
      "Parameter id: 1008 Numerical gradient of -0.037762237781180374 is not very close to the backpropagation gradient of -0.03780922657303533!\n",
      "Parameter id: 1009 Numerical gradient of 0.013502754470096079 is not very close to the backpropagation gradient of 0.013571411469147516!\n",
      "Parameter id: 1010 Numerical gradient of -0.010438983011340497 is not very close to the backpropagation gradient of -0.010446840388634387!\n",
      "Parameter id: 1011 Numerical gradient of 0.05300115901718527 is not very close to the backpropagation gradient of 0.05302258863190543!\n",
      "Parameter id: 1012 Numerical gradient of -0.05106004508093065 is not very close to the backpropagation gradient of -0.051084879906098356!\n",
      "Parameter id: 1013 Numerical gradient of 0.004635403172414954 is not very close to the backpropagation gradient of 0.004626285960930326!\n",
      "Parameter id: 1014 Numerical gradient of -0.04693334609839894 is not very close to the backpropagation gradient of -0.04698301487018129!\n",
      "Parameter id: 1015 Numerical gradient of 0.040107916987608405 is not very close to the backpropagation gradient of 0.04018661284705774!\n",
      "Parameter id: 1016 Numerical gradient of -0.03325117958752344 is not very close to the backpropagation gradient of -0.033327736856593806!\n",
      "Parameter id: 1017 Numerical gradient of -0.003698596984236246 is not very close to the backpropagation gradient of -0.00370332672914877!\n",
      "Parameter id: 1018 Numerical gradient of 0.02715783153917073 is not very close to the backpropagation gradient of 0.027190662150599447!\n",
      "Parameter id: 1019 Numerical gradient of 0.004866995695351761 is not very close to the backpropagation gradient of 0.004843921089785222!\n",
      "Parameter id: 1020 Numerical gradient of -0.04174460777051081 is not very close to the backpropagation gradient of -0.04181344853077901!\n",
      "Parameter id: 1021 Numerical gradient of 0.004235500838944972 is not very close to the backpropagation gradient of 0.004394390494656451!\n",
      "Parameter id: 1022 Numerical gradient of -0.028432811660650256 is not very close to the backpropagation gradient of -0.028621908736214823!\n",
      "Parameter id: 1023 Numerical gradient of 0.008814726726313893 is not very close to the backpropagation gradient of 0.008791459376901024!\n",
      "Parameter id: 1024 Numerical gradient of -0.024419799515840168 is not very close to the backpropagation gradient of -0.024657662165547017!\n",
      "Parameter id: 1025 Numerical gradient of -0.031311397918898365 is not very close to the backpropagation gradient of -0.031543108543766143!\n",
      "Parameter id: 1026 Numerical gradient of -0.0219939622070342 is not very close to the backpropagation gradient of -0.022342267405581262!\n",
      "Parameter id: 1027 Numerical gradient of 0.08869038836678556 is not very close to the backpropagation gradient of 0.08930752326694644!\n",
      "Parameter id: 1028 Numerical gradient of 0.02514322083868592 is not very close to the backpropagation gradient of 0.025269534861297818!\n",
      "Parameter id: 1029 Numerical gradient of -0.039364733694924325 is not very close to the backpropagation gradient of -0.039633545204545845!\n",
      "Parameter id: 1030 Numerical gradient of 0.06038436417554748 is not very close to the backpropagation gradient of 0.060849032586632414!\n",
      "Parameter id: 1031 Numerical gradient of 0.04676214970800174 is not very close to the backpropagation gradient of 0.04682548594258951!\n",
      "Parameter id: 1032 Numerical gradient of -0.038183234352118234 is not very close to the backpropagation gradient of -0.03812785463324785!\n",
      "Parameter id: 1033 Numerical gradient of 0.06635114679909293 is not very close to the backpropagation gradient of 0.06638728796609153!\n",
      "Parameter id: 1034 Numerical gradient of -0.003619105015673085 is not very close to the backpropagation gradient of -0.0035446352585879!\n",
      "Parameter id: 1035 Numerical gradient of 0.0662931931572075 is not very close to the backpropagation gradient of 0.06636151077871372!\n",
      "Parameter id: 1036 Numerical gradient of 0.06345435288324097 is not very close to the backpropagation gradient of 0.06352098645339113!\n",
      "Parameter id: 1037 Numerical gradient of 0.07011102809428849 is not very close to the backpropagation gradient of 0.07015251363182612!\n",
      "Parameter id: 1038 Numerical gradient of -0.15856738144748306 is not very close to the backpropagation gradient of -0.15889287174726982!\n",
      "Parameter id: 1040 Numerical gradient of 0.06454436984881795 is not very close to the backpropagation gradient of 0.06463847426909569!\n",
      "Parameter id: 1041 Numerical gradient of -0.1167204111141018 is not very close to the backpropagation gradient of -0.11689342040889864!\n",
      "Parameter id: 1042 Numerical gradient of -0.02662203790748663 is not very close to the backpropagation gradient of -0.02670227410750363!\n",
      "Parameter id: 1043 Numerical gradient of 0.030012659024691853 is not very close to the backpropagation gradient of 0.030099461343632252!\n",
      "Parameter id: 1044 Numerical gradient of -0.04067834957766081 is not very close to the backpropagation gradient of -0.040794058065927466!\n",
      "Parameter id: 1045 Numerical gradient of 0.01159805584904916 is not very close to the backpropagation gradient of 0.011581643717490667!\n",
      "Parameter id: 1046 Numerical gradient of -0.029508395726907107 is not very close to the backpropagation gradient of -0.029677115691689644!\n",
      "Parameter id: 1047 Numerical gradient of -0.03486078092862499 is not very close to the backpropagation gradient of -0.03503897662005248!\n",
      "Parameter id: 1048 Numerical gradient of -0.04453637458823323 is not very close to the backpropagation gradient of -0.04475565706170772!\n",
      "Parameter id: 1049 Numerical gradient of 0.08140843554826915 is not very close to the backpropagation gradient of 0.08195068419697726!\n",
      "Parameter id: 1050 Numerical gradient of 0.026174840073167616 is not very close to the backpropagation gradient of 0.026295223511597125!\n",
      "Parameter id: 1051 Numerical gradient of -0.03983569030197032 is not very close to the backpropagation gradient of -0.04010476220118966!\n",
      "Parameter id: 1052 Numerical gradient of 0.06684874875872993 is not very close to the backpropagation gradient of 0.06714583097837329!\n",
      "Parameter id: 1053 Numerical gradient of -0.04690936528106704 is not very close to the backpropagation gradient of -0.04693647682677589!\n",
      "Parameter id: 1054 Numerical gradient of 0.012115197733919558 is not very close to the backpropagation gradient of 0.012126019518204762!\n",
      "Parameter id: 1055 Numerical gradient of -0.047128523306128045 is not very close to the backpropagation gradient of -0.04718448666903784!\n",
      "Parameter id: 1056 Numerical gradient of -0.001816991002101531 is not very close to the backpropagation gradient of -0.001844967596357034!\n",
      "Parameter id: 1057 Numerical gradient of -0.041138870088275326 is not very close to the backpropagation gradient of -0.04120980817848953!\n",
      "Parameter id: 1058 Numerical gradient of -0.04080269455641883 is not very close to the backpropagation gradient of -0.04088158052276314!\n",
      "Parameter id: 1059 Numerical gradient of -0.0328010951733404 is not very close to the backpropagation gradient of -0.03293796458880929!\n",
      "Parameter id: 1060 Numerical gradient of 0.10272560579949186 is not very close to the backpropagation gradient of 0.10308785521012147!\n",
      "Parameter id: 1061 Numerical gradient of 0.03694022865374791 is not very close to the backpropagation gradient of 0.03696187247313462!\n",
      "Parameter id: 1062 Numerical gradient of -0.030362157232843853 is not very close to the backpropagation gradient of -0.030534465356861492!\n",
      "Parameter id: 1063 Numerical gradient of 0.08917977467604032 is not very close to the backpropagation gradient of 0.08935400253763334!\n",
      "Parameter id: 1064 Numerical gradient of 0.05334954700231265 is not very close to the backpropagation gradient of 0.053389584035742246!\n",
      "Parameter id: 1065 Numerical gradient of -0.0168329794547617 is not very close to the backpropagation gradient of -0.01675255944664582!\n",
      "Parameter id: 1066 Numerical gradient of 0.048797632601349505 is not very close to the backpropagation gradient of 0.04883245058571624!\n",
      "Parameter id: 1067 Numerical gradient of -0.003303801676679541 is not very close to the backpropagation gradient of -0.0032314649350255603!\n",
      "Parameter id: 1068 Numerical gradient of 0.05637645905665067 is not very close to the backpropagation gradient of 0.05640070313664486!\n",
      "Parameter id: 1069 Numerical gradient of 0.0556563684028788 is not very close to the backpropagation gradient of 0.0556910982317029!\n",
      "Parameter id: 1070 Numerical gradient of 0.044245718200386364 is not very close to the backpropagation gradient of 0.04429985298677974!\n",
      "Parameter id: 1071 Numerical gradient of -0.13782042174170783 is not very close to the backpropagation gradient of -0.1381124480227728!\n",
      "Parameter id: 1072 Numerical gradient of -0.04297673328323981 is not very close to the backpropagation gradient of -0.04295944003844846!\n",
      "Parameter id: 1073 Numerical gradient of 0.04934319619565031 is not very close to the backpropagation gradient of 0.04943545104864993!\n",
      "Parameter id: 1074 Numerical gradient of -0.10683609552586404 is not very close to the backpropagation gradient of -0.10699322758234328!\n",
      "Parameter id: 1075 Numerical gradient of -0.03317213170817013 is not very close to the backpropagation gradient of -0.03318759536897082!\n",
      "Parameter id: 1076 Numerical gradient of 0.008426370712300013 is not very close to the backpropagation gradient of 0.00835244397410008!\n",
      "Parameter id: 1077 Numerical gradient of -0.04306022205469162 is not very close to the backpropagation gradient of -0.043055783239941664!\n",
      "Parameter id: 1078 Numerical gradient of -0.0074493744506298745 is not very close to the backpropagation gradient of -0.007503140637156355!\n",
      "Parameter id: 1079 Numerical gradient of -0.029752422747719717 is not very close to the backpropagation gradient of -0.029751783270146503!\n",
      "Parameter id: 1080 Numerical gradient of -0.026472379843767158 is not very close to the backpropagation gradient of -0.026472861273648264!\n",
      "Parameter id: 1081 Numerical gradient of -0.020726531602122122 is not very close to the backpropagation gradient of -0.020701583525388097!\n",
      "Parameter id: 1082 Numerical gradient of 0.06366107641042618 is not very close to the backpropagation gradient of 0.06376442404776855!\n",
      "Parameter id: 1083 Numerical gradient of 0.030580205034880233 is not very close to the backpropagation gradient of 0.03055756664038216!\n",
      "Parameter id: 1084 Numerical gradient of -0.005482503340203948 is not very close to the backpropagation gradient of -0.005487852245107791!\n",
      "Parameter id: 1085 Numerical gradient of 0.06344857972351292 is not very close to the backpropagation gradient of 0.0634861648981087!\n",
      "Parameter id: 1086 Numerical gradient of -0.030185853816533378 is not very close to the backpropagation gradient of -0.030264334328665897!\n",
      "Parameter id: 1087 Numerical gradient of 0.04298295053217771 is not very close to the backpropagation gradient of 0.042949623814330776!\n",
      "Parameter id: 1088 Numerical gradient of -0.05292233318243689 is not very close to the backpropagation gradient of -0.05297752698216214!\n",
      "Parameter id: 1089 Numerical gradient of 0.00929256671611256 is not very close to the backpropagation gradient of 0.009223274905636533!\n",
      "Parameter id: 1090 Numerical gradient of -0.052589932408864115 is not very close to the backpropagation gradient of -0.05267694073816777!\n",
      "Parameter id: 1091 Numerical gradient of -0.05435141225973439 is not very close to the backpropagation gradient of -0.05443143679299161!\n",
      "Parameter id: 1092 Numerical gradient of -0.06953615461213758 is not very close to the backpropagation gradient of -0.06957995521374424!\n",
      "Parameter id: 1093 Numerical gradient of 0.131126220992428 is not very close to the backpropagation gradient of 0.13144292555934237!\n",
      "Parameter id: 1094 Numerical gradient of 0.04896238969820388 is not very close to the backpropagation gradient of 0.04898440941064161!\n",
      "Parameter id: 1095 Numerical gradient of -0.06519318418440889 is not very close to the backpropagation gradient of -0.06527424812636912!\n",
      "Parameter id: 1096 Numerical gradient of 0.0924129661683537 is not very close to the backpropagation gradient of 0.09260420849214956!\n",
      "Parameter id: 1097 Numerical gradient of 0.04684941323773728 is not very close to the backpropagation gradient of 0.04688454342330794!\n",
      "Parameter id: 1098 Numerical gradient of -0.011790346476914237 is not very close to the backpropagation gradient of -0.011689086189696886!\n",
      "Parameter id: 1099 Numerical gradient of 0.04788769381036673 is not very close to the backpropagation gradient of 0.047882321564137346!\n",
      "Parameter id: 1100 Numerical gradient of -0.0016187051699034782 is not very close to the backpropagation gradient of -0.0015616402326571028!\n",
      "Parameter id: 1101 Numerical gradient of 0.03917777213757745 is not very close to the backpropagation gradient of 0.039166772103479935!\n",
      "Parameter id: 1102 Numerical gradient of 0.04157785227221211 is not very close to the backpropagation gradient of 0.04158166619340984!\n",
      "Parameter id: 1103 Numerical gradient of 0.03293298966866587 is not very close to the backpropagation gradient of 0.03292255085753464!\n",
      "Parameter id: 1104 Numerical gradient of -0.10093770264063551 is not very close to the backpropagation gradient of -0.10111592743845496!\n",
      "Parameter id: 1105 Numerical gradient of -0.03525268965631767 is not very close to the backpropagation gradient of -0.0352093233818918!\n",
      "Parameter id: 1106 Numerical gradient of 0.02992117664746274 is not very close to the backpropagation gradient of 0.029962955980902966!\n",
      "Parameter id: 1107 Numerical gradient of -0.08874700974104144 is not very close to the backpropagation gradient of -0.0888248328800719!\n",
      "Parameter id: 1108 Numerical gradient of 0.05207190234557402 is not very close to the backpropagation gradient of 0.05197362149161303!\n",
      "Parameter id: 1109 Numerical gradient of 0.06340217240108359 is not very close to the backpropagation gradient of 0.0635932995919153!\n",
      "Parameter id: 1110 Numerical gradient of 0.007292388914947877 is not very close to the backpropagation gradient of 0.007156248251874216!\n",
      "Parameter id: 1111 Numerical gradient of 0.0061239902038323635 is not very close to the backpropagation gradient of 0.00612623193763529!\n",
      "Parameter id: 1112 Numerical gradient of -0.025624613542163388 is not very close to the backpropagation gradient of -0.025856655022972214!\n",
      "Parameter id: 1113 Numerical gradient of -0.02147748645597858 is not very close to the backpropagation gradient of -0.021748357243660282!\n",
      "Parameter id: 1114 Numerical gradient of -0.05926192869765145 is not very close to the backpropagation gradient of -0.059416932492045965!\n",
      "Parameter id: 1115 Numerical gradient of 0.030643265702678942 is not very close to the backpropagation gradient of 0.031141530683592086!\n",
      "Parameter id: 1116 Numerical gradient of -0.002701394663517931 is not very close to the backpropagation gradient of -0.0025787128892616053!\n",
      "Parameter id: 1117 Numerical gradient of -0.05281530768286302 is not very close to the backpropagation gradient of -0.05310477924822827!\n",
      "Parameter id: 1118 Numerical gradient of 0.0023450130726132556 is not very close to the backpropagation gradient of 0.0027145049395384414!\n",
      "Parameter id: 1119 Numerical gradient of 0.044544590238615456 is not very close to the backpropagation gradient of 0.044612825649273664!\n",
      "Parameter id: 1120 Numerical gradient of -0.043620440592917475 is not very close to the backpropagation gradient of -0.04358359166372433!\n",
      "Parameter id: 1121 Numerical gradient of 0.06934430807348235 is not very close to the backpropagation gradient of 0.06939065125113075!\n",
      "Parameter id: 1122 Numerical gradient of -0.005484723786253198 is not very close to the backpropagation gradient of -0.005407274152183267!\n",
      "Parameter id: 1123 Numerical gradient of 0.06501732485730827 is not very close to the backpropagation gradient of 0.06510113077702571!\n",
      "Parameter id: 1124 Numerical gradient of 0.06276468234034382 is not very close to the backpropagation gradient of 0.06284364393991666!\n",
      "Parameter id: 1125 Numerical gradient of 0.08087708280868355 is not very close to the backpropagation gradient of 0.08092615858915583!\n",
      "Parameter id: 1126 Numerical gradient of -0.162536428760518 is not very close to the backpropagation gradient of -0.1628778630434717!\n",
      "Parameter id: 1127 Numerical gradient of -0.06638556371285631 is not very close to the backpropagation gradient of -0.06639761526175642!\n",
      "Parameter id: 1128 Numerical gradient of 0.07052114447958502 is not very close to the backpropagation gradient of 0.07062172628959346!\n",
      "Parameter id: 1129 Numerical gradient of -0.11504175390086856 is not very close to the backpropagation gradient of -0.11522979237496044!\n",
      "Parameter id: 1130 Numerical gradient of -0.055894400219358424 is not very close to the backpropagation gradient of -0.055922869391339954!\n",
      "Parameter id: 1131 Numerical gradient of 0.003114175584073564 is not very close to the backpropagation gradient of 0.0030364285413438364!\n",
      "Parameter id: 1132 Numerical gradient of -0.04763234251470294 is not very close to the backpropagation gradient of -0.04766823973997822!\n",
      "Parameter id: 1133 Numerical gradient of 0.0014601653219870059 is not very close to the backpropagation gradient of 0.0013949589346034102!\n",
      "Parameter id: 1134 Numerical gradient of -0.03770739276376389 is not very close to the backpropagation gradient of -0.03772837532819723!\n",
      "Parameter id: 1135 Numerical gradient of -0.04413736043318295 is not very close to the backpropagation gradient of -0.0441670848636448!\n",
      "Parameter id: 1136 Numerical gradient of -0.02621747263731322 is not very close to the backpropagation gradient of -0.026277857550665574!\n",
      "Parameter id: 1137 Numerical gradient of 0.11212297756912902 is not very close to the backpropagation gradient of 0.11240475919097194!\n",
      "Parameter id: 1138 Numerical gradient of 0.03466182896261216 is not very close to the backpropagation gradient of 0.03463415504468977!\n",
      "Parameter id: 1139 Numerical gradient of -0.034437785956242806 is not very close to the backpropagation gradient of -0.0345281413029586!\n",
      "Parameter id: 1140 Numerical gradient of 0.09466760708676247 is not very close to the backpropagation gradient of 0.09482226646517541!\n",
      "Parameter id: 1141 Numerical gradient of 0.033445690661437766 is not very close to the backpropagation gradient of 0.03349171198897838!\n",
      "Parameter id: 1142 Numerical gradient of -0.03719180519112797 is not very close to the backpropagation gradient of -0.03720849450235989!\n",
      "Parameter id: 1143 Numerical gradient of 0.05313993689526342 is not very close to the backpropagation gradient of 0.05319981721472068!\n",
      "Parameter id: 1144 Numerical gradient of -0.010544010109470037 is not very close to the backpropagation gradient of -0.010486845527665456!\n",
      "Parameter id: 1145 Numerical gradient of 0.04191313962564891 is not very close to the backpropagation gradient of 0.042033467107082105!\n",
      "Parameter id: 1146 Numerical gradient of 0.04535771758185092 is not very close to the backpropagation gradient of 0.045470333646390845!\n",
      "Parameter id: 1147 Numerical gradient of 0.05810307790454771 is not very close to the backpropagation gradient of 0.058219464234674785!\n",
      "Parameter id: 1148 Numerical gradient of -0.10978995490518173 is not very close to the backpropagation gradient of -0.11018528704738498!\n",
      "Parameter id: 1149 Numerical gradient of -0.04108269280322929 is not very close to the backpropagation gradient of -0.04113304662145343!\n",
      "Parameter id: 1150 Numerical gradient of 0.052057247401648965 is not very close to the backpropagation gradient of 0.05221748254566788!\n",
      "Parameter id: 1151 Numerical gradient of -0.08540701479375912 is not very close to the backpropagation gradient of -0.0856186030386509!\n",
      "Parameter id: 1152 Numerical gradient of 0.02297895207448164 is not very close to the backpropagation gradient of 0.023117186722323678!\n",
      "Parameter id: 1153 Numerical gradient of -0.040498937536881385 is not very close to the backpropagation gradient of -0.040471113902816366!\n",
      "Parameter id: 1154 Numerical gradient of 0.044923176290012634 is not very close to the backpropagation gradient of 0.04503269664228923!\n",
      "Parameter id: 1155 Numerical gradient of -0.004658495811327157 is not very close to the backpropagation gradient of -0.004601846423259278!\n",
      "Parameter id: 1156 Numerical gradient of 0.05123657054184605 is not very close to the backpropagation gradient of 0.05135075250949388!\n",
      "Parameter id: 1157 Numerical gradient of 0.05043521156267161 is not very close to the backpropagation gradient of 0.05054614329597282!\n",
      "Parameter id: 1158 Numerical gradient of 0.06377343098051824 is not very close to the backpropagation gradient of 0.06383934023015722!\n",
      "Parameter id: 1159 Numerical gradient of -0.11604850413959865 is not very close to the backpropagation gradient of -0.11644310648932418!\n",
      "Parameter id: 1160 Numerical gradient of -0.042653880427678814 is not very close to the backpropagation gradient of -0.042726538276607585!\n",
      "Parameter id: 1161 Numerical gradient of 0.05540590208852336 is not very close to the backpropagation gradient of 0.05550790153115983!\n",
      "Parameter id: 1162 Numerical gradient of -0.08489298153335767 is not very close to the backpropagation gradient of -0.08515141583938916!\n",
      "Parameter id: 1163 Numerical gradient of 0.04782818585624682 is not very close to the backpropagation gradient of 0.04788577008811564!\n",
      "Parameter id: 1164 Numerical gradient of -0.034266145476635756 is not very close to the backpropagation gradient of -0.034202059035730724!\n",
      "Parameter id: 1165 Numerical gradient of 0.061926463956751825 is not very close to the backpropagation gradient of 0.06195632781691678!\n",
      "Parameter id: 1166 Numerical gradient of -0.003099964729358362 is not very close to the backpropagation gradient of -0.0030188306431024976!\n",
      "Parameter id: 1167 Numerical gradient of 0.06644174099790234 is not very close to the backpropagation gradient of 0.06649245844779735!\n",
      "Parameter id: 1168 Numerical gradient of 0.06313682909819818 is not very close to the backpropagation gradient of 0.06319735295958523!\n",
      "Parameter id: 1169 Numerical gradient of 0.06657030482415394 is not very close to the backpropagation gradient of 0.0666031109326053!\n",
      "Parameter id: 1170 Numerical gradient of -0.15302115130566563 is not very close to the backpropagation gradient of -0.15332945774233728!\n",
      "Parameter id: 1171 Numerical gradient of -0.05720757201288506 is not very close to the backpropagation gradient of -0.057191836522246575!\n",
      "Parameter id: 1172 Numerical gradient of 0.058496096855265016 is not very close to the backpropagation gradient of 0.05858351313849378!\n",
      "Parameter id: 1173 Numerical gradient of -0.11680412193015853 is not very close to the backpropagation gradient of -0.11697634944009701!\n",
      "Parameter id: 1174 Numerical gradient of 0.02836997303745647 is not very close to the backpropagation gradient of 0.02842971681155753!\n",
      "Parameter id: 1175 Numerical gradient of -0.051630477670983055 is not very close to the backpropagation gradient of -0.05160442898362252!\n",
      "Parameter id: 1176 Numerical gradient of 0.0608821881797894 is not very close to the backpropagation gradient of 0.06091410433291783!\n",
      "Parameter id: 1177 Numerical gradient of -0.006836087251826939 is not very close to the backpropagation gradient of -0.006768006524066545!\n",
      "Parameter id: 1178 Numerical gradient of 0.058386628865036976 is not very close to the backpropagation gradient of 0.05847607329400113!\n",
      "Parameter id: 1179 Numerical gradient of 0.05917466516791591 is not very close to the backpropagation gradient of 0.059253727510618415!\n",
      "Parameter id: 1180 Numerical gradient of 0.08120970562686125 is not very close to the backpropagation gradient of 0.08124114359601395!\n",
      "Parameter id: 1181 Numerical gradient of -0.14201728681939585 is not very close to the backpropagation gradient of -0.14231004342245612!\n",
      "Parameter id: 1182 Numerical gradient of -0.054710902475108014 is not very close to the backpropagation gradient of -0.05470896917995928!\n",
      "Parameter id: 1183 Numerical gradient of 0.06967604271324035 is not very close to the backpropagation gradient of 0.06975313387215186!\n",
      "Parameter id: 1184 Numerical gradient of -0.10250444937298653 is not very close to the backpropagation gradient of -0.1026764513071563!\n",
      "Parameter id: 1185 Numerical gradient of 0.00015276668818842154 is not very close to the backpropagation gradient of -3.0039086833899163e-05!\n",
      "Parameter id: 1186 Numerical gradient of 0.0615749673471555 is not very close to the backpropagation gradient of 0.06153987324919656!\n",
      "Parameter id: 1187 Numerical gradient of -0.04132560960101728 is not very close to the backpropagation gradient of -0.04145237101810845!\n",
      "Parameter id: 1188 Numerical gradient of 0.004735323244631218 is not very close to the backpropagation gradient of 0.004686004883849918!\n",
      "Parameter id: 1189 Numerical gradient of -0.05388356427715735 is not very close to the backpropagation gradient of -0.05401158172285523!\n",
      "Parameter id: 1190 Numerical gradient of -0.05274536363231164 is not very close to the backpropagation gradient of -0.052861473956014274!\n",
      "Parameter id: 1191 Numerical gradient of -0.07919886968466017 is not very close to the backpropagation gradient of -0.07925440688582021!\n",
      "Parameter id: 1192 Numerical gradient of 0.12165046747725226 is not very close to the backpropagation gradient of 0.12204086291298766!\n",
      "Parameter id: 1193 Numerical gradient of 0.04013411825098956 is not very close to the backpropagation gradient of 0.04022454942815244!\n",
      "Parameter id: 1194 Numerical gradient of -0.07799072498926307 is not very close to the backpropagation gradient of -0.07805666138823994!\n",
      "Parameter id: 1195 Numerical gradient of 0.07689693326540237 is not very close to the backpropagation gradient of 0.07717524358368541!\n",
      "Parameter id: 1196 Numerical gradient of -0.04709788115064839 is not very close to the backpropagation gradient of -0.047161675583358695!\n",
      "Parameter id: 1197 Numerical gradient of 0.0427247126566499 is not very close to the backpropagation gradient of 0.04268226329039534!\n",
      "Parameter id: 1198 Numerical gradient of -0.07269940205389958 is not very close to the backpropagation gradient of -0.07273368328998729!\n",
      "Parameter id: 1199 Numerical gradient of 0.00492605956026182 is not very close to the backpropagation gradient of 0.004847626590199635!\n",
      "Parameter id: 1200 Numerical gradient of -0.06382561146267562 is not very close to the backpropagation gradient of -0.06391153676320926!\n",
      "Parameter id: 1201 Numerical gradient of -0.06288503051621319 is not very close to the backpropagation gradient of -0.06296599905593259!\n",
      "Parameter id: 1202 Numerical gradient of -0.07912492883122013 is not very close to the backpropagation gradient of -0.07916033276842027!\n",
      "Parameter id: 1203 Numerical gradient of 0.1602886712248619 is not very close to the backpropagation gradient of 0.16061600853406055!\n",
      "Parameter id: 1204 Numerical gradient of 0.06617462133817753 is not very close to the backpropagation gradient of 0.06617276461202945!\n",
      "Parameter id: 1205 Numerical gradient of -0.0673807676321303 is not very close to the backpropagation gradient of -0.06747286330788434!\n",
      "Parameter id: 1206 Numerical gradient of 0.11890022300065083 is not very close to the backpropagation gradient of 0.1190874277421074!\n",
      "Parameter id: 1207 Numerical gradient of 0.04702460643102313 is not very close to the backpropagation gradient of 0.04707247569681718!\n",
      "Parameter id: 1208 Numerical gradient of -0.020117241206207837 is not very close to the backpropagation gradient of -0.020047399413419757!\n",
      "Parameter id: 1209 Numerical gradient of 0.04970490685707318 is not very close to the backpropagation gradient of 0.04974390602977118!\n",
      "Parameter id: 1210 Numerical gradient of -0.013062884107739592 is not very close to the backpropagation gradient of -0.012984364849531973!\n",
      "Parameter id: 1211 Numerical gradient of 0.04107891804494557 is not very close to the backpropagation gradient of 0.04111943270228357!\n",
      "Parameter id: 1212 Numerical gradient of 0.047948089942906336 is not very close to the backpropagation gradient of 0.04799839049440331!\n",
      "Parameter id: 1213 Numerical gradient of 0.04845746026660436 is not very close to the backpropagation gradient of 0.04848874234674795!\n",
      "Parameter id: 1214 Numerical gradient of -0.11975842539868607 is not very close to the backpropagation gradient of -0.12002546132210562!\n",
      "Parameter id: 1215 Numerical gradient of -0.0416566781069605 is not very close to the backpropagation gradient of -0.04164224862709372!\n",
      "Parameter id: 1216 Numerical gradient of 0.050350612568195174 is not very close to the backpropagation gradient of 0.05041926202302189!\n",
      "Parameter id: 1217 Numerical gradient of -0.08996758893431434 is not very close to the backpropagation gradient of -0.09014273508089404!\n",
      "Parameter id: 1218 Numerical gradient of 0.060824456582508894 is not very close to the backpropagation gradient of 0.060867930411940704!\n",
      "Parameter id: 1219 Numerical gradient of -0.011947776101806085 is not very close to the backpropagation gradient of -0.01185463735486528!\n",
      "Parameter id: 1220 Numerical gradient of 0.06559197629485425 is not very close to the backpropagation gradient of 0.0656142428638941!\n",
      "Parameter id: 1221 Numerical gradient of -0.002176925306685007 is not very close to the backpropagation gradient of -0.0020993814668922354!\n",
      "Parameter id: 1222 Numerical gradient of 0.04426681243785424 is not very close to the backpropagation gradient of 0.04428773518043338!\n",
      "Parameter id: 1223 Numerical gradient of 0.04735323244631218 is not very close to the backpropagation gradient of 0.04738385303503297!\n",
      "Parameter id: 1224 Numerical gradient of 0.03699995865247274 is not very close to the backpropagation gradient of 0.03703278383299455!\n",
      "Parameter id: 1225 Numerical gradient of -0.12139378391395893 is not very close to the backpropagation gradient of -0.12167335661336695!\n",
      "Parameter id: 1226 Numerical gradient of -0.04380296125816585 is not very close to the backpropagation gradient of -0.04376150747168228!\n",
      "Parameter id: 1227 Numerical gradient of 0.03441669171877493 is not very close to the backpropagation gradient of 0.03448937764162343!\n",
      "Parameter id: 1228 Numerical gradient of -0.11013323586439583 is not very close to the backpropagation gradient of -0.11028704005911244!\n",
      "Parameter id: 1229 Numerical gradient of -0.03757194555475962 is not very close to the backpropagation gradient of -0.03763488535912884!\n",
      "Parameter id: 1230 Numerical gradient of 0.0397006871821759 is not very close to the backpropagation gradient of 0.039643029045701494!\n",
      "Parameter id: 1231 Numerical gradient of -0.06095124405192109 is not very close to the backpropagation gradient of -0.06097346956198088!\n",
      "Parameter id: 1232 Numerical gradient of 0.002610356375498668 is not very close to the backpropagation gradient of 0.002537259642977601!\n",
      "Parameter id: 1233 Numerical gradient of -0.05480726983364548 is not very close to the backpropagation gradient of -0.05487468845233243!\n",
      "Parameter id: 1234 Numerical gradient of -0.05481259890416368 is not very close to the backpropagation gradient of -0.054874499180313965!\n",
      "Parameter id: 1235 Numerical gradient of -0.06676348363043871 is not very close to the backpropagation gradient of -0.066775144042716!\n",
      "Parameter id: 1236 Numerical gradient of 0.1320923370684568 is not very close to the backpropagation gradient of 0.1323663667057277!\n",
      "Parameter id: 1237 Numerical gradient of 0.051482595964102984 is not very close to the backpropagation gradient of 0.05146775905872046!\n",
      "Parameter id: 1238 Numerical gradient of -0.05519096291095593 is not very close to the backpropagation gradient of -0.05525153823172829!\n",
      "Parameter id: 1239 Numerical gradient of 0.10421108420644032 is not very close to the backpropagation gradient of 0.1043698103750732!\n",
      "Parameter id: 1240 Numerical gradient of 0.03723776842434745 is not very close to the backpropagation gradient of 0.0373021016839285!\n",
      "Parameter id: 1241 Numerical gradient of -0.04868527803125744 is not very close to the backpropagation gradient of -0.048659631187650924!\n",
      "Parameter id: 1242 Numerical gradient of 0.06725153767206393 is not very close to the backpropagation gradient of 0.06728797878875456!\n",
      "Parameter id: 1243 Numerical gradient of -0.004794387109541276 is not very close to the backpropagation gradient of -0.004726855902349839!\n",
      "Parameter id: 1244 Numerical gradient of 0.06131517515939321 is not very close to the backpropagation gradient of 0.06141215608297947!\n",
      "Parameter id: 1245 Numerical gradient of 0.05985523188201113 is not very close to the backpropagation gradient of 0.05994099631084915!\n",
      "Parameter id: 1246 Numerical gradient of 0.08203659973560207 is not very close to the backpropagation gradient of 0.08208434021227519!\n",
      "Parameter id: 1247 Numerical gradient of -0.15423218258092675 is not very close to the backpropagation gradient of -0.15456749694752578!\n",
      "Parameter id: 1248 Numerical gradient of -0.0625532958764552 is not very close to the backpropagation gradient of -0.06256489877116365!\n",
      "Parameter id: 1249 Numerical gradient of 0.07178457828160845 is not very close to the backpropagation gradient of 0.07188633873462369!\n",
      "Parameter id: 1250 Numerical gradient of -0.10921463733382097 is not very close to the backpropagation gradient of -0.10939502900472035!\n",
      "Parameter id: 1251 Numerical gradient of -0.02861133552300998 is not very close to the backpropagation gradient of -0.02871265922815644!\n",
      "Parameter id: 1252 Numerical gradient of 0.04781641749218579 is not very close to the backpropagation gradient of 0.0477810334164934!\n",
      "Parameter id: 1253 Numerical gradient of -0.05463829388929753 is not very close to the backpropagation gradient of -0.05470896218063797!\n",
      "Parameter id: 1254 Numerical gradient of 0.006860734202973617 is not very close to the backpropagation gradient of 0.006793093322173722!\n",
      "Parameter id: 1255 Numerical gradient of -0.057610360926219066 is not very close to the backpropagation gradient of -0.05770779237240128!\n",
      "Parameter id: 1256 Numerical gradient of -0.05729283714117627 is not very close to the backpropagation gradient of -0.05738184183011062!\n",
      "Parameter id: 1257 Numerical gradient of -0.07674683111247305 is not very close to the backpropagation gradient of -0.07679146791458234!\n",
      "Parameter id: 1258 Numerical gradient of 0.1392659321197698 is not very close to the backpropagation gradient of 0.1396076617433544!\n",
      "Parameter id: 1259 Numerical gradient of 0.05306421968498398 is not very close to the backpropagation gradient of 0.053098491629154324!\n",
      "Parameter id: 1260 Numerical gradient of -0.06918843276082498 is not very close to the backpropagation gradient of -0.06927106571947023!\n",
      "Parameter id: 1261 Numerical gradient of 0.09708100989769264 is not very close to the backpropagation gradient of 0.09729350913418294!\n",
      "Parameter id: 1262 Numerical gradient of 0.04633160521905211 is not very close to the backpropagation gradient of 0.046400512882086846!\n",
      "Parameter id: 1263 Numerical gradient of -0.0409781097943096 is not very close to the backpropagation gradient of -0.04092897614029613!\n",
      "Parameter id: 1264 Numerical gradient of 0.06934075535980355 is not very close to the backpropagation gradient of 0.06937676884638813!\n",
      "Parameter id: 1265 Numerical gradient of -0.004323430502495285 is not very close to the backpropagation gradient of -0.004246427033958824!\n",
      "Parameter id: 1266 Numerical gradient of 0.06315103995291338 is not very close to the backpropagation gradient of 0.06322949823451628!\n",
      "Parameter id: 1267 Numerical gradient of 0.062248650678498045 is not very close to the backpropagation gradient of 0.062322500826565555!\n",
      "Parameter id: 1268 Numerical gradient of 0.07440847937800754 is not very close to the backpropagation gradient of 0.07444502003840733!\n",
      "Parameter id: 1269 Numerical gradient of -0.15622347859789443 is not very close to the backpropagation gradient of -0.15655275051644896!\n",
      "Parameter id: 1271 Numerical gradient of 0.06525935347667655 is not very close to the backpropagation gradient of 0.06535193508387369!\n",
      "Parameter id: 1272 Numerical gradient of -0.11679079925386303 is not very close to the backpropagation gradient of -0.11697403411531988!\n",
      "Parameter id: 1273 Numerical gradient of -0.04808553555335493 is not very close to the backpropagation gradient of -0.048058915525901635!\n",
      "Parameter id: 1274 Numerical gradient of -0.06134115437816944 is not very close to the backpropagation gradient of -0.06138787940803383!\n",
      "Parameter id: 1275 Numerical gradient of -0.005519362744621503 is not very close to the backpropagation gradient of -0.005507137424919532!\n",
      "Parameter id: 1276 Numerical gradient of -0.007192690887336538 is not very close to the backpropagation gradient of -0.0071804145563848735!\n",
      "Parameter id: 1277 Numerical gradient of 0.023174351326815668 is not very close to the backpropagation gradient of 0.02324562005592627!\n",
      "Parameter id: 1278 Numerical gradient of 0.02054245662463927 is not very close to the backpropagation gradient of 0.020596409580486107!\n",
      "Parameter id: 1279 Numerical gradient of 0.05450595530476221 is not very close to the backpropagation gradient of 0.05449763261734643!\n",
      "Parameter id: 1280 Numerical gradient of -0.023398394333185024 is not very close to the backpropagation gradient of -0.023440127614155596!\n",
      "Parameter id: 1281 Numerical gradient of 0.006922684647747701 is not very close to the backpropagation gradient of 0.0069009930428102426!\n",
      "Parameter id: 1282 Numerical gradient of 0.04721312230060448 is not very close to the backpropagation gradient of 0.04720834624390598!\n",
      "Parameter id: 1283 Numerical gradient of -0.006821654352506812 is not very close to the backpropagation gradient of -0.006874527011079549!\n",
      "Parameter id: 1284 Numerical gradient of 0.030429880837345987 is not very close to the backpropagation gradient of 0.030520938472205647!\n",
      "Parameter id: 1285 Numerical gradient of -0.051953552571148975 is not very close to the backpropagation gradient of -0.051920742699312905!\n",
      "Parameter id: 1286 Numerical gradient of 0.06144484920866943 is not very close to the backpropagation gradient of 0.06150720394152426!\n",
      "Parameter id: 1287 Numerical gradient of -0.006113110018191037 is not very close to the backpropagation gradient of -0.0060399729734191155!\n",
      "Parameter id: 1288 Numerical gradient of 0.061474159096519536 is not very close to the backpropagation gradient of 0.061573526416465775!\n",
      "Parameter id: 1289 Numerical gradient of 0.06090372650646713 is not very close to the backpropagation gradient of 0.06099226394140818!\n",
      "Parameter id: 1290 Numerical gradient of 0.08407408103039415 is not very close to the backpropagation gradient of 0.08411685350985307!\n",
      "Parameter id: 1291 Numerical gradient of -0.15224355109921817 is not very close to the backpropagation gradient of -0.15258636356753452!\n",
      "Parameter id: 1292 Numerical gradient of -0.06031664057104535 is not very close to the backpropagation gradient of -0.060344477905224264!\n",
      "Parameter id: 1293 Numerical gradient of 0.07565192916558772 is not very close to the backpropagation gradient of 0.07573932313147397!\n",
      "Parameter id: 1294 Numerical gradient of -0.10335243771919522 is not very close to the backpropagation gradient of -0.10355855226744014!\n",
      "Parameter id: 1295 Numerical gradient of 1.554312234475219e-05 is not very close to the backpropagation gradient of -8.599237568145129e-05!\n",
      "Parameter id: 1296 Numerical gradient of 0.002320366121466577 is not very close to the backpropagation gradient of 0.002296639540745923!\n",
      "Parameter id: 1297 Numerical gradient of 0.0032440716779547074 is not very close to the backpropagation gradient of 0.0031619693193327553!\n",
      "Parameter id: 1298 Numerical gradient of -0.0014102052858788738 is not very close to the backpropagation gradient of -0.0014181725644701222!\n",
      "Parameter id: 1299 Numerical gradient of -0.008800293826993766 is not very close to the backpropagation gradient of -0.008848115771528315!\n",
      "Parameter id: 1300 Numerical gradient of -0.008875344903458426 is not very close to the backpropagation gradient of -0.008922987574078696!\n",
      "Parameter id: 1301 Numerical gradient of -0.008632650150275367 is not very close to the backpropagation gradient of -0.00864725430092244!\n",
      "Parameter id: 1302 Numerical gradient of 0.030897284730713178 is not very close to the backpropagation gradient of 0.03102586126923294!\n",
      "Parameter id: 1303 Numerical gradient of 0.01035438401686406 is not very close to the backpropagation gradient of 0.010394429173220941!\n",
      "Parameter id: 1304 Numerical gradient of -0.025830004801719042 is not very close to the backpropagation gradient of -0.025822405006854753!\n",
      "Parameter id: 1305 Numerical gradient of 0.006718403611216672 is not very close to the backpropagation gradient of 0.006866028291003218!\n",
      "Parameter id: 1306 Numerical gradient of 0.048538506547401994 is not very close to the backpropagation gradient of 0.048602784112537746!\n",
      "Parameter id: 1307 Numerical gradient of -0.027781554834405142 is not very close to the backpropagation gradient of -0.02769607875313844!\n",
      "Parameter id: 1308 Numerical gradient of 0.06054357015727873 is not very close to the backpropagation gradient of 0.06057398720079335!\n",
      "Parameter id: 1309 Numerical gradient of -0.006642464356332312 is not very close to the backpropagation gradient of -0.006558428307372096!\n",
      "Parameter id: 1310 Numerical gradient of 0.053222315443690604 is not very close to the backpropagation gradient of 0.05326629090753094!\n",
      "Parameter id: 1311 Numerical gradient of 0.05613176590202328 is not very close to the backpropagation gradient of 0.056186033428096464!\n",
      "Parameter id: 1312 Numerical gradient of 0.05893552312841165 is not very close to the backpropagation gradient of 0.058943461053758146!\n",
      "Parameter id: 1313 Numerical gradient of -0.13401324494566325 is not very close to the backpropagation gradient of -0.13428073035487648!\n",
      "Parameter id: 1314 Numerical gradient of -0.050321968814159845 is not very close to the backpropagation gradient of -0.05029636023631102!\n",
      "Parameter id: 1315 Numerical gradient of 0.04966138611450788 is not very close to the backpropagation gradient of 0.04971084015043352!\n",
      "Parameter id: 1316 Numerical gradient of -0.10827516661038317 is not very close to the backpropagation gradient of -0.10845179920145218!\n",
      "Parameter id: 1317 Numerical gradient of 0.04631850458736153 is not very close to the backpropagation gradient of 0.04637605042937948!\n",
      "Parameter id: 1318 Numerical gradient of -0.037168712552215766 is not very close to the backpropagation gradient of -0.03710409169890743!\n",
      "Parameter id: 1319 Numerical gradient of 0.06297429244739305 is not very close to the backpropagation gradient of 0.06299951127905362!\n",
      "Parameter id: 1320 Numerical gradient of -0.004211075932403219 is not very close to the backpropagation gradient of -0.004134019961366975!\n",
      "Parameter id: 1321 Numerical gradient of 0.0655622223177943 is not very close to the backpropagation gradient of 0.0656213816326825!\n",
      "Parameter id: 1322 Numerical gradient of 0.06297407040278813 is not very close to the backpropagation gradient of 0.063032344742656!\n",
      "Parameter id: 1323 Numerical gradient of 0.06961897724977462 is not very close to the backpropagation gradient of 0.06964697139030188!\n",
      "Parameter id: 1324 Numerical gradient of -0.15654988416713422 is not very close to the backpropagation gradient of -0.15686360437253938!\n",
      "Parameter id: 1325 Numerical gradient of -0.05908362687989665 is not very close to the backpropagation gradient of -0.05906953435945536!\n",
      "Parameter id: 1326 Numerical gradient of 0.0648632259014903 is not very close to the backpropagation gradient of 0.06495517422443249!\n",
      "Parameter id: 1327 Numerical gradient of -0.11529688315192742 is not very close to the backpropagation gradient of -0.11546018330623686!\n",
      "Parameter id: 1328 Numerical gradient of -0.03347633281691742 is not very close to the backpropagation gradient of -0.03355046457269483!\n",
      "Parameter id: 1329 Numerical gradient of 0.04658584629169127 is not very close to the backpropagation gradient of 0.04656003811770708!\n",
      "Parameter id: 1330 Numerical gradient of -0.06051448231403355 is not very close to the backpropagation gradient of -0.06056116429956934!\n",
      "Parameter id: 1331 Numerical gradient of 0.005346167952779979 is not very close to the backpropagation gradient of 0.005283526586803925!\n",
      "Parameter id: 1332 Numerical gradient of -0.05808487024694386 is not very close to the backpropagation gradient of -0.05817901077573331!\n",
      "Parameter id: 1333 Numerical gradient of -0.056855187224869035 is not very close to the backpropagation gradient of -0.05694145264723015!\n",
      "Parameter id: 1334 Numerical gradient of -0.07558065284740678 is not very close to the backpropagation gradient of -0.07563637077083706!\n",
      "Parameter id: 1335 Numerical gradient of 0.14425882710611404 is not very close to the backpropagation gradient of 0.14459397158125586!\n",
      "Parameter id: 1336 Numerical gradient of 0.05570055527925888 is not very close to the backpropagation gradient of 0.05572198411467641!\n",
      "Parameter id: 1337 Numerical gradient of -0.06890843451401452 is not very close to the backpropagation gradient of -0.0690073661696376!\n",
      "Parameter id: 1338 Numerical gradient of 0.10152967355736564 is not very close to the backpropagation gradient of 0.10171630883525898!\n",
      "Parameter id: 1340 Numerical gradient of -0.018575141425003494 is not very close to the backpropagation gradient of -0.018626603835114153!\n",
      "Parameter id: 1341 Numerical gradient of -0.030819125029779567 is not very close to the backpropagation gradient of -0.030828685485887058!\n",
      "Parameter id: 1342 Numerical gradient of 0.005513589584893452 is not very close to the backpropagation gradient of 0.0054847799110588815!\n",
      "Parameter id: 1343 Numerical gradient of -0.01066990940046253 is not very close to the backpropagation gradient of -0.010658946323051872!\n",
      "Parameter id: 1344 Numerical gradient of -0.026122215501800383 is not very close to the backpropagation gradient of -0.026130751890970583!\n",
      "Parameter id: 1345 Numerical gradient of 0.004100275674545628 is not very close to the backpropagation gradient of 0.004007892189873118!\n",
      "Parameter id: 1346 Numerical gradient of 0.07317013661634064 is not very close to the backpropagation gradient of 0.07339385466017942!\n",
      "Parameter id: 1347 Numerical gradient of 0.019698909170529078 is not very close to the backpropagation gradient of 0.01965448236489355!\n",
      "Parameter id: 1348 Numerical gradient of -0.01922106918073041 is not very close to the backpropagation gradient of -0.019333423417253068!\n",
      "Parameter id: 1349 Numerical gradient of 0.06036482425031408 is not very close to the backpropagation gradient of 0.060477110831899744!\n",
      "Parameter id: 1350 Numerical gradient of -0.07772515964177273 is not very close to the backpropagation gradient of -0.07779451506955981!\n",
      "Parameter id: 1351 Numerical gradient of 0.03945421767070911 is not very close to the backpropagation gradient of 0.03942165947629931!\n",
      "Parameter id: 1352 Numerical gradient of -0.07813261149181017 is not very close to the backpropagation gradient of -0.07817124734429477!\n",
      "Parameter id: 1353 Numerical gradient of 0.006008304964666422 is not very close to the backpropagation gradient of 0.005934691237816293!\n",
      "Parameter id: 1354 Numerical gradient of -0.06117217843382149 is not very close to the backpropagation gradient of -0.061274865953568396!\n",
      "Parameter id: 1355 Numerical gradient of -0.058775428968260705 is not very close to the backpropagation gradient of -0.05886833407403354!\n",
      "Parameter id: 1356 Numerical gradient of -0.08240097493228404 is not very close to the backpropagation gradient of -0.08244601216573329!\n",
      "Parameter id: 1357 Numerical gradient of 0.1779394409595625 is not very close to the backpropagation gradient of 0.17828984215124244!\n",
      "Parameter id: 1358 Numerical gradient of 0.07932410284183788 is not very close to the backpropagation gradient of 0.07933309797425929!\n",
      "Parameter id: 1359 Numerical gradient of -0.06920952699829286 is not very close to the backpropagation gradient of -0.06931325566347792!\n",
      "Parameter id: 1360 Numerical gradient of 0.12468892585104639 is not very close to the backpropagation gradient of 0.12488893943105038!\n"
     ]
    }
   ],
   "source": [
    "#The gradient checking of the model\n",
    "model = ModelSort(input_size, output_size, hidden_size, seq_length)\n",
    "# Get the gradients of the parameters from a subset of the data\n",
    "backprop_grads = model.get_gradients(\n",
    "    x_train[:100], y_train[:100])\n",
    "\n",
    "eps = 1e-9  # Set the small change to compute the numerical gradient\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for p_idx, param in enumerate(model.get_params_iter()):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    \n",
    "    # + eps\n",
    "    param += eps\n",
    "    plus_loss = model.loss(\n",
    "        model.predict_proba(x_train[0:100,:,:]), y_train[0:100,:,:])\n",
    "    \n",
    "    # - eps\n",
    "    param -= 2 * eps\n",
    "    min_loss = model.loss(\n",
    "        model.predict_proba(x_train[0:100,:,:]), y_train[0:100,:,:])\n",
    "    \n",
    "    # reset param value\n",
    "    param += eps\n",
    "    \n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_loss - min_loss) / (2*eps)\n",
    "    numerator = np.linalg.norm(grad_backprop - grad_num)\n",
    "    denominator = np.linalg.norm(grad_backprop) + np.linalg.norm(grad_num)\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    if not np.isclose(grad_num, grad_backprop, ):\n",
    "        print((\n",
    "            f'Parameter id: {p_idx} '\n",
    "            f'Numerical gradient of {grad_num} is not very close '\n",
    "            f'to the backpropagation gradient of {grad_backprop}!'\n",
    "        ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
